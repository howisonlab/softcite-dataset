Motivation and Prior Work

Researchers and industry are increasing labeling (or annotating) data to facilitate machine learning. Drawing on and adapting qualitative content analysis techniques. Little formalized guidance available. Data annotation is also rediscovering some challenges of data provenance familiar to the information sciences.

The goal of this paper is thus two-fold: to present a labeled dataset, and to reflect on the process and challenges in its creation and presentation. To meet these goals we present three sections. The first is a paper within a paper, straightforwardly describing our annotation project and its results. The second provides substantial process and provenance description (including mis-steps and detailed decisions). The third reflects on the overall process, describing tensions experienced and highlighting areas where information science can provide further guidance for supervised machine learning dataset creation projects.

# Genre paper: a gold standard dataset on software mentions in the scientific literature

We present an annotated dataset of software mentions in full-text scientific papers converted from PDFs of academic articles. The dataset provides a "gold-standard" training/testing set for developing supervised machine learning to accomplish named entity recognition at scale.

First we discuss motivations and prior work on named entity recognition of software, describe our coding scheme and reliability checks, present the dataset file (format and contents). Finally we briefly demonstrate the usefulness of the dataset and discuss future work.

## Motivations and prior work on software named entity recognition

Software is crucial to scholarship, forming a "knowledge infrastructure" that supports and enables research (cite Mayernik et al.). Yet, researchers are often frustrated by redundant, incompatible, or poorly supported pieces of software. Like other infrastructural work, the creation and maintenance of research software has been found to be relatively invisible, especially to the formalized scholarly reputation system built on bibliometrics. This is because software is rarely formally cited (cite Howison and Bullard). If we were able to more accurately identify the use of software in the published literature, we could make software infrastructural contributions more visible, creating a resource for demonstrating impact. Such a resource would be valuable to both those doing the software work and to those seeking to identify and fund needed software work to improve scholarship.

Ongoing work addresses making software visible in the academic literature in two ways. The first is to create and disseminate clear standards for citation, changing the behavior of authors and publishers. The second, known as named entity recognition, is to work with existing published literature (and that to be published while standards propagate) to automatically identify software, however it is mentioned.

named entity recognition is challenging, but has been successful across a range of entity types, including XXXX. Challenges involved are: the diversity of language used to reference entities, the lack of a controlled vocabulary or namespace for naming entities. Supervised machine-learning is a promising approach, but requires substantial datasets for training and testing.

Further challenges to useful systems based on named entity recognition come from the form of academic publishing. First, only a small portion of the published literature is available in clean full text, such as marked up XML. The baseline form of literature is the academic PDF, which are challenging to convert to text for machine processing. Thus named entity recognition is often limited to abstracts and targeted to literature discovery rather than full scale "distant reading" (cite Moretti/Mehta et al.) for bibliometrics and analysis of the literature at scale.

A recent review article (Kr√ºger and Schindler, 2020) provides an analysis of prior work on named entity recognition of software mentions ("software usage statements"). They identify 48 studies using four techniques, in increasing complexity: 1) Term Search, 2) Manual Extraction, 3) Rule-based Extraction, and 4) Supervised Learning. Term Search, employed in 12 articles, involves searching databases for known strings, such as URLs, DOIs, or particular software names. Manual Extraction, used in 16 studies, involve "approaches in which humans read a text in order to identify usage statements" but which are not then used for machine learning training/testing. Rule-based extraction, used by 16 articles, involves non-machine learning approaches, usually rules pre-specified by humans. For example, using regular expression matching, "special spellings such as capitalization", or "the appearance of special words indicating artifacts", informed by human experts. (cite BioNerDs). Rule-based extraction can be used to generate training/testing datasets for machine learning, including "distant supervision" and "weak supervision" through iterative bootstrapping (discussed further below). Finally supervised learning employs a manually annotated and curated "gold standard" dataset of software mentions to train a classifier.

The four techniques generally show a trade-off between scope, performance, and cost. For example, term search for specific software, but likely to have high performance, but is obviously of scope limited to the specific software searched. Manual annotation for specific software has wider scope, in that it can recognize alternative forms of mentioning the searched for software, but higher cost due to greater human reading requirements (cite HubZero paper). Rule-based extraction can be of high performance but with scope limited to specific domains or types of software with practices that leave identifiable artifacts in papers. For example BioNerDs (XXXXX check these numbers) showed precision of 0.49 and recall of 0.57, while rules specific to the R statistics software ecosystem, showed precision of 0.84 and recall of 0.87, drawing heavily on URL formats specific to R package management (XXXX check me). Rule-based extraction involves cost in designing the rules, but low cost of re-use across larger datasets.

Iterative boostrapping uses expert specified rules to identify an initial set of software mentions (as "seeds"), then using those as training examples for supervised machine learning to identify further examples for manual review, expanding the training set. Bootstrapping can provide high precision, but relatively low recall (e.g., Pan et al achieved precision of 0.83 but recall of 0.35). Recall is limited due to the incomplete coverage of identifying patterns of language use. In the language of content analysis, the initial seeds do not achieve saturation, even when complemented by expansion through machine learning. Evaluation of "distant" or "weak" supervision is expensive because while false-positives can be identified through review of the results (a relatively small amount of text), false-negatives require exhaustive manual review of full-text. That manual review increases the cost of bootstrapping, but is key to evaluation.

Supervised machine learning based on manually annotated "gold standard" datasets shows the most generalizable scope and flexibility, good performance. For example demonstration use of deep learning on a dataset of 85 annotated articles (in a specific domain) showed precision of .90 and recall of .94. These come at high cost both in the initial creation of the gold standard dataset and in computation costs for more advanced techniques. While different supervised machine learning algorithms (e.g., CRF, sequence models, SVMs, neural networks, deep learning, etc.) can boost performance the size of the gold standard manual annotation is important (cite something about ImageNet). Existing published datasets, all using slightly different annotation schemes, have annotated 90 (Howison and Bullard), 40 (Nangia and Katz), 166 (Allen et al), and 85 articles (Duck et al).

For these reasons we created a manually annotated dataset of software mentions on full-text derived automatically from academic PDFs. This dataset will be useful both for training supervised models and for evaluating bootstrapping techniques (which could then be used to further expand available training or specialize to a specific otherwise troublesome domain). We annotated XXXX4,973 PDFs, identifying XXXX7,024 mentions of software. Those mentions were found in XXXX1,247 articles (XXXX3,726 articles had no mentions, providing copious negative examples) (XXXX these numbers should come directly from Rtex queries of the published TEI XML files). We therefore provide a gold standard dataset significantly larger than those previously available, and one aligned with a high-quality, freely available extraction tool for academic PDFs and therefore likely to generalize.

For each mention we annotated details:  (full coding scheme is available online).

Summary figures and examples.

The dataset is published as a single TEI XML file, including identifiers for each paper, metadata on the papers. Mentions are encoded using `<rs >`, as shown in Figure XXXX.

Paper selection and conversion by GROBID.

Our annotation was done by XXXX24 annotators over YYYY2 year time period. After training of our annotators, KKKK articles were doubled-coded, showing agreement at XXXX. After talking to agreement as a group, then adjusting our coding scheme, articles were then singly coded by our trained annotators. At the conclusion of annotation, the full dataset of annotations was then reviewed by the authors (who are domain specialists on scientific software) resulting in curation adjustments made for consistency across the dataset. For example, expert review removed annotation of web services like Google and standardized on excluding strings like "v." and "version" from the software_version field. Finally, we used string searches of the mentions discovered across our full dataset, identifying a small number of annotations missed due to annotator fatigue or training inconsistency. Details on curation of the full dataset are in our full coding scheme, published online XXXX.

## Prototype use for supervised machine learning

We demonstrate the usefulness of this dataset by using it for training/testing a basic supervised machine learning system.

As expected a relatively simple supervised model (CRF) shows good performance, when compared to bootstrapping. This is particularly true for recall.

We report traditional precision, recall, and f-score of the CRF model performance. We divided the TEI XML corpus randomly with 90\% of the corpus for training and the rest 10\% for evaluation. All the performance measures were averaged with 10-fold cross-validation with the 90/10 segmentation of the whole corpus. Among all the fields, we have achieved a precision of .83, recall of .74, and f-score of .78. The performance metrics of each annotation label are detailed in table \ref{table:result2}. To our knowledge, the most similar effort recently to ours achieved a f-score of .74 with a precision of .63, leveraging a CRF model \cite{duck_2015}.

\begin{table}[h!]
\begin{center}
 \begin{tabular}{cccl}
 \toprule
 Labels & Precison & Recall & f-score  \\
 \midrule
 \emph{software} & 86.5 & 72.24 & 78.67 \\
 \emph{publisher} & 85.45 & 74.84 & 79.72 \\
 \emph{version} & 89.65 & 84.99 & 87.14 \\
 \emph{url} & 69.19 & 63.35 & 65.03 \\
 micro-average & 82.7 & 73.85 & 77.64 \\
 \bottomrule
\end{tabular}
\end{center}
\label{table:result2}
\caption{Recent Results for prototype machine learning training and evaluation by using a linear CRF approach}
\end{table}

It is a goal of our work to generate community interest and collaboration. We hope that the community will use this dataset for further improvement, including using computationally expensive algorithms such as BlistXXXX. We anticipate that the larger dataset should enable training systems of equivalent or better performance to that of Kruger (cite), but more robust to novel data and domains. To encourage others we do not report results from more computationally complex models, leaving "low hanging fruit" to encourage collaboration (cite Raymond.)

## Future work

We have provided a dataset of mentions of software in the academic literature. It is the largest dataset available to date, and is derived from full text of articles that have been converted directly from published academic PDFs. This makes the dataset useful for supervised machine learning at scale.

Our future work takes two forms: collaborative expansion of the dataset and use of large scale databases of mentions discovered by systems trained on the dataset.

First we are founding a community project to further increase the size and scope of the dataset. This involves adapting existing annotated corpuses for consistency with our scheme, collaboratively annotating articles in new domains, and using the gold standard to validate promising bootstrapping techniques to further increase available training/testing examples.

Second we are using the dataset to perform machine learning at scale and prototying three systems that use the resulting dataset of mentions. Our prototype scaled system will be used to develop "Software Impact Story" a companion to the alt-metrics enabled "Impact Story" providing suggested mentions of software developed by individuals. Softcite suggest is a prototype citation recommendation system, analyzing submitted manuscript text to highlight uncited mentions and to suggest unmentioned software that similar articles have used. Finally, discovered mentions will be deployed in the CiteAs.org system. CiteAs is a specialized search engine that takes the name or identifier of software, and returns a suggested citation. Currently CiteAs uses web crawling and conventions such as CITATION files in repositories to discover author's preferred citations. Software mentions in publications will provide additional sources. We expect that the diversity of mention forms for software‚Äîwhich causes trouble for impact assessment‚Äîwill encourage software contributors to make clearer requests for citation.

# Creation of the dataset

In this section we describe the process of creating the SoftCite dataset in greater detail. The provenance of datasets is crucial to their appropriate use, as machine learning models trained in one context will return errors if applied to another [@gebru_datasheets_2020], including reproducing, or even exacerbating, social bias. Additionally we hope that relatively rich detail in our account will offer methodological guidance to those researchers undertaking the creation of datasets.

As one reads this section some details may seem irrelevant or extraneous. Be assured that, as authors, we were often unsure what details to include. More, as you read, you may find details that undermine the impression of quality hopefully conveyed by the bare-bones presentation above. In the final section of this paper we reflect on the process, and emotions, of documenting our process and writing this section.

## Genesis of the project

The creation of this dataset followed the publication of Howison and Bullard [@howisonandbullard], which examined 90 articles from the literature, documenting low rates of formal citation suggested by earlier interviews with researchers who build software, where the interviews had been focused on incentives for software work and maintenance. The software mentions found in those articles formed the initial training examples. Two elements of the coding scheme are clearly linked to these origins, and might not be present in dataset creation processes for named entity extraction.

The first was the `software_used` tag, which indicating whether it seemed that the authors actually used the mentioned software in their work. The second was effort to connect in-text mentions with nearby citations, and to code the type of article cited in the linked reference.

## Selection of papers

It was key to the motivations for the project that any system ultimately be able to process PDFs, because we wanted to be able to process historical publications and publications yet to adopt any improved software citation policies. However, at the origins of the project we did not know about the `GROBID` tool for converting academic PDFs. We experimented with `pdf2text` (and the rOpenSci package, `fulltext`) [XXXXcite] but found the output too inconsistent to be readable as an article, undermining annotators engagement and contextual understanding.

We also wanted to be able to release a dataset that others could work with. This explains our choice to annotate open access articles, avoiding creating a dataset that annotates proprietary article collections from publishers. While not all articles could be republished in converted fulltext form, we reasoned that future users would be able to obtain the fulltext via the DOI.

Accordingly we started with PubMed Open Access, particularly because it provided XML versions of the article. However, given our ultimate intent to work with converted PDF fulltext, we did not provide the XML versions of the articles to annotators (since we would not be able to do that later). We did, as discussed below, use the available XML text for our first round of agreement coding. Having students select quotes directly from PDF articles was a risky decision; team-members disagreed about how hard it would be to later alignment these quotes with converted fulltext. Indeed alignment was difficult, causing long-running difficulties, although the discovery of GROBID helped immensely.

## Collaboration infrastructure

Our annotators used text editors to enter details into text files, formatted as RDF in Turtle format (cite). These files were generated by an article assignment process driven by a command line script, talking to a `mysql` database which kept track of which articles had been assigned to which annotators. Annotators edited these files, using keyboard shortcuts to insert "snippets" which were text templates for each step of the annotation.

These files were then checked into github, using the commandline git client and creating pull requests, which were accepted by doctoral student members of the research team. Each annotator worked in a folder named for their user, to avoid file editing conflicts. A python script (XXXXURL) was used to read all the RDF into a single RDF graph (full_dataset.ttl). Typos, unfilled fields, and RDF Turtle syntax errors were a frequent issue. Later we implemented continuous integration using TravisCI on github pull requests to check syntax and validate responses; we hoped that annotators would fix these when alerted by the pull request, but they almost always required help to both resolve the identified issues and to manage confusion over what work was "in" a pull request. When analyses began, we used SPARQL queries (citeXXXX) to export the dataset in CSV format.

The majority of annotation was done on a department Linux server with annotators provided command line user accounts and thus home directories. Annotators used the Atom editor running on their laptop to edit files on the server, mounted via SFTP provided by Atom.

We used this rather idiosyncratic infrastructure for three reasons. First, we intended to annotate the in-text mention and linked references as units, following the Howison and Bullard publication. This made existing content analysis tools, such as Atlas.TI (cite) harder to use. Second, we knew the project would take considerable time and we did not want to rely on hosted commercial annotation tools that might change in ways that undermined the project (and believed that github, at least in basic functions, was relatively stable.) Third, we chose to use git-based aggregation approach rather than building a custom web-UI because we reasoned the effort to train the student annotators would be valuable to them as students and us as educators, while the effort to build and maintain a web-UI would be equally large but of benefit only to those building it (as well as likely being fragile). We used SFTP mounted files because we wanted all work, even that in progress, to be in a file-space owned by the project and thus able to be backed up even prior to students submitting via git. Indeed, after the main phase of annotation, while checking our assignment database against work completed, we found some un-submitted work in annotator home folders (which we, as admins, submitted via git).

Annotators were recruited by advertising a research project in university venues. They were paid $15 an hour but restricted to 10 hours a week. Some wanted to work more, but we felt it was important to prevent fatigue and maintain some balance between annotator contribution level. Students were largely from a large public university (XXXXblinded) but we recruited a small number from a local historically black  university, with a large hispanic population. Unexpected issues with work authorization documentation (required of those not enrolled as students at our institution) meant those HBCU students were not able to contribute many annotations.

Students were trained in classrooms, with materials prepared by a doctoral student and published on our github site (where they remain, blindedXXXX). These materials addressed both our collaboration infrastructure and our annotation scheme. We first used the articles coded in the Howison and Bullard paper. We coded a number of files all together, including allowing and encouraging open questions, before breaking into groups. In later weeks, we conducted training for newly recruited annotators, with previously trained annotators working in pairs on assigned articles on one side of the large room. All were encouraged to get involved in discussions, or bring questions to the group. The PI was deferred to as the authority or oracle, with doctoral students writing down reasoning and testing it in open conversation. Despite efforts, new examples often lead to referring to the PI but the PIs responses were sometimes inconsistent over time. It was challenging to managing training, assignment logistics, payroll, and reason about specific examples. It was also challenging to encourage annotators to disagree in public. As these discussions were face to face, there are no records other than those ultimately encoded in the annotation scheme as examples and justifications. Evolution of the annotation scheme could be somewhat recovered by examining the git log history of the scheme and the training pages.

After initial training on articles assigned to all, annotators were assigned articles in pairs, such that each article was annotated by two annotators, managed by updates in the `mysql` database. After our first round of agreement assessment each article was assigned to only a single annotator. This explains patterns of multiple annotation that can be found in the csv version of the dataset, where some articles are coded by over 20 annotators, a group by 2, and many by a single annotator. In the final TEI XML version, however, multiple annotations are not present. The final annotations were derived from whichever annotation found the most mentions in an article, then processed through iterative refinement (as described tersely above and, with comment, below).

About half-way through the annotation process students within the group were invited to create a "dashboard" of annotation progress, showing graphs of articles annotated, broken down by annotator. While useful for identifying assigned but not completed work, the dashboard when shown to the group revealed differences in numbers of articles annotated, raising questions of productivity (even after adjusting for page lengths). Judging this potentially problematic, the dashboard was kept in the source code, but not further used.

## Assessing agreement

After the bulk of annotator training and a period of annotators dual coding articles we sought to assess annotator agreement. That required aligning the `full_quote` fields being produced by annotators with the PMC XML. That process was more difficult than expected, taking the PI around three weeks. During that time annotators continued to work, as they were "contracted" (as it were) for 10 hours a week.

More on XML full quote matching?

Show stats.

These stats were presented to the group (although not all annotators were present) and discussed. This brought a number of questions that annotators had to the fore, resulting in a small number of clarifications to the coding scheme, as well as identifying a number of "misses" that reflecting annotators ascribed to fatigue rather than conceptual confusion.

At this point, the PI and team had to decide how to proceed. Literature and industry press was discussing the "image-net" moment, highlighting the surprising importance of large datasets. Agreement levels were at arguably acceptable levels for content analysis, and confidence was high that the discussion had resolved remaining issues (and that fatigue issues were somewhat unavoidable, even with double annotation). Costs were mounting and some trained student annotators were approaching graduation. These reasons combined to inform the decision to switch to single annotators, going forward.

Around this time annotators, in face to face and email discussion, identified two patterns in the work they were doing. The first was that some papers were "monsters" with over 100 mentions. They questioned the value of such repetition and identified those papers as very tiring to work on. The PI and team decided that while packages might be repeated, each provided slightly different context and might be useful. Ultimately we assured the annotators that these had value.

The second pattern identified by annotators discussing their work was the tendency of mentions to "cluster" within a paper, occurring more densely in some parts, and less densely in others. Some students, seemingly with natural science backgrounds, suggested these were the "methods and materials" sections, but inspection suggested multiple clusters in different parts of papers. We created an analysis to document this, displaying the clustering to the group. We may follow this up in future research.

We mention these two events because they credibly might have affected annotators work. For example, it is possible that discussing and displaying cluster gave annotators reason to concentrate in some sections but skim others. Similarly, we mention the "monster" papers because annotation patterns may be different in those, particularly towards the end of the fatiguing and repetitious articles. We have not investigated whether annotations differ in these "monster" articles, or before and after group discussion of clustering (although we hope a community might be inspired to do so).

Student annotators then worked individually on articles, working towards completing all XXXX articles.

# Connecting with GROBID for full-text alignment

A core goal of this project was to enable entity extraction from full-text articles published in the default format, PDF. GROBID (GeneRation Of Bibliographic Data) is a text mining library utilizing machine learning techniques trained on the format of academic PDFs to extract, parse, and restructure PDF into TEI XML encoded documents \cite{grobid:2019}. The structured TEI XML representation explicitly marks up common structural attributes in a document such as paragraphs, section headers, formulas, figures, bibliographical references, also storing the original location on pages. GROBID can therefore display annotations on top of the source PDF, as shown in Figure \ref{fig:annotation1}, providing a visual, easy to apprehend, and contextual display for the annotations. GROBID can also scale; recently GROBID processed 915,000 publishers' PDF per day (around 20M pages per day) with a single server with 16 CPUs.

## Iterative refinement

With annotations complete and an appropriate PDF to XML conversion, we began to work with an entity extraction expert to complete alignment, and assess corpus usefulness through prototype machine learning. This process was iterative, with corpus wide summaries identifying issues resulting in consistency improvement. To track the changes to the corpus we identified XXXX processing steps, recorded against each article.

### Alignment

We transformed all the manual annotation data from RDF into CSV. We then normalized the text from both RDF and TEI XML sources by discarding all forms of whitespace, converting all the characters to lower case, and removing all non-ASCII characters. Next, we implemented a relatively straightforward string matching algorithm with these normalized strings. All the occurrences of the normalized software mention string for a given page are then identified within the TEI XML text. For each identified mention string, we also tried to match its left and right contexts with the contextual string present in the RDF annotations (\emph{full\textunderscore quote}). If the full context could be matched, we consider that successful alignment. Finally, we matched the version, software creator, and the URL associated with the software mention in the identified contextual string.

Even with the high-quality TEI XML output of GROBID, the alignment process was not effortless. Because misalignment emerged from the discrepancy between dual conversions of the PDF originals, first by annotators, second by the PDF-to-text conversion using GROBID. Annotators completed one conversion when copying all the required annotation string from the PDF document into the RDF templates. The copied text can vary and is not necessarily WYSIWYG (\textit{i.e.}, "what you see is what you get" in computing terms) due to the diverse PDF formats, operating systems, and the PDF readers/plugins used by annotators. Meanwhile the second conversion is performed by GROBID as it extracts usable text from the PDF in a consistent way. As a result, the inconsistencies between manual inputs and machine inputs include the misalignment of non-character glyphs (\textit{i.e.}, effectively in-line images in the PDF), inconsistent conversion of whitespace characters (often whitespace visible in the PDF were not included in the copied text), inconsistent non-ASCII character conversions, inconsistencies in resolved character codes (usually due to embedded fonts in original PDF files), inconsistent treatment of line and page breaks, remaining inclusion of PDF formatting artifacts (\textit{e.g.}, running headers) and different ordering of elements between PDF viewers used by annotators (\textit{e.g.}, macOS Preview and GROBID handle the ordering of PDF document elements in different ways: macOS Preview follows PDF object stream order while GROBID takes heuristics-based reading order). Human errors also engendered some alignment failure, including typos, disarranged character orders, or differing string content in the text input. For example, when the label \emph{software\textunderscore name} was filled up with the full name of software (\textit{e.g.}, "Statistical Package for Social Sciences") whereas the \emph{full\textunderscore quote} just contained an acronym (\textit{e.g.}, "SPSS").

After efforts of string normalization removing whitespace, line breaks, structural mark-ups, etc., we still had 547 cases of annotated software mentions where the inconsistencies between the two sources of inputs (\textit{i.e.}, RDF inputs from annotators and TEI XML inputs converted by GROBID from PDF originals) prevented automatic alignment. We then resolved these alignment failures through inspection of the two sources of converted text, manually fixing the typo in manual annotation results, and directly copying the corresponding context string from the TEI XML source into the initial RDF input (note that originally the contextual strings were copied from the PDF sources), in a newly added \emph{tei\textunderscore full\textunderscore quote} field. Following the manual improvement of RDF inputs, we reran the same alignment script, and achieved and validated more successful string matching. Alignment edits to the RDF files do change the provenance chain, but were recorded in git version control and could thus be recovered, if needed. For the final published dataset, we dropped the mention annotations that cannot be aligned with their PDF context even after all alignment efforts (XXXX how many?).

### Post-alignment agreement calculation

Our alignment work enabled us to identify and fix typos or copy/paste issues from different PDF readers that had lead to seeming disagreements during earlier agreement calculations. In other words, two annotators successfully identified the same entity as software but typos in annotation texts deflate the agreement. As of the release of our TEI XML corpus, among 260 PMC articles that are annotated by more than one annotator, a percentage agreement of 70.7\% is achieved in the field of \emph{software\textunderscore name}. \emph{Version\textunderscore number} has the highest agreement of 80.8\%, in contrast are an agreement of 42.7\% in the field of \emph{version\textunderscore date} and an agreement of 42.2\% in \emph{creator}; the field \emph{url} has an agreement of 66.8\%. Among all these fields, the inter-annotator agreement is 61.9\%.

In the economic article set, we have 26 articles that are annotated by more than one annotator. Despite the relatively small number of annotated articles, we have not spotted agreement level that is drastically different from that in the PMC article set. The overall agreement counting in all the fields is 66.3\%. The agreement in the \emph{software\textunderscore name} field is 72.1\%. \emph{Version\textunderscore number} still has the highest agreement of 76.7\%; while \emph{version\textunderscore date} has the least agreement of 43.4\%. The agreement is 59.2\% in the field of \emph{creator} and 71.3\% in the field of \emph{url}. In Table ?? we present the number of annotations and the corresponding agreement achieved by field in these two article sets.

### Consistency review

Alignment and prototype use of the corpus, together with manual inspection, revealed noticeable inconsistencies in application of annotation rules, semantic and syntactic. Through a series of review and manual curation, with discussions recorded in Github Issues we have resolved these inconsistencies and made consequent improvements in our annotation scheme. The published dataset notes which annotations were changed as a result of this process.

Semantically, we reduced the inconsistencies in the view of what should be counted as software. Some of these issues had been addressed during training, but others were novel, only becoming visible as the scale of annotation moved towards saturating the phenomena and practices. Issues included abstract discussions of software, web platforms, and annotation of other infrastructure. In improving consistency, we designed rules with reference to our overall goal: to increase the visibility of software related to the scientific reputation system. This meant that we excluded abstract discussions, requiring the presence of a specific piece of software distributed in some fashion. We excluded general web platforms, such as Github or Google, but included scientific tools, even if accessed via a web portal. Finally, our annotators had used `mention_type` to describe things that they had thought might be software, but had ultimately decided were not (and thus not coded further). Examples included "databases", "algorithms", and "instruments". We had not worked to achieve semantic agreement on these detailed categories, and therefore resolved these to a binary. The mentions annotated as non-software categories are excluded from our TEI XML dataset (but are available in our provenance data).

XXXX need to describe the use of `mention_type` earlier. Did we make this binary?

We made two changes to the names of annotations to make the annotation scheme more semantically coherent to users. We changed the label \emph{software\textunderscore name} to \emph{software} since there exist non-named software entities annotated in our dataset. We changed the label \emph{creator} to \emph{publisher} as most of the entities annotated as creator in the dataset are actually publishers of the software mentioned, rather than original creators.

One label, `software_used`, indicating whether or not the annotator thought the mentioned piece of software was, in fact, used in the research was not included in our consistency review, due to communication issues. XXXX what is the status of this label? Should it be dropped completely, just available in provenance?

In parallel, we improved the syntactic consistency of annotations at the level of each single field. For example, we removed the prefixes of \emph{version\textunderscore number} such as "ver.", "v.", or "version"; and we decided that the full and abbreviated name of the software should be annotated as a single string as \emph{software\textunderscore name}(\textit{e.g.}, "Statistical Package for Social Sciences (SPSS)"). In much of the annotation of software \emph{creator}, especially those commercial organizations usually mentioned with an address (\textit{e.g.}, "IBM Corp, Armonk, NY"), we decided to separate the geo-location away from the annotated string. These syntactic consistency improvements lessened the ambiguity of annotation data in the same field, and thus lower the chance of overfitting when the data is utilized as a training dataset.

We had initially annotated `version` and `version_date` separately, but found that `version_date` had not been used often. In the published dataset these are combined into a single `version` field.

The published dataset indicates whether an article was multiply coded, but does not directly record differences of opinion between annotators, nor the annotators certainty in their annotations. For multiply coded articles, we picked the annotation generated by the most productive annotator among all those who worked on the same article, intending to maximize the number of software examples available for training.

We excluded all the manual annotations of in-text citations and annotation of reference types from the final published TEI XML corpus. Our initial intention was to link these in-text citations with their corresponding entry in the reference list of an article, in order to identify creators and the type of referenced article (e.g., "software paper") But afterward we found that to annotate the in-text citation and their reference entry increased the complexity of tasks for annotators, and they performed inconsistently. Further GROBID automatically identifies all the in-text citations in PDF documents and links them to references, and those are included in the TEI XML dataset, albeit not specifically annotated as software related information. Future users of the dataset could extract and separate them if interested, as well as tie back to original annotation via the provenance identifier in the TEI XML.

These consistency decisions were made through intensive discussion within a group of curators, largely recorded through Github issues (although with some phone discussions). The consistency improvements were subsequently implemented in the TEI XML dataset by both scripting and manual edits. Those edited during this process are marked XXXX.

We then expanded the dataset by a string search using all the identified software names across XXXX all the picked paragraphs, seeking situations where a string in one context was identified as software but the same string in other contexts was not identified. Such a situation is a likely false negative for software mention, caused either by training issues or by annotator fatigue. We manually reviewed all the automatically retrieved cases, and when they were judged to be referencing software also applied the additional detail annotations (`publisher`, `version`, etc.). However, this scripted semantic expansion was not conducted across the full-text of all articles inthe corpus, but only those that had initially been identified as having annotations. XXXX (or was it just the paragraphs that had annotations?)


3. Tensions and Reflections about creation

As supervised machine learning becomes more widespread, researchers from many fields and backgrounds will be annotating data. In this section we record reflections on our process and experience, particularly highlighting tensions we identify.  These are: inter-coder reliability vs dataset size/speed, guidance during the consistency/review stage, provenance extent vs perceived usefulness, interdisciplinary tensions, and tensions from openness. Some of these tensions may seem naive to researchers from some disciplines; we hope that discussing them will help interdisciplinary collaboration in annotation for machine learning.

We found it difficult to balance reliability (as judged by inter-coder agreement) against the size of the dataset to be produced. Clearly greater agreement between annotators is a key indicator of quality, yet increasingly machine learning experience demonstrates the importance of large datasets, even with the possibility of lower quality annotations. Thus there is an inherent trade-off of time and expense. Should effort be put into continuing training and driving up inter-annotator agreement, or should effort be put into annotating additional examples? This trade-off is made more difficult by four factors: saturation, labor turnover, and differing expectations on the value of negative examples and reasonable agreement levels.

The first is that, when working in a new domain, saturation is difficult to understand; if annotators have not yet encountered a wide enough set of examples, then discussion and training are pre-mature. This is perhaps more evidence with a phenomena that changes over time (we annotated articles from a 10 year period and the forms of software changed over time). We had intended continual review of agreement, highlighting emergent issues for resolution and re-training. However we found that very difficult to implement in practice, in part because changes would require re-annotation of already coded articles (to avoid false negatives). Given the sparseness of software mentions, we judged that would be very expensive and highly de-motivating, resulting in a very small number of changed examples.

The second is that we had a workforce with turnover. In particular the availability of student labor varies over time, due to course-load and graduation. We faced the tension of investing in training annotators, but then having them leave the project, requiring us to train again.

The third is the value of negative examples. In our case, this means parts of articles in which there were no software mentions. Given the sparseness of software mentions that is a very large quantity of text. Trained in qualitative content analysis, including grounded theory techniques, the initial annotation team expected negative examples to be important, being concerned about including false negatives in the dataset. Our machine learning experts, joining later, obtained negative examples from text nearby to positive examples, dropping text from articles with no mentions entirely. This decision reflects expectations of balance in training data (or mild imbalances if adjusting to emphasize recall/precision). Accordingly, our published dataset only includes text from paragraphs which had mentions. The expectation is that negative examples will be drawn from those paragraphs. That decision removes the concern that false negatives would be included, since the nearby text was reviewed during the consistency phase discussed below.


The fourth is that some amount of disagreement ought to be expected, due to both annotator fatigue and, ultimately, the conceptual complexity of language and the real world. We followed recommendations from qualitative content analysis, arguing that agreement levels around 0.7 were reasonable (XXXX) but found that the machine learning experts on our team expected much higher levels of agreement, such as 0.95. These levels are simply much higher than would be expected in qualitative content analysis. There is very limited explanation in machine learning papers as to how those levels of agreement were achieved. In particular, as discussed next, it is unclear whether these are achieved during training or as a result of consistency review.

## Guidance in the consistency phase

As our machine learning experts joined the project, they quickly emphasized high agreement, suggesting consistency review. Those trained in qualitative content analysis found this problematic. First it implied overriding the individual perspectives of annotators, considered a plus in qualitative research. Second, it implied the possibility, indeed desirability, of complete agreement. Third, by reviewing only the "positive" mentions, it implied differential treatment of different parts of the dataset (explained in part by different perspectives on the value of copious negative examples, discussed above). These are in part differing values and experiences of the interdisciplinary team, but there is also a surprising lack of guidance available for this phase.

These tensions played out in discussions and revisiting of the provenance chain. By retaining provenance connections, our hope was that steps later viewed as undesirable could later be "un-done" and "re-done" a different way, as future users desired. Yet it was difficult to maintain provenance data in total, and its value was questionable.  

In particular, this provenance seems low value, since aspects of the underlying human process of annotation, such as disagreement, change, turnover, copious negative examples, are opaque to the machine learning algorithms and indeed to the genre of machine learning dataset presentations. Conversely, the impact of disagreements or conceptual complexity shows up as lack of performance in the machine learning system, and so has immediate salience.  

Take, for example, the decision to make more consistent the "version" field, by removing abbreviations and words, such as "version", "ver.", "v", but to retain "non-number" characters within the version number (e.g., "3.14.5"). Those trained in qualitative analysis might argue that these are key signifiers, which were clearly important to some annotators, perhaps those less conversant with versioning number schemes and thus likely to be from more diverse backgrounds. Those trained in machine learning see this as confusing lack of concision, probably due to inadequate training, which may confuse the machine.  Ultimately, we decided that our goal was not to capture human diversity, but to recognize software mentions, and were comforted by the fact that these signifiers were retained in the dataset, and available to sequential language models, in that they reside next to the annotated portion.

In particular these differing interdisciplinary perspectives on the value of provenance play out in seemingly innocuous serialization decisions. In the shift to a TEI XML representation our first version presented the dataset as it would be read by a straight-forward machine learning system: text and annotations. Identifiers for each annotation were internal within the TEI XML document, but not connected to the provenance information (in our case the csv dataset and, ultimately, the RDF representation and thus the github log records). In our final published dataset we have retained those indicators, enabling re-connection. Nonetheless we had numerous discussions about how much provenance to "bring forward" into the final published artifact. Some of these decisions were ultimately driven by the serialization. For example, we recorded "certainty" of annotations by annotators, an attribute rarely used in machine learning (although becoming more common XXXX cite VizWiz). Yet to represent that in our final dataset, designed for maximal usability by current machine learning tools, was complicated. It would require displaying original annotations, in some cases by multiple annotators, and possibly altered by the "curator". Such a representation is particularly complex in XML, requiring overlapping annotations or changing the representation from "text with annotations" to "annotation events" (ie repetition of the underlying text), closer to our RDF and CSV representations. Each decision had to be weighed across disciplinary perspectives and likely usability of the dataset. In particular we did not want users to have to conduct unfamiliar and error prone pre-processing of the dataset to obtain something that machine learning tools could consume. Thus we opted to use identifiers to enable provenance discovery by those interested while presenting a usable and (in our judgement credible) dataset. This decision, though, removes the salience of provenance and probably makes it less likely to be pursued by future users.

## Provenance vs usefulness

Throughout the project we worked hard to maintain provenance of the dataset, from recording who annotated which articles when, to maintaining version control on the coding scheme, to conducting conversations about approaches and reliability using archived github issues. Also throughout there was clear tension about this process; in short it is hard to anticipate future uses of this provenance data. And so the classic, well-known, situation of metadata comes to the fore: the collection of provenance data is expensive and time-consuming, reducing the ultimate size of the dataset produced and slowing its publication. (XXXX we will need some citations here, perhaps Ribes and Polk, 2015 is a starting point)

Similarly, recording provenance involves the display of the "back stage" (Goffman) of research activity, the messy "in flight" creation and waves of review and (hoped for) improvement. Even in writing the drafts of this paper, the more revealing of process, the more we felt we were undermining the confidence a reader would have in the quality of the dataset. Thus documenting and describing process is not only time-consuming, but may raise more questions about the process than it answers. This is certainly true compared to the typical genre of machine learning dataset papers, which tend to report the end results only.

Additional tensions derived from working in the open, using a public github repository. In addition to showing the warts and all history of collection and discussion, openness meant that our dataset was available much sooner than it would otherwise be. It fact the repository was cited before there was an available publication. Particularly as we worked out the documentation of provenance, this meant that others could legitimately take the dataset and use it for supervised machine learning, "scooping" the results. We managed this intension internally by reminding ourselves that having others use the dataset was the ultimate aim of the project, and therefore inviting use (while warning of "in progress"). Nonetheless the fear of "being scooped" persisted, generating additional time pressure during the documentation of provenance, and working through the interdisciplinary issues described above.

Finally, working in the open on github means that the annotation work of the students employed on the project could be traced to their personal identities. The students were aware of this during employment, but it was a condition of employment, so whether such open participation was sufficiently voluntary is debatable. It is hard to anticipate negative consequences for annotators, but that is often the case when considering privacy of activity. In the published dataset we have anonymized the annotation identifiers, but each are traceable through the provenance identifiers, back through to the github history.

# Conclusion

In this paper we describe a dataset of software mentions in full-text publications extracted from PDFs. The dataset is designed to facilitate a community effort of supervised machine learning named entity extraction of software in academic publications, and ultimately to help in improving the visibility of software contributions. The dataset itself is many times larger than those previously available, well aligned to extraction of useable text from academic PDFs, and constructed in a way to help full evaluation of shortcut techniques to expanding the collection further.

In addition to a standard genre presentation of the dataset, we provide a detailed discussion of the process of dataset creation, to provide provenance for the dataset and to provide insights for others creating datasets for supervised machine learning. In particular we highlight interdisciplinary tensions between machine learning and qualitative research traditions, revealing our "back stage" to help improve methods and practices in this increasingly important research approach.
