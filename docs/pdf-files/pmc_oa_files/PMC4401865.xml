<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.0 20120330//EN" "JATS-archivearticle1.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">BMJ Open</journal-id><journal-id journal-id-type="iso-abbrev">BMJ Open</journal-id><journal-id journal-id-type="hwp">bmjopen</journal-id><journal-id journal-id-type="publisher-id">bmjopen</journal-id><journal-title-group><journal-title>BMJ Open</journal-title></journal-title-group><issn pub-type="epub">2044-6055</issn><publisher><publisher-name>BMJ Publishing Group</publisher-name><publisher-loc>BMA House, Tavistock Square, London, WC1H 9JR</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">25888476</article-id><article-id pub-id-type="pmc">4401865</article-id><article-id pub-id-type="publisher-id">bmjopen-2014-007170</article-id><article-id pub-id-type="doi">10.1136/bmjopen-2014-007170</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Methods</subject><subj-group><subject>Research</subject></subj-group></subj-group><subj-group subj-group-type="hwp-journal-coll"><subject>1506</subject><subject>1730</subject><subject>1730</subject><subject>1697</subject><subject>1692</subject></subj-group></article-categories><title-group><article-title>One-step extrapolation of the prediction performance of a gene signature derived from a small study</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Wang</surname><given-names>Ling-Yi</given-names></name><xref ref-type="aff" rid="af1">1</xref><xref ref-type="aff" rid="af2">2</xref></contrib><contrib contrib-type="author"><name><surname>Lee</surname><given-names>Wen-Chung</given-names></name><xref ref-type="aff" rid="af1">1</xref></contrib></contrib-group><aff id="af1"><label>1</label><institution>Research Center for Genes, Environment and Human Health, and Institute of Epidemiology and Preventive Medicine, College of Public Health, National Taiwan University</institution>, <addr-line>Taipei</addr-line>, <country>Taiwan</country></aff><aff id="af2"><label>2</label><addr-line>Department of Medical Research</addr-line>, <institution>Tzu Chi General Hospital</institution>, <addr-line>Hualien</addr-line>, <country>Taiwan</country></aff><author-notes><corresp><label>Correspondence to</label> Professor Wen-Chung Lee; <email>wenchung@ntu.edu.tw</email></corresp></author-notes><pub-date pub-type="collection"><year>2015</year></pub-date><pub-date pub-type="epub"><day>17</day><month>4</month><year>2015</year></pub-date><volume>5</volume><issue>4</issue><elocation-id>e007170</elocation-id><history><date date-type="received"><day>14</day><month>11</month><year>2014</year></date><date date-type="rev-recd"><day>4</day><month>3</month><year>2015</year></date><date date-type="accepted"><day>20</day><month>3</month><year>2015</year></date></history><permissions><copyright-statement>Published by the BMJ Publishing Group Limited. For permission to use (where not already granted under a licence) please go to http://group.bmj.com/group/rights-licensing/permissions</copyright-statement><copyright-year>2015</copyright-year><license license-type="open-access"><license-p>This is an Open Access article distributed in accordance with the Creative Commons Attribution Non Commercial (CC BY-NC 4.0) license, which permits others to distribute, remix, adapt, build upon this work non-commercially, and license their derivative works on different terms, provided the original work is properly cited and the use is non-commercial. See: <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by-nc/4.0/">http://creativecommons.org/licenses/by-nc/4.0/</ext-link></license-p></license></permissions><self-uri xlink:title="pdf" xlink:href="bmjopen-2014-007170.pdf"/><abstract><sec><title>Objective</title><p>Microarray-related studies often involve a very large number of genes and small sample size. Cross-validating or bootstrapping is therefore imperative to obtain a fair assessment of the prediction/classification performance of a gene signature. A deficiency of these methods is the reduced training sample size because of the partition process in cross-validation and sampling with replacement in bootstrapping. To address this problem, we aim to obtain a prediction performance estimate that strikes a good balance between bias and variance and has a small root mean squared error.</p></sec><sec><title>Methods</title><p>We propose to make a one-step extrapolation from the fitted learning curve to estimate the prediction/classification performance of the model trained by all the samples.</p></sec><sec><title>Results</title><p>Simulation studies show that the method strikes a good balance between bias and variance and has a small root mean squared error. Three microarray data sets are used for demonstration.</p></sec><sec><title>Conclusions</title><p>Our method is advocated to estimate the prediction performance of a gene signature derived from a small study.</p></sec></abstract><kwd-group><kwd>gene signature</kwd><kwd>prediction</kwd><kwd>classification</kwd><kwd>receiver operating characteristic curve</kwd><kwd>microarray</kwd><kwd>learning curve</kwd></kwd-group></article-meta></front><body><boxed-text position="float" orientation="portrait"><caption><title>Strengths and limitations of this study</title></caption><list list-type="bullet"><list-item><p>The proposed method estimates the prediction performance of a gene signature derived from a small study.</p></list-item><list-item><p>The proposed method strikes a good balance between bias and variance and has a small root mean squared error.</p></list-item><list-item><p>The proposed method can be applied to linear and non-linear prediction models.</p></list-item><list-item><p>The proposed method may not work well for studies with an extremely small sample size (eg, n&#x0003c;10).</p></list-item></list></boxed-text><sec sec-type="intro" id="s1"><title>Introduction</title><p>With the advances in microarray technology, hundreds of thousands of genes with expression information on an individual can be obtained in a single experiment. This high throughput technology enables us to make diagnostic and prognostic predictions based on a participant's gene signature.<xref rid="R1" ref-type="bibr">1&#x02013;14</xref> There are four key steps in microarray-based studies: (1) data processing (eg, data normalisation), (2) gene selection, (3) prediction model construction and (4) prediction performance evaluation.<xref rid="R15" ref-type="bibr">15</xref> This paper focuses on the last step, the evaluation of prediction performances.</p><p>Microarray-based studies often involve a very large number of genes and a relatively small sample size. The same small data set being used for constructing the prediction model and subsequently evaluating the prediction performance tends to give over-optimistic estimates. This is why cross-validating or bootstrapping is imperative if a fair assessment of the prediction/classification performance of a gene signature is to be made. Popular cross-validation (CV) methods are k-fold CV, Monte Carlo CV and leave-one-out CV (LOOCV, also known as jackknifing).<xref rid="R16" ref-type="bibr">16</xref> These methods partition the original data into a training set and a testing set. The training sample size, therefore, is reduced. The bootstrap method is an alternative to CV that operates by sampling with replacement of the original data.<xref rid="R17" ref-type="bibr">17</xref>
<xref rid="R18" ref-type="bibr">18</xref> Even though the bootstrap sample has the same sample size as the original one, the overlapping of subjects between the bootstrap sample and the original data still reduces the effective (non-overlapping) training sample size. The reduced training sample size will curtail the prediction/classification performance of a gene signature, especially when the sample size of a study is already small.<xref rid="R19" ref-type="bibr">19</xref></p><p>Estimating the performance of a prediction model built from a small study is a vexing task. For CV purposes, the already small sample size still needs to be partitioned further into a training set and a testing one. If we make the training size as large as possible (such as LOOCV), it would be nearly unbiased, but the effective sample size left for validation is one subject and we would get a variance that is unduly large, that is, the notorious bias&#x02013;variance dilemma.<xref rid="R20" ref-type="bibr">20</xref>
<xref rid="R21" ref-type="bibr">21</xref></p><p>The accuracy rate, the error rate and the area under the receiver operating characteristic curve (AUC) are commonly used performance indicators of a prediction model for a binary outcome.<xref rid="R15" ref-type="bibr">15</xref>
<xref rid="R22" ref-type="bibr">22</xref> In this paper, we focus on the AUC index because it evaluates the global performance of a prediction model, not just at a particular cut-off point, but for each and every possible cut-off point. In a small data set, each and every subject is indispensable. We propose to make a one-step extrapolation from the fitted learning curve for AUC to estimate the prediction/classification performance of the model trained by all the samples.</p></sec><sec sec-type="methods" id="s2"><title>Methods</title><sec id="s2a"><title>Monte Carlo CVs</title><p>Suppose that a given prediction model is to be evaluated based on a data set with a total of N<sub>1</sub> cases (or individuals with adverse events) and N<sub>0</sub> controls (or individuals without adverse events). First, we use CVs with five different folds (LOOCV, 10-fold, 5-fold, 3-fold and 2-fold CV, respectively) to evaluate the performances of the prediction model. (LOOCV is the largest training size possible and the 2-fold CV is the smallest. Between these two extremes, we add three additional fold numbers. More folds can also be tried, but the results are similar.) To be more precise, we use the Monte Carlo random partition to partition the data set into two parts: the training set and the testing set. The random partitions are operated separately for the case group and the control group to ensure that the number of cases and controls is as balanced as possible;<xref rid="R23" ref-type="bibr">23</xref>
<xref rid="R24" ref-type="bibr">24</xref> the training set has a total of <inline-formula><inline-graphic xlink:href="bmjopen2014007170ileq1.jpg"/></inline-formula> cases and <inline-formula><inline-graphic xlink:href="bmjopen2014007170ileq2.jpg"/></inline-formula> controls (both to the nearest integers), where <inline-formula><inline-graphic xlink:href="bmjopen2014007170ileq3.jpg"/></inline-formula> for LOOCV, 10-fold, 5-fold, 3-fold and 2-fold CVs, respectively.</p><p>To be representative of all possible partitions, we suggest running at least 100 Monte Carlo random partitions for all folds, although this may be superfluous for some situations. (For example, there are only <inline-formula><inline-graphic xlink:href="bmjopen2014007170ileq4.jpg"/></inline-formula> distinctive partitions for LOOCV with <inline-formula><inline-graphic xlink:href="bmjopen2014007170ileq5.jpg"/></inline-formula>. Note that in a random partition, we leave out &#x02018;one pair&#x02019; of a case and a control, instead of &#x02018;one subject&#x02019;.<xref rid="R24" ref-type="bibr">24</xref>) For each Monte Carlo partition, a prediction model will be built from the training set, and the testing set used to evaluate the performance of this model. The AUCs under the same fold are to be averaged (denoted as <inline-formula><inline-graphic xlink:href="bmjopen2014007170ileq6.jpg"/></inline-formula>), which leaves us with a total of five <inline-formula><inline-graphic xlink:href="bmjopen2014007170ileq7.jpg"/></inline-formula>s.</p></sec><sec id="s2b"><title>Learning curve for AUC</title><p>A learning curve is an assumed functional relation between prediction performance and training sample size.<xref rid="R25" ref-type="bibr">25</xref> In this study, we take <inline-formula><inline-graphic xlink:href="bmjopen2014007170ileq8.jpg"/></inline-formula> as our learning curve, where <inline-formula><inline-graphic xlink:href="bmjopen2014007170ileq9.jpg"/></inline-formula> and <inline-formula><inline-graphic xlink:href="bmjopen2014007170ileq10.jpg"/></inline-formula>. Essentially, this learning curve is a straight line in a double-inverse coordinate. For the ordinate, the AUC values are first transformed to quantiles of standard normal distribution, then squared and finally inversed. This expands the range of 0.5&#x02013;1.0 in an AUC to a range of <inline-formula><inline-graphic xlink:href="bmjopen2014007170ileq11.jpg"/></inline-formula> in the ordinate. For the abscissa, the range of the sample size is also between 0 and &#x0221e;. Such a sample size in inverse should be more sensitive to changes when the original sample size is small. The online supplementary appendix 1 shows that this learning curve is the exact functional relation between prediction performance and training sample size when normality and independence of the data are assumed.</p></sec><sec id="s2c"><title>One-step extrapolation from the learning curve</title><p>We calculate <inline-formula><inline-graphic xlink:href="bmjopen2014007170ileq12.jpg"/></inline-formula> and <inline-formula><inline-graphic xlink:href="bmjopen2014007170ileq13.jpg"/></inline-formula> for each fold. On the basis of the five coordinate points, (x, y)s, we draw a linear regression line, that is, <inline-formula><inline-graphic xlink:href="bmjopen2014007170ileq14.jpg"/></inline-formula> To extrapolate the performance (denoted as AUC<sub>T</sub>) of the prediction model when all the subjects <inline-formula><inline-graphic xlink:href="bmjopen2014007170ileq15.jpg"/></inline-formula>(<inline-formula><inline-graphic xlink:href="bmjopen2014007170ileq16.jpg"/></inline-formula>) are used as training samples, we enter <inline-formula><inline-graphic xlink:href="bmjopen2014007170ileq17.jpg"/></inline-formula> into the equation to get <inline-formula><inline-graphic xlink:href="bmjopen2014007170ileq18.jpg"/></inline-formula>.</p><p>With <inline-formula><inline-graphic xlink:href="bmjopen2014007170ileq19.jpg"/></inline-formula>calculated, we then obtain <inline-formula><inline-graphic xlink:href="bmjopen2014007170ileq20.jpg"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="bmjopen2014007170ileq21.jpg"/></inline-formula> is the cumulative distribution for the standard normal.</p></sec></sec><sec id="s3"><title>Simulation studies</title><sec id="s3a"><title>Data and prediction models</title><p>We consider four different sample sizes: <inline-formula><inline-graphic xlink:href="bmjopen2014007170ileq22.jpg"/></inline-formula> (cases)+10 (controls), 15+15, 20+20 and 25+25, respectively. A total of 10 genes are considered. The gene expression levels are generated from a normal distribution with a variance of 1. For the cases, the means of the gene expressions are distributed as uniform (&#x02212;0.8, 0.8); for the controls, the means are set to 0.</p><p>Four different data structures are considered. The first three are normally distributed with a correlation coefficient of 0 (independence), 0.2 and 0.5 (dependency). The last data structure is more complicated. For the cases, the expression level of each gene is distributed as a mixture of three normal distributions with variances of 1. The three means are generated from a uniform (&#x02212;1.5, 1.5) distribution with a probability of 0.6, a uniform (&#x02212;1.2, 1.2) distribution with a probability of 0.3 and a uniform (&#x02212;1, 1) distribution with a probability of 0.1, respectively (see online supplementary appendix 2 for this non-normal distribution). The gene expression level for the controls follows the standard normal distribution. The correlation coefficient between any two genes is set at 0.5 in these complex data.</p><p>There are many methods to build prediction models. In this paper, we use the na&#x000ef;ve multiple regression and the support vector machine (SVM), as detailed below, to build prediction models. Another machine learning method, the random forest (RF), is detailed in online supplementary materials.</p><p>The na&#x000ef;ve multiple regression is a simple prediction method. First, the &#x003b2;-coefficients (<inline-formula><inline-graphic xlink:href="bmjopen2014007170ileq23.jpg"/></inline-formula> where p is the number of genes in the gene signature) are calculated as the mean expression difference for each gene between the case and the control groups in the training set. The prediction score of the na&#x000ef;ve multiple regression for the jth subject in the testing set is then <inline-formula><inline-graphic xlink:href="bmjopen2014007170ileq24.jpg"/></inline-formula>, where &#x003c7;<sub>ij</sub> is the observed gene expression level of the ith gene for this jth subject. (The na&#x000ef;ve multiple regression used in this study is similar to a previously proposed compound covariate method<xref rid="R26" ref-type="bibr">26</xref> where the prediction score for the jth subject in the testing set is <inline-formula><inline-graphic xlink:href="bmjopen2014007170ileq25.jpg"/></inline-formula> with the two-sample t-statistic of each gene serving its own weight in the prediction model).</p><p>SVM is a more sophisticated method; it is a very efficient learning algorithm for high-dimensional data in classification, regression and pattern recognition. The basis of SVM is to implicitly map data to a higher dimensional space via a kernel function in order to identify an optimal hyperplane that maximises the margin between the two groups.<xref rid="R27" ref-type="bibr">27</xref> There are many software packages available to implement SVM. In this study, we use the e1071-package of R with a default radial basis function kernel to obtain the prediction scores.<xref rid="R28" ref-type="bibr">28</xref></p></sec><sec id="s3b"><title>CVs of the prediction models</title><p>In our simulation study, we perform a total of 5000 simulations. In each simulation, a total of 100 random partitions are performed for each fold CV (LOOCV, 10-fold, 5-fold, 3-fold and 2-fold CVs, respectively). From these, we use the previously described learning curve to make a one-step extrapolation to the cross-validated AUC when all the samples are utilised to train the model. For a comparison, we also calculate the internally validated AUCs of the LOO bootstrap in each simulation. This is a modified bootstrap procedure of the ordinary bootstrap. We draw a total of 100 resamplings. At each draw, the observations left out serve as the testing set. The effective (non-overlapping) training sample size of the LOO bootstrap is around 63.2% of the total sample size.<xref rid="R17" ref-type="bibr">17</xref>
<xref rid="R18" ref-type="bibr">18</xref> (Out-of-bag (OOB)<xref rid="R29" ref-type="bibr">29</xref> estimation employs a majority vote on the multiple prediction made for observation i based on the bootstrap samples at each draw, while the LOO bootstrap takes an average on error of these predictions. Therefore, OOB estimation may have larger variability than the LOO bootstrap when the sample size is small.<xref rid="R30" ref-type="bibr">30</xref>) In the simulation, we additionally create a large data set of 1000 cases and 1000 controls for external validation. For a prediction model, an externally validated AUC against this data set is considered as its true AUC value (one true AUC for each round of simulation). It should be pointed out that in real practice, one rarely has the luxury to conduct such a large-scale external validation, but will often have to settle for a satisfactory internal validation method which is precisely the focal point of this paper.</p></sec><sec id="s3c"><title>Bias, variance and root mean squared error</title><p>In each round of the simulation, we calculate an estimated AUC, an error (the difference between the estimated AUC and the true AUC) and an error square for each performance evaluation method. On the basis of the 5000 simulations, the bias is calculated as the sample mean of the errors; the variance is the sample variance of the estimated AUCs; and the mean squared error (MSE) is the sample mean of the error squares. Finally, the root mean squared error (RMSE) is calculated from the square root of MSE. (RMSE simultaneously considers bias and variance. This value represents the &#x02018;average&#x02019; (root-mean-square average, to be precise) difference between the estimated AUC and the true AUC).</p></sec><sec id="s3d"><title>Simulation results</title><p>In <xref ref-type="fig" rid="BMJOPEN2014007170F1">figure 1</xref>, we present the bias (panels A&#x02013;D), the variance (panels E&#x02013;H) and the RMSE (panels I&#x02013;L), respectively, using na&#x000ef;ve multiple regression under different sample sizes. When the variables are independently distributed (panels A, E and I), the bias becomes smaller as the sample size becomes larger (closer to zero; panel A). All the fold-based CV methods underestimate the true AUC value because the training sample sizes they use are smaller than the total sample size given. The training sample size of LOOCV is closest to the total sample size (total sample size minus one pair); hence, it is the least biased (blue line) among all the fold-based CV methods. The training sample size of the LOO bootstrap is about 63% of the total sample size,<xref rid="R17" ref-type="bibr">17</xref>
<xref rid="R18" ref-type="bibr">18</xref> which makes its bias (black dashed line) comparable to that of the twofold CV (with 50% of the total sample size; green line). As for the bias of our extrapolation method (red line), it is comparable to that of LOOCV.</p><fig id="BMJOPEN2014007170F1" orientation="portrait" position="float"><label>Figure&#x000a0;1</label><caption><p>Bias, variance and root mean squared error (RMSE) of the various methods under different sample sizes when the na&#x000ef;ve multiple regression is used to build the gene signature leave-one-out cross-validation (blue line), fivefold cross-validation (yellow line), twofold cross-validation (green line), leave-one-out bootstrap (black dashed line) and the proposed method (red line). The leftmost column of panels is for normally distributed data with a correlation coefficient of 0, the second column from left with a correlation coefficient of 0.2, and the third column from left with a correlation coefficient of 0.5. The rightmost column of panels is for complex data (mixture of normal distributions). The horizontal thin lines indicate a position of no bias.</p></caption><graphic xlink:href="bmjopen2014007170f01"/></fig><p>In <xref ref-type="fig" rid="BMJOPEN2014007170F1">figure 1</xref>E, we see that the variance reveals a different story; the LOOCV now has the largest variance, and the twofold CV has the smallest variance among the fold-based CV methods. In terms of variance, the extrapolation method is now comparable to the twofold CV. From the RMSE index (<xref ref-type="fig" rid="BMJOPEN2014007170F1">figure 1</xref>I), we see that the proposed extrapolation method strikes a good balance between the bias and variance.</p><p>Similar results can be found when the variables are correlated (panels B, F and J, with a correlation coefficient of 0.2; panels C, G and K, with a correlation coefficient of 0.5) and when they are not normally distributed (panels D, H and L), or when SVM (<xref ref-type="fig" rid="BMJOPEN2014007170F2">figure 2</xref>) is used for constructing prediction models.</p><fig id="BMJOPEN2014007170F2" orientation="portrait" position="float"><label>Figure&#x000a0;2</label><caption><p>Bias, variance and root mean squared error (RMSE) of the various methods under different sample sizes when the support vector machine is used to build the gene signature leave-one-out cross-validation (blue line), fivefold cross-validation (yellow line), twofold cross-validation (green line), leave-one-out bootstrap (black dashed line) and the proposed method (red line). The leftmost column of panels is for normally distributed data with a correlation coefficient of 0, the second column from left with a correlation coefficient of 0.2, and the third column from left with a correlation coefficient of 0.5. The rightmost column of panels is for complex data (mixture of normal distributions). The horizontal thin lines indicate a position of no bias.</p></caption><graphic xlink:href="bmjopen2014007170f02"/></fig><p>We simulated a more substantially non-normal data set (see online supplementary appendix 3). We found that the proposed extrapolation method can still strike a good balance between bias and variance (see online supplementary appendix 4). In addition, we examined the performances of the 0.632 bootstrap<xref rid="R17" ref-type="bibr">17</xref> and the 0.632+ bootstrap,<xref rid="R31" ref-type="bibr">31</xref> both of which are weighted averages between the LOO bootstrap estimate and the resubstitution estimate. (The 0.632+ bootstrap is an improved version of the 0.632 bootstrap.) We found that the 0.632 bootstrap produces very large upward biases while the 0.632+ bootstrap is quite comparable to our method (see online supplementary appendix 5). We also see that the proposed extrapolation method can outperform the 0.632+ bootstrap in terms of RMSE when sample size <inline-formula><inline-graphic xlink:href="bmjopen2014007170ileq26.jpg"/></inline-formula> (online supplementary appendix 6).</p><p>We also tried extrapolation based on different learning curves (a linear equation <inline-formula><inline-graphic xlink:href="bmjopen2014007170ileq27.jpg"/></inline-formula> with <inline-formula><inline-graphic xlink:href="bmjopen2014007170ileq28.jpg"/></inline-formula> and <inline-formula><inline-graphic xlink:href="bmjopen2014007170ileq29.jpg"/></inline-formula>, and a quadratic equation <inline-formula><inline-graphic xlink:href="bmjopen2014007170ileq30.jpg"/></inline-formula> with <inline-formula><inline-graphic xlink:href="bmjopen2014007170ileq31.jpg"/></inline-formula> and <inline-formula><inline-graphic xlink:href="bmjopen2014007170ileq32.jpg"/></inline-formula>), but we found the results to be no better than using the learning curve in this paper (see online supplementary appendices 7 and 8).</p></sec></sec><sec id="s4"><title>Real data demonstration</title><p>We take three microarray data sets to demonstrate how the extrapolation method can be applied step by step.<xref rid="R32" ref-type="bibr">32&#x02013;34</xref> As this paper focuses on prediction model evaluation, and not on gene selection, we conveniently construct a 10-gene signature based on the top 10 genes with the smallest Mann-Whitney U test p values for the first two data sets, respectively. In the last example, we directly take the 76-gene signature identified by the original study as the prediction model. Both na&#x000ef;ve multiple regression and SVM are used to build the prediction model for each data set. Monte Carlo random partition (a total of 1000 partitions for each fold) is performed to obtain the cross-validated AUCs.</p><sec id="s4a"><title>Example 1</title><p>The first data set is colon cancer data.<xref rid="R32" ref-type="bibr">32</xref> The data consist of 2000 gene expressions in 62 tissue samples (40 tumour and 22 normal colon tissue samples). The data are available at <ext-link ext-link-type="uri" xlink:href="http://genomics-pubs.princeton.edu/oncology/">http://genomics-pubs.princeton.edu/oncology/</ext-link>. The gene expression level is presented in intensity value, and is otherwise unprocessed. Hence, we first normalise the data by the mean and SD of each gene. The data are then randomly divided into two parts, one for gene selection (28 tumour tissue samples and 10 normal colon tissue samples) and the other for model building and CV (12 tumour tissue samples and 12 normal colon tissue samples). In the gene selection data set, we use the Mann-Whitney U test to identify the top 10 genes with the smallest p value from among the 2000 genes. These are <italic>Hsa.627_M26383</italic>, <italic>Hsa.6814_H08393</italic>, <italic>Hsa.37937_R87126</italic>, <italic>Hsa.692_M76378-3</italic>, <italic>Hsa.3016_T47377</italic>, <italic>Hsa.31630_R64115</italic>, <italic>Hsa.831_M22382</italic>, <italic>Hsa.36689_Z50753</italic>, <italic>Hsa.3331_T86473</italic> and <italic>Hsa.43279_H64489</italic>. In the remaining data set, we build and cross-validate a prediction model for this 10-gene signature.</p><p>For na&#x000ef;ve multiple regression, the <inline-formula><inline-graphic xlink:href="bmjopen2014007170ileq33.jpg"/></inline-formula>s (averaged from 1000 Monte Carlo partitions) are 0.936 (LOOCV), 0.929 (10-fold CV), 0.928 (5-fold CV), 0.925 (3-fold CV) and 0.921 (2-fold CV), respectively. The (x, y)s are then calculated as: <inline-formula><inline-graphic xlink:href="bmjopen2014007170ileq34.jpg"/></inline-formula>=(0.182, 0.432) for LOOCV, (0.200, 0.465) for 10-fold CV, (0.220, 0.466) for 5-fold CV, (0.250, 0.483) for 3-fold CV and (0.330, 0.503) for 2-fold CV, respectively. These results are plotted in <xref ref-type="fig" rid="BMJOPEN2014007170F3">figure 3</xref>A. We then draw a linear regression based on the five (x, y) points: <inline-formula><inline-graphic xlink:href="bmjopen2014007170ileq35.jpg"/></inline-formula> (the red line in <xref ref-type="fig" rid="BMJOPEN2014007170F3">figure 3</xref>A). To predict the performance with a sample size of 24 (all samples in the model building and CV data set are used as the training set, ie, 12 tumour and 12 normal tissue samples), we enter <inline-formula><inline-graphic xlink:href="bmjopen2014007170ileq36.jpg"/></inline-formula> into the regression equation to get <inline-formula><inline-graphic xlink:href="bmjopen2014007170ileq37.jpg"/></inline-formula> (* in <xref ref-type="fig" rid="BMJOPEN2014007170F3">figure 3</xref>A). The extrapolated performance is therefore <inline-formula><inline-graphic xlink:href="bmjopen2014007170ileq38.jpg"/></inline-formula>. We next perform a total of 100 bootstrapping for this example and the bootstrapped SE for <inline-formula><inline-graphic xlink:href="bmjopen2014007170ileq39.jpg"/></inline-formula> is calculated as 0.080. The results for this example when SVM is used for constructing the prediction model are shown in <xref ref-type="fig" rid="BMJOPEN2014007170F3">figure 3</xref>B. The <inline-formula><inline-graphic xlink:href="bmjopen2014007170ileq40.jpg"/></inline-formula> (&#x000b1; bootstrapped SE) is calculated as 0.940 (&#x000b1;0.079).</p><fig id="BMJOPEN2014007170F3" orientation="portrait" position="float"><label>Figure&#x000a0;3</label><caption><p>Demonstration of the three microarray data using the proposed extrapolation method. In this double-inverse coordinate system, the ordinate is the inverse of the (transformed) AUC value, and the abscissa is the inverse of the training sample size. The red, blue and green lines represent the colon cancer data (example 1), the GSE2990 breast cancer data (example 2) and the GSE2034 breast cancer data (example 3), respectively. The five dots along each line are the estimates of the five different fold CV and the * is the extrapolated AUC (AUC, area under the receiver operating characteristic curve; CV, cross-validation).</p></caption><graphic xlink:href="bmjopen2014007170f03"/></fig></sec><sec id="s4b"><title>Example 2</title><p>The second example is a breast cancer data set,<xref rid="R33" ref-type="bibr">33</xref> which is available at the Gene Expression Omnibus database (<ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/geo">http://www.ncbi.nlm.nih.gov/geo</ext-link>), with accession code GSE2990. These data consist of 22&#x02005;215 genes for 189 patients with breast cancer (120 patients without relapse and 67 with relapse; 2 patients with unknown relapse status are omitted from our demonstration). The provided data set has already been processed (with background correction, quantile normalisation and log transformation). The data set details and patient profile can be found in the corresponding reference and aforementioned GEO website. We choose those &#x02018;extreme&#x02019; patients to be in the gene selection data set, that is, those 43 patients who developed relapse within 5&#x02005;years and those 91 patients who were free of relapse for at least 5&#x02005;years. The remaining data set (for model building and CV) now consists of 67&#x02212;43=24&#x000a0;patients who developed relapses after 5&#x02005;years and 120&#x02212;91=29&#x000a0;patients free of relapses during their less than 5-year follow-up periods.</p><p>In the gene selection data set, we again pick the top 10 genes from among the 22&#x02005;215 genes with the smallest p value after Mann-Whitney U test. These 10 genes are <italic>203213_at</italic>, <italic>210222_s_at</italic>, <italic>205898_at</italic>, <italic>218883_s_at</italic>, <italic>203485_at</italic>, <italic>201890_at</italic>, <italic>214710_s_at</italic>, <italic>202779_s_at</italic>, <italic>202503_s_at</italic> and <italic>201291_s_at</italic>. The remaining data set is used to build and cross-validate the prediction model for this 10-gene signature.</p><p>The results for na&#x000ef;ve multiple regression are presented in <xref ref-type="fig" rid="BMJOPEN2014007170F3">figure 3</xref>A (blue line). The one-step extrapolated AUC (&#x000b1; bootstrappedSE) is calculated as 0.803 (&#x000b1;0.082). The results for SVM are presented in <xref ref-type="fig" rid="BMJOPEN2014007170F3">figure 3</xref>B (blue line). The one-step extrapolated AUC (&#x000b1; bootstrappedSE) is calculated as 0.781 (&#x000b1;0.063).</p></sec><sec id="s4c"><title>Example 3</title><p>The third example is a different breast cancer data set. The data set<xref rid="R34" ref-type="bibr">34</xref> and its patient profile are available at the GEO database (<ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/geo">http://www.ncbi.nlm.nih.gov/geo</ext-link>) with accession code GSE2034. The data consisting of 107 patients with breast cancer with distant relapse and 197 without distant relapse) was divided into training (115 patients) and testing (171 patients) by concentration of the oestrogen receptor and a 76-gene signature was identified by previous researchers.<xref rid="R34" ref-type="bibr">34</xref> We use our method to estimate the prediction performance of this 76-gene signature.</p><p>The results for na&#x000ef;ve multiple regression are presented in <xref ref-type="fig" rid="BMJOPEN2014007170F3">figure 3</xref>A (green line). The one-step extrapolated AUC (&#x000b1; bootstrappedSE) is calculated as 0.726 (&#x000b1;0.005). The results for SVM are presented in <xref ref-type="fig" rid="BMJOPEN2014007170F3">figure 3</xref>B (green line). The one-step extrapolated AUC (&#x000b1; bootstrapped SE is calculated as 0.716 (&#x000b1;0.010).</p></sec></sec><sec sec-type="discussion" id="s5"><title>Discussion</title><p>When estimating the performance of a model derived from a small study, there seems to be no reason to settle for a sample size of <inline-formula><inline-graphic xlink:href="bmjopen2014007170ileq41.jpg"/></inline-formula> (or <inline-formula><inline-graphic xlink:href="bmjopen2014007170ileq42.jpg"/></inline-formula>, in a leave-one-pair-out CV), since what we are looking for is the performance at sample size <inline-formula><inline-graphic xlink:href="bmjopen2014007170ileq43.jpg"/></inline-formula>. In this study, we extrapolate the performance to <inline-formula><inline-graphic xlink:href="bmjopen2014007170ileq44.jpg"/></inline-formula> by exploiting the linear relation between <inline-formula><inline-graphic xlink:href="bmjopen2014007170ileq45.jpg"/></inline-formula> and <inline-formula><inline-graphic xlink:href="bmjopen2014007170ileq46.jpg"/></inline-formula>. The extrapolation is based on five CV methods (LOOCV, 10-fold, 5-fold, 3-fold and 2-fold CV) and is carried out only one-step ahead. The resulting estimate thus inherits the lack of bias in LOOCV and strikes a satisfying variance among the five CV methods. A computer simulation shows that our method performs the best in terms of RMSE when sample sizes are small.</p><p>The learning curve for AUC used in this study is based on a linear prediction model (online supplementary appendix 1). However, our simulation study shows that the learning curve is equally suited for non-linear prediction models, such as SVM (<xref ref-type="fig" rid="BMJOPEN2014007170F2">figure 2</xref>) and RF (online supplementary appendix 9). When making the extrapolation, we may sometimes encounter a slope (b in <inline-formula><inline-graphic xlink:href="bmjopen2014007170ileq47.jpg"/></inline-formula>) that is near zero (eg, the colon cancer example demonstrated in <xref ref-type="fig" rid="BMJOPEN2014007170F3">figure 3</xref>). This may occur when the model performance has reached its plateau; thus, varying the training size (as in different CV methods) has little effect on AUC estimates. This can also occur at the other extreme when the prediction/classification problem at hand is more complex and requires a much larger training size than is currently available, to significantly enhance the model performance. In either case, our method amounts to taking the average of the five CV estimates, thereby stabilising the variances.</p><p>Two previous studies<xref rid="R30" ref-type="bibr">30</xref>
<xref rid="R35" ref-type="bibr">35</xref> also exploited the extrapolation concept. Both used an inverse power-law model as the empirical learning curve. In this paper, we are only interested in how the performance will result if all the samples we have are utilised to train the model. We do need an equation (a learning curve) for extrapolation, but this requires extrapolating a mere one step ahead. Our results should therefore be less dependent on what learning curves are being used.</p></sec></body><back><ack><p>The authors would like to thank Dr Yung-Hsiang Huang and Mr Po-Chang Hsiao for technical supports.</p></ack><fn-group><fn><p><bold>Contributors:</bold> L-YW designed the simulation study and drafted the manuscript. W-CL conceived the study and participated in its design and coordination.</p></fn><fn><p><bold>Funding:</bold> Ministry of Science and Technology, Taiwan (grant number NSC 102-2628-B-002-036-MY3) and National Taiwan University, Taiwan (grant number NTU-CESRP-104R7622-8).</p></fn><fn><p><bold>Competing interests:</bold> None declared.</p></fn><fn><p><bold>Provenance and peer review:</bold> Not commissioned; externally peer reviewed.</p></fn><fn><p><bold>Data sharing statement:</bold> No additional data are available.</p></fn></fn-group><ref-list><title>References</title><ref id="R1"><label>1</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cai</surname><given-names>YD</given-names></name>, <name><surname>Huang</surname><given-names>T</given-names></name>, <name><surname>Feng</surname><given-names>KY</given-names></name><etal/></person-group>
<article-title>A unified 35-gene signature for both subtype classification and survival prediction in diffuse large B-cell lymphomas</article-title>. <source>PLoS ONE</source><year>2010</year>;<volume>5</volume>:<fpage>e12726</fpage>
<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pone.0012726">doi:10.1371/journal.pone.0012726</ext-link><pub-id pub-id-type="pmid">20856936</pub-id></mixed-citation></ref><ref id="R2"><label>2</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chibon</surname><given-names>F</given-names></name>, <name><surname>Lagarde</surname><given-names>P</given-names></name>, <name><surname>Salas</surname><given-names>S</given-names></name><etal/></person-group>
<article-title>Validated prediction of clinical outcome in sarcomas and multiple types of cancer on the basis of a gene expression signature related to genome complexity</article-title>. <source>Nat Med</source><year>2010</year>;<volume>16</volume>:<fpage>781</fpage>&#x02013;<lpage>7</lpage>
<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nm.2174">doi:10.1038/nm.2174</ext-link><pub-id pub-id-type="pmid">20581836</pub-id></mixed-citation></ref><ref id="R3"><label>3</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Levan</surname><given-names>K</given-names></name>, <name><surname>Partheen</surname><given-names>K</given-names></name>, <name><surname>Osterberg</surname><given-names>L</given-names></name><etal/></person-group>
<article-title>Identification of a gene expression signature for survival prediction in type I endometrial carcinoma</article-title>. <source>Gene Expr</source><year>2010</year>;<volume>14</volume>:<fpage>361</fpage>&#x02013;<lpage>70</lpage>
<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3727/105221610X12735213181242">doi:10.3727/105221610X12735213181242</ext-link><pub-id pub-id-type="pmid">20635577</pub-id></mixed-citation></ref><ref id="R4"><label>4</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roessler</surname><given-names>S</given-names></name>, <name><surname>Jia</surname><given-names>HL</given-names></name>, <name><surname>Budhu</surname><given-names>A</given-names></name><etal/></person-group>
<article-title>A unique metastasis gene signature enables prediction of tumor relapse in early-stage hepatocellular carcinoma patients</article-title>. <source>Cancer Res</source><year>2010</year>;<volume>70</volume>:<fpage>10202</fpage>&#x02013;<lpage>12</lpage>
<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1158/0008-5472.CAN-10-2607">doi:10.1158/0008-5472.CAN-10-2607</ext-link><pub-id pub-id-type="pmid">21159642</pub-id></mixed-citation></ref><ref id="R5"><label>5</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wan</surname><given-names>YW</given-names></name>, <name><surname>Sabbagh</surname><given-names>E</given-names></name>, <name><surname>Raese</surname><given-names>R</given-names></name><etal/></person-group>
<article-title>Hybrid models identified a 12-gene signature for lung cancer prognosis and chemoresponse prediction</article-title>. <source>PLoS ONE</source><year>2010</year>;<volume>5</volume>:<fpage>e12222</fpage>
<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pone.0012222">doi:10.1371/journal.pone.0012222</ext-link><pub-id pub-id-type="pmid">20808922</pub-id></mixed-citation></ref><ref id="R6"><label>6</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhu</surname><given-names>CQ</given-names></name>, <name><surname>Ding</surname><given-names>K</given-names></name>, <name><surname>Strumpf</surname><given-names>D</given-names></name><etal/></person-group>
<article-title>Prognostic and predictive gene signature for adjuvant chemotherapy in resected non-small-cell lung cancer</article-title>. <source>J Clin Oncol</source><year>2010</year>;<volume>28</volume>:<fpage>4417</fpage>&#x02013;<lpage>24</lpage>
<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1200/JCO.2009.26.4325">doi:10.1200/JCO.2009.26.4325</ext-link><pub-id pub-id-type="pmid">20823422</pub-id></mixed-citation></ref><ref id="R7"><label>7</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>DT</given-names></name>, <name><surname>Hsu</surname><given-names>YL</given-names></name>, <name><surname>Fulp</surname><given-names>WJ</given-names></name><etal/></person-group>
<article-title>Prognostic and predictive value of a malignancy-risk gene signature in early-stage non-small cell lung cancer</article-title>. <source>J Natl Cancer Inst</source><year>2011</year>;<volume>103</volume>:<fpage>1859</fpage>&#x02013;<lpage>70</lpage>
<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1093/jnci/djr420">doi:10.1093/jnci/djr420</ext-link><pub-id pub-id-type="pmid">22157961</pub-id></mixed-citation></ref><ref id="R8"><label>8</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Herold</surname><given-names>T</given-names></name>, <name><surname>Jurinovic</surname><given-names>V</given-names></name>, <name><surname>Metzeler</surname><given-names>KH</given-names></name><etal/></person-group>
<article-title>An eight-gene expression signature for the prediction of survival and time to treatment in chronic lymphocytic leukemia</article-title>. <source>Leukemia</source><year>2011</year>;<volume>25</volume>:<fpage>1639</fpage>&#x02013;<lpage>45</lpage>
<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/leu.2011.125">doi:10.1038/leu.2011.125</ext-link><pub-id pub-id-type="pmid">21625232</pub-id></mixed-citation></ref><ref id="R9"><label>9</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Minguez</surname><given-names>B</given-names></name>, <name><surname>Hoshida</surname><given-names>Y</given-names></name>, <name><surname>Villanueva</surname><given-names>A</given-names></name><etal/></person-group>
<article-title>Gene-expression signature of vascular invasion in hepatocellular carcinoma</article-title>. <source>J Hepatol</source><year>2011</year>;<volume>55</volume>:<fpage>1325</fpage>&#x02013;<lpage>31</lpage>
<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.jhep.2011.02.034">doi:10.1016/j.jhep.2011.02.034</ext-link><pub-id pub-id-type="pmid">21703203</pub-id></mixed-citation></ref><ref id="R10"><label>10</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Salazar</surname><given-names>R</given-names></name>, <name><surname>Roepman</surname><given-names>P</given-names></name>, <name><surname>Capella</surname><given-names>G</given-names></name><etal/></person-group>
<article-title>Gene expression signature to improve prognosis prediction of stage II and III colorectal cancer</article-title>. <source>J Clin Oncol</source><year>2011</year>;<volume>29</volume>:<fpage>17</fpage>&#x02013;<lpage>24</lpage>
<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1200/JCO.2010.30.1077">doi:10.1200/JCO.2010.30.1077</ext-link><pub-id pub-id-type="pmid">21098318</pub-id></mixed-citation></ref><ref id="R11"><label>11</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>DY</given-names></name>, <name><surname>Done</surname><given-names>SJ</given-names></name>, <name><surname>McCready</surname><given-names>DR</given-names></name><etal/></person-group>
<article-title>A new gene expression signature, the ClinicoMolecular Triad Classification, may improve prediction and prognostication of breast cancer at the time of diagnosis</article-title>. <source>Breast Cancer Res</source><year>2011</year>;<volume>13</volume>:<fpage>R92</fpage>
<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1186/bcr3017">doi:10.1186/bcr3017</ext-link><pub-id pub-id-type="pmid">21939527</pub-id></mixed-citation></ref><ref id="R12"><label>12</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xie</surname><given-names>Y</given-names></name>, <name><surname>Xiao</surname><given-names>G</given-names></name>, <name><surname>Coombes</surname><given-names>KR</given-names></name><etal/></person-group>
<article-title>Robust gene expression signature from formalin-fixed paraffin-embedded samples predicts prognosis of non-small-cell lung cancer patients</article-title>. <source>Clin Cancer Res</source><year>2011</year>;<volume>17</volume>:<fpage>5705</fpage>&#x02013;<lpage>14</lpage>
<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1158/1078-0432.CCR-11-0196">doi:10.1158/1078-0432.CCR-11-0196</ext-link><pub-id pub-id-type="pmid">21742808</pub-id></mixed-citation></ref><ref id="R13"><label>13</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Riester</surname><given-names>M</given-names></name>, <name><surname>Taylor</surname><given-names>JM</given-names></name>, <name><surname>Feifer</surname><given-names>A</given-names></name><etal/></person-group>
<article-title>Combination of a novel gene expression signature with a clinical nomogram improves the prediction of survival in high-risk bladder cancer</article-title>. <source>Clin Cancer Res</source><year>2012</year>;<volume>18</volume>:<fpage>1323</fpage>&#x02013;<lpage>33</lpage>
<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1158/1078-0432.CCR-11-2271">doi:10.1158/1078-0432.CCR-11-2271</ext-link><pub-id pub-id-type="pmid">22228636</pub-id></mixed-citation></ref><ref id="R14"><label>14</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schramm</surname><given-names>SJ</given-names></name>, <name><surname>Campain</surname><given-names>AE</given-names></name>, <name><surname>Scolyer</surname><given-names>RA</given-names></name><etal/></person-group>
<article-title>Review and cross-validation of gene expression signatures and melanoma prognosis</article-title>. <source>J Invest Dermatol</source><year>2012</year>;<volume>132</volume>:<fpage>274</fpage>&#x02013;<lpage>83</lpage>
<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/jid.2011.305">doi:10.1038/jid.2011.305</ext-link><pub-id pub-id-type="pmid">21956122</pub-id></mixed-citation></ref><ref id="R15"><label>15</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Simon</surname><given-names>R</given-names></name></person-group>
<article-title>Diagnostic and prognostic prediction using gene expression profiles in high-dimensional microarray data</article-title>. <source>Br J Cancer</source><year>2003</year>;<volume>89</volume>:<fpage>1599</fpage>&#x02013;<lpage>604</lpage>
<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/sj.bjc.6601326">doi:10.1038/sj.bjc.6601326</ext-link><pub-id pub-id-type="pmid">14583755</pub-id></mixed-citation></ref><ref id="R16"><label>16</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Molinaro</surname><given-names>AM</given-names></name>, <name><surname>Simon</surname><given-names>R</given-names></name>, <name><surname>Pfeiffer</surname><given-names>RM</given-names></name></person-group>
<article-title>Prediction error estimation: a comparison of resampling methods</article-title>. <source>Bioinformatics</source><year>2005</year>;<volume>21</volume>:<fpage>3301</fpage>&#x02013;<lpage>7</lpage>
<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1093/bioinformatics/bti499">doi:10.1093/bioinformatics/bti499</ext-link><pub-id pub-id-type="pmid">15905277</pub-id></mixed-citation></ref><ref id="R17"><label>17</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Efron</surname><given-names>B</given-names></name></person-group>
<article-title>Estimating the error rate of a prediction rule: improvement on cross-validation</article-title>. <source>J Am Stat Assoc</source><year>1983</year>;<volume>78</volume>:<fpage>316</fpage>&#x02013;<lpage>31</lpage>
<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1080/01621459.1983.10477973">doi:10.1080/01621459.1983.10477973</ext-link></mixed-citation></ref><ref id="R18"><label>18</label><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Efron</surname><given-names>B</given-names></name>, <name><surname>Tibshirani</surname><given-names>RJ</given-names></name></person-group>
<source>An introduction to the bootstrap</source>. <publisher-name>CRC Press</publisher-name>, <year>1994</year>.</mixed-citation></ref><ref id="R19"><label>19</label><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Refaeilzadeh</surname><given-names>P</given-names></name>, <name><surname>Tang</surname><given-names>L</given-names></name>, <name><surname>Liu</surname><given-names>H</given-names></name></person-group>
<source>Cross-validation. <italic>Encyclopedia of database systems</italic></source>. <publisher-name>Springer</publisher-name>, <year>2009</year>:<fpage>532</fpage>&#x02013;<lpage>8</lpage>.</mixed-citation></ref><ref id="R20"><label>20</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dougherty</surname><given-names>ER</given-names></name></person-group>
<article-title>Small sample issues for microarray-based classification</article-title>. <source>Comp Funct Genomics</source><year>2001</year>;<volume>2</volume>:<fpage>28</fpage>&#x02013;<lpage>34</lpage>
<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1002/cfg.62">doi:10.1002/cfg.62</ext-link><pub-id pub-id-type="pmid">18628896</pub-id></mixed-citation></ref><ref id="R21"><label>21</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Braga-Neto</surname><given-names>UM</given-names></name>, <name><surname>Dougherty</surname><given-names>ER</given-names></name></person-group>
<article-title>Is cross-validation valid for small-sample microarray classification?</article-title><source>Bioinformatics</source><year>2004</year>;<volume>20</volume>:<fpage>374</fpage>&#x02013;<lpage>80</lpage>
<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1093/bioinformatics/btg419">doi:10.1093/bioinformatics/btg419</ext-link><pub-id pub-id-type="pmid">14960464</pub-id></mixed-citation></ref><ref id="R22"><label>22</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hanley</surname><given-names>JA</given-names></name>, <name><surname>McNeil</surname><given-names>BJ</given-names></name></person-group>
<article-title>The meaning and use of the area under a receiver operating characteristic (ROC) curve</article-title>. <source>Radiology</source><year>1982</year>;<volume>143</volume>:<fpage>29</fpage>&#x02013;<lpage>36</lpage>
<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1148/radiology.143.1.7063747">doi:10.1148/radiology.143.1.7063747</ext-link><pub-id pub-id-type="pmid">7063747</pub-id></mixed-citation></ref><ref id="R23"><label>23</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Parker</surname><given-names>BJ</given-names></name>, <name><surname>G&#x000fc;nter</surname><given-names>S</given-names></name>, <name><surname>Bedo</surname><given-names>J</given-names></name></person-group>
<article-title>Stratification bias in low signal microarray studies</article-title>. <source>BMC Bioinform</source><year>2007</year>;<volume>8</volume>:<fpage>326</fpage>
<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1186/1471-2105-8-326">doi:10.1186/1471-2105-8-326</ext-link></mixed-citation></ref><ref id="R24"><label>24</label><mixed-citation publication-type="other"><person-group person-group-type="author"><name><surname>Airola</surname><given-names>A</given-names></name>, <name><surname>Pahikkala</surname><given-names>T</given-names></name>, <name><surname>Waegeman</surname><given-names>W</given-names></name><etal/></person-group>
<comment>A comparison of AUC estimators in small-sample studies. <italic>3rd International workshop on Machine Learning in Systems Biology (MLSB 09)</italic></comment><year>2009</year>:<fpage>15</fpage>&#x02013;<lpage>23</lpage>.</mixed-citation></ref><ref id="R25"><label>25</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yelle</surname><given-names>LE</given-names></name></person-group>
<article-title>The learning curve: Historical review and comprehensive survey</article-title>. <source>Decis Sci</source><year>1979</year>;<volume>10</volume>:<fpage>302</fpage>&#x02013;<lpage>28</lpage>
<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1111/j.1540-5915.1979.tb00026.x">doi:10.1111/j.1540-5915.1979.tb00026.x</ext-link></mixed-citation></ref><ref id="R26"><label>26</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Radmacher</surname><given-names>MD</given-names></name>, <name><surname>McShane</surname><given-names>LM</given-names></name>, <name><surname>Simon</surname><given-names>R</given-names></name></person-group>
<article-title>A paradigm for class prediction using gene expression profiles</article-title>. <source>J Comput Biol</source><year>2002</year>;<volume>9</volume>:<fpage>505</fpage>&#x02013;<lpage>11</lpage>.<pub-id pub-id-type="pmid">12162889</pub-id></mixed-citation></ref><ref id="R27"><label>27</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vapnik</surname><given-names>VN</given-names></name></person-group>
<article-title>An overview of statistical learning theory</article-title>. <source>IEEE Trans Neural Netw</source><year>1999</year>;<volume>10</volume>:<fpage>988</fpage>&#x02013;<lpage>99</lpage>
<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1109/72.788640">doi:10.1109/72.788640</ext-link><pub-id pub-id-type="pmid">18252602</pub-id></mixed-citation></ref><ref id="R28"><label>28</label><mixed-citation publication-type="other"><person-group person-group-type="author"><name><surname>Karatzoglou</surname><given-names>A</given-names></name>, <name><surname>Meyer</surname><given-names>D</given-names></name>, <name><surname>Hornik</surname><given-names>K</given-names></name></person-group>
<comment>Support vector machines in R. <italic>J Stat Softw</italic> 2006;15:1&#x02013;28.</comment></mixed-citation></ref><ref id="R29"><label>29</label><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Breiman</surname><given-names>L</given-names></name></person-group>
<comment><italic>Out-of-bag estimation</italic>. Technical report</comment>
<publisher-loc>Berkeley, CA</publisher-loc>: <publisher-name>Department of Statistics, University of California</publisher-name>, <year>1996</year>.</mixed-citation></ref><ref id="R30"><label>30</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jiang</surname><given-names>W</given-names></name>, <name><surname>Simon</surname><given-names>R</given-names></name></person-group>
<article-title>A comparison of bootstrap methods and an adjusted bootstrap approach for estimating the prediction error in microarray classification</article-title>. <source>Stat Med</source><year>2007</year>;<volume>26</volume>:<fpage>5320</fpage>&#x02013;<lpage>34</lpage>
<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1002/sim.2968">doi:10.1002/sim.2968</ext-link><pub-id pub-id-type="pmid">17624926</pub-id></mixed-citation></ref><ref id="R31"><label>31</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Efron</surname><given-names>B</given-names></name>, <name><surname>Tibshirani</surname><given-names>R</given-names></name></person-group>
<article-title>Improvements on cross-validation: the 632+bootstrap method</article-title>. <source>J Am Statist Assoc</source><year>1997</year>;<volume>92</volume>:<fpage>548</fpage>&#x02013;<lpage>60</lpage>.</mixed-citation></ref><ref id="R32"><label>32</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alon</surname><given-names>U</given-names></name>, <name><surname>Barkai</surname><given-names>N</given-names></name>, <name><surname>Notterman</surname><given-names>DA</given-names></name><etal/></person-group>
<article-title>Broad patterns of gene expression revealed by clustering analysis of tumor and normal colon tissues probed by oligonucleotide arrays</article-title>. <source>Proc Natl Acad Sci USA</source><year>1999</year>;<volume>96</volume>:<fpage>6745</fpage>&#x02013;<lpage>50</lpage>
<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.96.12.6745">doi:10.1073/pnas.96.12.6745</ext-link><pub-id pub-id-type="pmid">10359783</pub-id></mixed-citation></ref><ref id="R33"><label>33</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sotiriou</surname><given-names>C</given-names></name>, <name><surname>Wirapati</surname><given-names>P</given-names></name>, <name><surname>Loi</surname><given-names>S</given-names></name><etal/></person-group>
<article-title>Gene expression profiling in breast cancer: understanding the molecular basis of histologic grade to improve prognosis</article-title>. <source>J Natl Cancer Inst</source><year>2006</year>;<volume>98</volume>:<fpage>262</fpage>&#x02013;<lpage>72</lpage>
<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1093/jnci/djj052">doi:10.1093/jnci/djj052</ext-link><pub-id pub-id-type="pmid">16478745</pub-id></mixed-citation></ref><ref id="R34"><label>34</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Y</given-names></name>, <name><surname>Klijn</surname><given-names>JG</given-names></name>, <name><surname>Zhang</surname><given-names>Y</given-names></name><etal/></person-group>
<article-title>Gene-expression profiles to predict distant metastasis of lymph-node-negative primary breast cancer</article-title>. <source>Lancet</source><year>2005</year>;<volume>365</volume>:<fpage>671</fpage>&#x02013;<lpage>9</lpage>
<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S0140-6736(05)70933-8">doi:10.1016/S0140-6736(05)70933-8</ext-link><pub-id pub-id-type="pmid">15721472</pub-id></mixed-citation></ref><ref id="R35"><label>35</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mukherjee</surname><given-names>S</given-names></name>, <name><surname>Tamayo</surname><given-names>P</given-names></name>, <name><surname>Rogers</surname><given-names>S</given-names></name><etal/></person-group>
<article-title>Estimating dataset size requirements for classifying DNA microarray data</article-title>. <source>J Comput Biol</source><year>2003</year>;<volume>10</volume>:<fpage>119</fpage>&#x02013;<lpage>42</lpage>
<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1089/106652703321825928">doi:10.1089/106652703321825928</ext-link><pub-id pub-id-type="pmid">12804087</pub-id></mixed-citation></ref></ref-list></back></article>