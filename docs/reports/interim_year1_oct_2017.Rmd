---
title: "SoftCite - Interim Year 1 Report for Sloan Foundation"
author: "James Howison, Jason Priem, and Heather Piwowar"
date: "October 1st, 2017"
output: pdf_document
urlcolor: blue
# bibliography: report.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r, echo=F, results='hide', message=FALSE, warning=FALSE}
library(tidyverse)
library(data.world)
library(glue)
```


```{r, echo=F, results='hide', message=FALSE, warning=FALSE}

# Datasets are referenced by their URL or path
dataset_key <- "https://data.world/jameshowison/software-citations"
# List tables available for SQL queries
tables_qry <- data.world::qry_sql("SELECT * FROM Tables")
tables_df <- data.world::query(tables_qry, dataset = dataset_key)

prefixes <- "PREFIX bioj: <http://james.howison.name/ontologies/bio-journal-sample#>
PREFIX bioj-cited: <http://james.howison.name/ontologies/bio-journal-sample-citation#>
PREFIX ca: <http://floss.syr.edu/ontologies/2008/4/contentAnalysis.owl#>
PREFIX citec: <http://james.howison.name/ontologies/software-citation-coding#> 
PREFIX dc: <http://dublincore.org/documents/2012/06/14/dcmi-terms/>
PREFIX doap: <http://usefulinc.com/ns/doap#>
PREFIX owl: <http://www.w3.org/2002/07/owl#>
PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>
PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
PREFIX vivo: <http://vivoweb.org/ontology/core#>
PREFIX xml: <http://www.w3.org/XML/1998/namespace>
PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>"

qry_string <-  "
SELECT ?article ?coder ?has_mentions 
WHERE {
	?article ca:isTargetOf [ ca:hasCoder ?coder ;
	                         ca:appliesCode [ rdf:type citec:coded_no_in_text_mentions ;
                                                citec:isPresent ?has_mentions ]
                            ]
                                                
}
"

qry <- data.world::qry_sparql(paste0(prefixes,qry_string))
results_df <- data.world::query(qry, dataset = dataset_key)

results_df <- results_df %>% mutate(has_mentions = ifelse(has_mentions == "true", T, F))

# unique articles
num_articles <- results_df %>% pull(article) %>% n_distinct()

# unique coders
num_coders <- results_df %>% pull(coder) %>% n_distinct()

# articles with and without mentions
with_mentions <- results_df %>% filter(has_mentions == T) %>% pull(article) %>% n_distinct()
no_mentions <- results_df %>% filter(has_mentions == F) %>% pull(article) %>% n_distinct()

```

```{r, echo=F, results='hide', message=FALSE, warning=FALSE}
qry_string = "SELECT ?article ?coder ?selection ?quote ?page ?software
WHERE {
	?article citec:has_in_text_mention ?selection .
	?selection citec:full_quote ?quote ;
	OPTIONAL { 
  ?selection citec:on_pdf_page ?page ;
	           ca:isTargetOf [ ca:hasCoder ?coder ;
	                           ca:appliesCode [ rdf:type citec:software_name ;
	                                            citec:isPresent true ;
	                                            rdfs:label ?software ]
	                          ]
	}
}"

qry <- data.world::qry_sparql(paste0(prefixes,qry_string))
results_df <- data.world::query(qry, dataset = dataset_key)

results_df <- results_df %>% 
  filter(!str_detect(article, "bio-journal"))

num_selections <- results_df %>% pull(selection) %>% n_distinct()


articles_in_count_order <- results_df %>%
  group_by(article) %>% 
  tally() %>% 
  arrange(desc(n)) %>%
  select(article) %>% 
  mutate(article_id = row_number())

# results_df %>% 
#   left_join(articles_in_count_order, by = "article") %>% 
#   ggplot(aes(x=article_id), color="blue") +
#   geom_histogram(stat="bin", binwidth = 1) +
#   xlab("article index") +
#   labs(title="Count of mentions per article")

```
```{r, echo=F, results='hide', message=FALSE, warning=FALSE}
# Software per article

results_df <- results_df %>%
  mutate(software_norm = str_to_lower(software))

unique_software <- results_df %>%
  pull(software_norm) %>%
  n_distinct()

software_ordered <- results_df %>%
  group_by(software_norm) %>% 
  tally() %>% 
  arrange(desc(n)) %>% 
  mutate(software_id = row_number())

# results_df %>% 
#   left_join(software_ordered, by = "software_norm") %>% 
#   ggplot(aes(x=software_id), color="blue") +
#   geom_histogram(stat="bin", binwidth = 1) +
#   xlab("software index") +
#   labs(title="Count of articles per software packages")

```

## SoftCite goals and activities

Our grant proposal lays out the goals of the SoftCite project:

> Our goal is to improve software in scholarship (science, engineering, and the humanities) by raising the visibility of software work as a contribution in the literature, thus improving incentives for software work in scholarship.

Our major activites are:

1. Develop a "Gold Standard" manually coded dataset of software mentions in the literature
2. Use that standard toward machine learning discovery of software mentions
3. Develop a database linking software tools to associated citations
4. Using this database, prototype and study three tools: CiteAs, CiteSuggest, and Software ImpactStory

In this report we describe progress in each of these activities.

## Progress on "Gold Standard" dataset development

Since coding began in Spring 2017 the team at UT Austin has coded `r num_articles` articles from the PubMed Open Access Collection, finding `r num_selections` mentions of software (and many, many more examples of sentences that do not mention software, just as important to the machine learning). The coders have found `r with_mentions` articles with mentions of software and `r no_mentions` articles without software mentions and thus far have identified `r unique_software` separate software packages. En route we trained a doctoral student (Hannah Cohoon) in content analysis, revised our coding scheme to best match the future machine learning task, established a collaborative workflow hosted on github ([publicly available repository](https://github.com/howisonlab/softcite-dataset), [documentation](https://howisonlab.github.io/softcite-dataset/)). We have on-boarded and trained ~35 undergraduate and masters students.

We are growing the student group: from ~4 in Spring 2017, ~8 over Summer 2017, to ~25 in Fall 2017 (the fall group has only just finished training and has not yet contibuted coding of new articles). In addition we have reached out to the minority-serving institution Houston Tillotson University and will on-board a group of ~5 coders from there in the next two weeks. Currently each article is being coded by two students, but as agreement between new coders improves, only a small percentage will be double coded.

Our speed of coding is accelerating; we expect to have ~400 PubMed articles coded by the end of this semester. We will then move onto collections from astronomy and economics. Nonetheless, we are also exploring two techniques to move faster. Both techniques will require validation from our current completely manually coded dataset and so must follow the work we're currently doing. 

The first is bootstraping using a "whitelist" of software packages, identifying sentences with those words in them. We would then manually code just those sentences. Assuming that packages known beforehand (and therefore on the white list) are not mentioned in systematically different ways bootstraping can train a system to learn generally how packages are mentioned and apply that learning to find mentions of packages not on the whitelist. Colleagues, such as Pan et al. (2015), have taken this approach but they have not had the fully manually coded set to validate their approach; our manually coded full dataset will provide validation.

The second approach takes advantage of one finding so far: software mentions cluster together in articles. However they do not cluster predictably (e.g., one paper might have multiple clusters and clusters are not neccessarily only in methods sections). We will experiment using a whitelist to locate mentions of known packages and then expand up and down 2 or 3 paragraphs to increase our chance of finding mentions of packages not on the whitelist; this ought to amerliorate bias resulting from the choice of the whitelist and allow us to more reliably characterize mentions per article en rouate to a full machine learning system.

## Progress towards machine learning discovery of software mentions

The machine learning to automate recognition of software mentions builds on the dataset discussed above; only when it is large enough will we be able to begin training.  However, we have worked to ensure that the results of manual coding are most useful for machine learning training. Specifically we have experimented with approaches to converting the pdf articles to text in a way that we can label the sentences identified by the coders. As expected this is a difficult process (issues include headers and column conversion problems). We have two approaches to this. First we chose to work with the PubMed Open Access dataset which has clean XML encoded versions of each article, that decision will enable us to get started training without solving pdf conversion issues. Second Heather and Jason at ImpactStory have been exploring new generation pdf conversion tools, such as CERMINE (CERMINE, 2017; Tkaczyk et al, 2015), which identify elements such as headers, columns, tables etc. prior to text conversion.

## Progress towards CiteAs

The CiteAs tool, available at [citeas.org](http://citeas.org), allows users to map from software to ways to cite that software. The backend system provides an API (in preparation for future integration into other tools, both ours and others) integrated with the database described above. The web frontend uses the API to provide the current application. The open repository is available at https://github.com/Impactstory/citeas-api

Users can provide the text name of a software package "yt", a URL such as a project landing page "yt-project.org" or "`https://cran.r-project.org/web/packages/stringr`" or a DOI (publication or software), such as "10.5281/zenodo.160400". The system then checks the database and if no entry is found kicks off a procedure to locate an appropriate citation. Potential sources are checked in sequence and those results returned to users with a provenance chain which helps users known where the suggestions come from and will help software projects adjust how they request citations.

As an example Figure 1 shows a screenshot showing the results of querying "yt-project". In the background the system has downloaded the page and parsed it for a citation request. In the case of `yt` the citation request on the yt-project homepage links to a bibtex file which was then downloaded and parsed to provide the citation.

![CiteAs output for yt-project](yt-project-screenshot.png)

![CiteAs output for stringr](stringr.png)

The second screenshot below, Figure 2, shows an example of a user seeking a citation for an R package. The system downloads the URL `https://cran.r-project.org/web/packages/stringr` and identifies a link to a repository (here the repository is github). We then seek a file in the repository that makes a citation request, checking README and the DESCRIPTION file on CRAN (in the future we will also check for a CITATION / CITATION.TXT in the repository). The citation below is built from the DESCRIPTION file.

The provenance chain is illustrated in the API output below, Figure 3, (actually for the CERMINE tool cited above, CiteAs was provided with the url `https://github.com/CeON/CERMINE`), this shows the locations and techniques that CiteAs attempted in finding a citation. In this case the system identified DOIs located in the README file; the DOIs relate to both an article and to a zenodo archived version of the software. These DOIs are each are mapped to a citation and a prefered citation displayed to the user. In future versions, to be released first week of October, all found citations will be presented to the user as options, together with the provenance of how they were located.

![CiteAs API data showing origns of CERMINE citation](cermine-prov.png)

## Progress towards database linking software tools to associated citations

The backend for CiteAs consults what will become the database linking tools to associated citations. Thus far the database is built from the techniques that CiteAs uses to locate citation requests. 

Over time we will expand this database in two ways. First we will run our citation lcoation techniques pre-emptively using catalogues of scientific software (e.g., libraries.io, NSF and NIH listings, and field specific listings such as the ASCL and SBGrid). Second, we will add tool-to-citation links from the literature including initially those identified through our manual coding, and eventually those identified as our machine learning improves.  As links are added to the database they will become available to CiteAs and our two other tools (CiteSuggest and Software ImpactStory).

## Our collaboration is working well

We are working well as a group. We have scheduled calls each second week to report progress and provide input to each other's activities. Between these calls we have regular email contact and watch each others software repositories. Jason and Heather remotely attended one of James and Hannah's student coding sessions to motivate the students by explaining the end-use of the dataset and demonstrating the CiteAs tool. We distributed CiteAs stickers to the students.

We may seek to arrange face-to-face working sessions, co-located with conferences we would naturally attend (such as the 2018 RDA or FORCE11 Scholarly Communications conference) but we do not see the need to spend funds on separate face to face meetings.

We hope to expand our collaboration beyond our funded group. As mentioned above we are collecting software mention coding from others and working to make it compatible with our schema to publish them together. We are also in communication with [libraries.io](http://libraries.io) which will form a natural extension, allowing the joining of their dependency information with our citation information, per software package.

## Plans for next year

Our priorities for work in the immediate future are:

1. Expand and accelerate our gold-standard dataset production at UT Austin.
2. Soft-launch CiteAs.org and monitor feedback and iterate the tool
3. Prototype machine learning using the gold-standard dataset
4. Drawing on the database and API created for CiteAs and machine learning results as they become available we will:
  - Develop CiteSuggest
  - Develop Software ImpactStory
5. James at UT Austin will recruit an additional doctoral student to undertake socio-technical studies of the three tools.

While the grant was for 2 years, we have adjusted our budgeting to anticipate completing the work over three years, so that we can gather sufficient coded articles and implement the machine learning. ImpactStory plans to implement initial versions of the prototypes and work on the machine learning but then slow their pace of work until a large enough "gold standard" collection has been completed.

# References

CERMINE: Content ExtRactor and MINEr. (2017). Java, Centre for Open Science. Retrieved from https://github.com/CeON/CERMINE (Original work published October 8, 2012)

Pan, X., Yan, E., Wang, Q., & Hua, W. (2015). Assessing the impact of software on science: A bootstrapped learning of software entities in full-text papers. Journal of Informetrics, 9(4), 860–871. https://doi.org/10.1016/j.joi.2015.07.012

Tkaczyk, D., Szostek, P., Fedoryszak, M., Dendek, P. J., & Bolikowski, Ł. (2015). CERMINE: automatic extraction of structured metadata from scientific literature. International Journal on Document Analysis and Recognition (IJDAR), 18(4), 317–335. https://doi.org/10.1007/s10032-015-0249-8
