---
title: "Find quotes in PDF text"
output: html_notebook
---

```{r}
# library(plyr)
library(tidyverse)
library(data.world) # loads saved config see quickstart vignette

prefixes <- "
PREFIX bioj: <http://james.howison.name/ontologies/bio-journal-sample#>
PREFIX bioj-cited: <http://james.howison.name/ontologies/bio-journal-sample-citation#>
PREFIX ca: <http://floss.syr.edu/ontologies/2008/4/contentAnalysis.owl#>
PREFIX citec: <http://james.howison.name/ontologies/software-citation-coding#> 
PREFIX dc: <http://dublincore.org/documents/2012/06/14/dcmi-terms/>
PREFIX doap: <http://usefulinc.com/ns/doap#>
PREFIX owl: <http://www.w3.org/2002/07/owl#>
PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>
PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
PREFIX vivo: <http://vivoweb.org/ontology/core#>
PREFIX xml: <http://www.w3.org/XML/1998/namespace>
PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>
"

softcite_ds = "https://data.world/jameshowison/software-citations/"

# should pull from coding scheme
valid_codes = c("has_supplement",
"has_in_text_mention",
"coded_no_in_text_mentions",
"memo",
"full_quote",
"on_pdf_page",
"spans_pages",
"mention_type",
"software_was_used",
"software_name",
"version_number",
"version_date",
"url",
"creator",
"has_reference",
"reference_type")
```

This gets the codes from the top of the files.

```{r}
# top_code_query <- data.world::qry_sparql(paste(prefixes,
#       "SELECT ?article ?coder ?selection ?full_quote ?on_pdf_page ?spans_pages
# WHERE {
#     ?article citec:has_in_text_mention ?selection .
#     ?selection ca:isTargetOf
#         [ rdf:type ca:CodeApplication ;
#           ca:hasCoder ?coder ;
#           ca:appliesCode [ rdf:type citec:mention_type ]
#         ] .
#     ?selection citec:full_quote ?full_quote ;
#                citec:on_pdf_page ?on_pdf_page ;
#                citec:spans_pages ?spans_pages
#     }"
# ))
# top_codes <- data.world::query(top_code_query, softcite_ds)
# top_codes <- as.tibble(top_codes)
# write_csv(top_codes, "top_codes.csv")
top_codes <- read_csv("top_codes.csv")
```

SPARQL queries return everything as URLs, but we want just the localPart at the end.

```{r}
top_codes <- top_codes %>%
       mutate_at(vars(article, selection), funs(str_extract(.,"[#/]([^#/]+)$"))) %>%
       mutate_at(vars(article,selection), funs(str_sub(.,2)))
```


```{r}
# Focus on PMC dataset and remove newlines from full_quote
top_codes <- top_codes %>% 
  filter(str_detect(article, "PMC")) 

#%>% 
#  mutate(full_quote = str_replace_all(full_quote, "[\r\n]" , ""))


# # Only retain selections from articles with multiple coders
# multi_coders <- top_codes %>% 
#   group_by(article) %>% 
#   mutate(num_coder = n_distinct(coder)) %>% 
#   ungroup() %>%
#   filter(num_coder > 1)
```

Now load the pdf for each article

```{r}
library(extractr)

# locate PDFs
folder <-  "/Users/howison/Documents/UTexas/Projects/SloanSoftCite/softcite-dataset/docs/pdf-files/pmc_oa_files/"

articles <- top_codes %>% 
  select(article) %>% 
  distinct() %>% 
  mutate(path = str_c(folder, article, ".pdf", sep=""))

# load pdfs.
my_extract <- function(p) {
  pdf_t <- extract(p)
  as_text <- pdf_t$data
  no_linebreaks <- str_replace_all(as_text, "[\\r\\n\\s]" , "")
  return(no_linebreaks)
}

# with an unvectorized function use map (or use rowwise())
articles <- articles %>% 
  mutate(pdf_as_string = map_chr(path, my_extract)) 

# articles %>% filter(article == "PMC1747177")
```

```{r}
# for each article, run a function
match_in_article <- function(df) {
  #df <- top_codes[36, ]
  art <- df[[1, "article"]] # grouping variable
  pdf_as_string = (articles %>% filter(article == art))$pdf_as_string[[1]] 
  # ddply(df, .(selection), match_selection, BString(pdf_as_string))
  df %>% group_by(selection) %>% do(match_selection(., BString(pdf_as_string)))
}

match_selection <- function(df, pdf_as_xstring) {
  full_quote <- str_replace_all(df[[1, "full_quote"]], "[\\r\\n\\s]" , "")
  # full_quote <- str_sub(full_quote, 1, 40)
  # Attempt exact match (without indels)
  results <- matchPattern(full_quote, pdf_as_xstring)
  if (length(results) == 0) {
      mismatch_allow <- floor(str_length(full_quote) / 6)
      # mismatch_allow <- 10
      results <- matchPattern(full_quote, pdf_as_xstring, 
                          with.indels = T, max.mismatch = mismatch_allow)
  }
  # Matching something with a header and footer in it, could be arbitrary
  # length.  Could split the string in everyway and find the parts?
   results <- list(results)
   df %>% mutate(num_found = map_int(results, length),
                 start = map_int(results, start),
                 end = map_int(results, end))
}



# one_article <- top_codes %>% filter(article == "PMC3035800")

# top_codes[[36, "selection"]]
# 
# sel_df <- top_codes[36, ]
# 
# PMC3035800 <- articles %>% filter(article == "PMC3035800")
# PMC3035800 <- PMC3035800[[1,"pdf_as_string"]]
# PMC3035800_xstring <- BString(PMC3035800)
# 
# 
# test <- match_selection(sel_df, PMC3035800_xstring)
# 
# test <- match_in_article(sel_df)

# remove
known_problematic = c("PMC5039120_SK02", "PMC5039120_BB02",
                      # interspersed columns
                      "PMC3198286_MD03", 
                      # interspersed headers
                      "PMC4926940_MS01", "PMC4926940_SK01",
                      # interspersed columns
                      "PMC5039120_BB01", "PMC5039120_SK01",
                      # interspersed columns
                      "PMC5238813_RA08", "PMC5238813_SK28",
                      # article has extraction issues, these two don't match
                      "PMC5080194_MD02", "PMC5080194_RA01",
                      # below here are those that find two matches
                      # These two are found twice in the same article)
                      "PMC5339831_BB01", "PMC5339831_SK01",
                      # These two match but are same text as next two
                      "PMC5339831_BB02", "PMC5339831_SK02",
                      # These two match but are same as last two.
                      "PMC3309518_RA09")
                      # This one shows up twice.

achievable <- top_codes %>% filter(!(selection %in% known_problematic) )

# found <- ddply(achievable, .(article), match_in_article, .progress = "text")

# the dot here is a pronoun for the group, ie the split up dataframe.
found <- achievable %>% group_by(article) %>% do(match_in_article(.))


found %>% filter(num_found == 0)
found %>% filter(num_found > 1)

```

We miss some using this technique because text gets inserted into the middle of sentences during the PDF conversion.  e.g., PMC3198286, around "Natick". Also PMC5039120_SK02 and PMC5039120_BB02 looks like:

```
SAS statistical software ver. 9.4 (SAS In-

married but separated, widows/widowers, and divorcees; and non-re-

stitute Inc., Cary, NC, USA) was used to conduct statistical analyses
```
the middle line there is text from another column!

Article PMC4926940 similarly extracts with text from each column interspersed, so it's useless.

These are taken out in the achieveables conversion, above.

Another technique might be to start with the name of the software highlighted and expand from there until the specific mention is found?

PMC3198286_RA01 RA_02 and RA_03 are just too long?

PMC3309518_RA09 shows up twice because of indels allowing too much flexibility.

PMC5080194_MD02
PMC5080194_RA01
PMC5080194_RA02

These show up twice because there are 2 very similar sentences about the same software.

PMC5339831_BB01 has two very similar sentences, but it's definitely the first one, BB02 is the second one.  (first has PerkinElmer)

PMC5339831_SK01 and SK02 are the same story.

```{r}
# Now need to figure out which overlap with which.
found <- found %>% 
  mutate(range = map2(start, end, IRanges))

# Probable issues in passing in found, which is grouped.
get_possible_matches <- function(df, found) {
  # filter the found dataframe to find possible matches
  curr_article <- df[[1,"article"]]
  curr_coder <- df[[1,"coder"]]
  others <- found %>% filter(article == curr_article, coder != curr_coder ) %>% 
    transmute(possible_match = selection, possible_range = range)
  
  df %>% rowwise() %>% mutate(poss_match = list(others))
}

return_group <- function(df) {
  # head(df, 2)
  m = tibble(new_col = "hat")
  m$selection <- df$selection
  m
}

found %>% group_by(article) %>% do(return_group(.))

poss_matches <- found %>%
  select(article, coder, selection, range) %>% 
  group_by(article, coder) %>%
  do(get_possible_matches(., found)) %>% 
  unnest(poss_match)

poss_matches %>% unnest(poss_match)

poss_matches[[300,"range"]]

# Now have a row for every possible pair.
# Must have found available
does_overlap <- function(sel_range, poss_overlap_range) {
 # sel_range_list <- (found %>% filter(selection == sel))[[1, "range"]]
#  sel_range <- sel_range_list[[1]] # unlist
#  poss_overlap_range_list <- found %>% 
#  filter(selection == poss_overlap) %>% pull(range)
 # poss_overlap_range <- poss_overlap_range_list[[1]] # unlist
  num_overlap = nrow(as.matrix(findOverlaps(sel_range, poss_overlap_range)))
  return(num_overlap != 0)
}

# does_overlap("PMC2529246_BB02", "PMC2529246_MS02", found)

poss_matches <- poss_matches %>% 
  mutate(match = map2(range, poss_overlap_range, does_overlap))

poss_matches %>% group_by(match) %>% tally()

final_matches <- poss_matches %>% filter(match == T) %>% 
  rename(matching_selection = poss_match) %>% 
  select(selection, matching_selection)

top_codes_with_matches <- left_join(top_codes, final_matches, by = "selection") %>% mutate(matched = !is.na(matching_selection))

top_codes_with_matches %>% group_by(matched) %>% tally()

# Should add in manual matches from list above.

```

```{r}
library(IRanges)
self <- tribble(
  ~start, ~end,
  1,   6,
  6,   8
)

other <- tribble(
  ~start, ~end,
  1,   14,
  7,   14
)

self_r <-  self %>% mutate(range = map2(start, end, IRanges))
other_r <- other %>% mutate(range = map2(start, end, IRanges))

run_match <- function(df, other_list) {
  self_list <-  RangesList(df[[1, "range"]])
  return(overlapsAny(self_list, other_list))
}

dlply(self_r, .(start), run_match, RangesList(other_r$range))


# Hmmm, even using RangesLists it still just matches each pair
```
```{r}
# take found and nest each coder into a column?
nest_coders <- function(df) {
  coders <- split(df, df$coder)
  coders[1] <- coders[1] %>% select(-article)
  coders[2] <- coders[2] %>% select(-article)
  
  tibble(coder1 = coders[1], coder2 = coders[2])
}

(found %>% group_by(article) %>% do(nest_coders(.)))[[1,"coder1"]]
(found %>% group_by(article) %>% do(nest_coders(.)))[[10,"coder1"]]
```

