---
title: "Find quotes in PDF text"
output: html_notebook
---

```{r}
# library(plyr)
library(tidyverse)
library(data.world) # loads saved config see quickstart vignette

prefixes <- "
PREFIX bioj: <http://james.howison.name/ontologies/bio-journal-sample#>
PREFIX bioj-cited: <http://james.howison.name/ontologies/bio-journal-sample-citation#>
PREFIX ca: <http://floss.syr.edu/ontologies/2008/4/contentAnalysis.owl#>
PREFIX citec: <http://james.howison.name/ontologies/software-citation-coding#> 
PREFIX dc: <http://dublincore.org/documents/2012/06/14/dcmi-terms/>
PREFIX doap: <http://usefulinc.com/ns/doap#>
PREFIX owl: <http://www.w3.org/2002/07/owl#>
PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>
PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
PREFIX vivo: <http://vivoweb.org/ontology/core#>
PREFIX xml: <http://www.w3.org/XML/1998/namespace>
PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>
"

softcite_ds = "https://data.world/jameshowison/software-citations/"

# should pull from coding scheme
valid_codes = c("has_supplement",
"has_in_text_mention",
"coded_no_in_text_mentions",
"memo",
"full_quote",
"on_pdf_page",
"spans_pages",
"mention_type",
"software_was_used",
"software_name",
"version_number",
"version_date",
"url",
"creator",
"has_reference",
"reference_type")
```

This gets the codes from the top of the files.

```{r}
top_code_query <- data.world::qry_sparql(paste(prefixes,
      "SELECT ?article ?coder ?selection ?full_quote ?on_pdf_page ?spans_pages
WHERE {
    ?article citec:has_in_text_mention ?selection .
    ?selection ca:isTargetOf
        [ rdf:type ca:CodeApplication ;
          ca:hasCoder ?coder ;
          ca:appliesCode [ rdf:type citec:mention_type ]
        ] .
    ?selection citec:full_quote ?full_quote ;
               citec:on_pdf_page ?on_pdf_page ;
               citec:spans_pages ?spans_pages
    }"
))
top_codes <- data.world::query(top_code_query, softcite_ds)
top_codes <- as.tibble(top_codes)
write_csv(top_codes, "top_codes.csv")
# top_codes <- read_csv("top_codes.csv")
```

SPARQL queries return everything as URLs, but we want just the localPart at the end.

```{r}
top_codes <- top_codes %>%
       mutate_at(vars(article, selection), funs(str_extract(.,"[#/]([^#/]+)$"))) %>%
       mutate_at(vars(article,selection), funs(str_sub(.,2)))
```


```{r}
# Focus on PMC dataset and remove newlines from full_quote
top_codes <- top_codes %>% 
  filter(str_detect(article, "PMC")) %>% 
  filter(selection != "PMC4683424_MS03") # erroneously coded reference as in-text.

#%>% 
#  mutate(full_quote = str_replace_all(full_quote, "[\r\n]" , ""))


# # Only retain selections from articles with multiple coders
# multi_coders <- top_codes %>% 
#   group_by(article) %>% 
#   mutate(num_coder = n_distinct(coder)) %>% 
#   ungroup() %>%
#   filter(num_coder > 1)


```

```{r}
norm_text <- function(text_to_norm){
  text_to_norm %>% 
    str_replace_all("[\\r\\n\\s]" , "") %>% 
    str_replace_all("\\(http.*?\\)" , "") %>% 
    str_to_lower()
}

test_txt = "The cat sat ON the (http://REMOVE_ME)"
norm_text(test_txt)
```


Try locating in XML files.

```{r}
library(xml2)
library(glue)

path = "/Users/howison/Documents/UTexas/Projects/SloanSoftCite/softcite-dataset/docs/pdf-files/pmc_oa_files/"

test_articles <- tribble(
  ~pmcid,
  "PMC5381613", 
  "PMC5421183" 
)

articles <- top_codes %>% select(article) %>% distinct()

articles <- articles %>% 
  mutate(xml_path       = glue_data(., "{path}{article}.xml"),
         article_as_xml = map(xml_path, read_xml),
         pdf_as_string  = map_chr(article_as_xml, xml_text),
         pdf_as_string  = norm_text(pdf_as_string))
                        
```

Now load the pdf for each article

```{r, eval=F}
# library(extractr)
# 
# # locate PDFs
# folder <-  "/Users/howison/Documents/UTexas/Projects/SloanSoftCite/softcite-dataset/docs/pdf-files/pmc_oa_files/"
# 
# articles <- top_codes %>% 
#   select(article) %>% 
#   distinct() %>% 
#   mutate(path = str_c(folder, article, ".pdf", sep=""))
# 
# # load pdfs.
# my_extract <- function(p) {
#   pdf_t <- extract(p)
#   as_text <- pdf_t$data
#   no_linebreaks <- str_replace_all(as_text, "[\\r\\n\\s]" , "")
#   return(no_linebreaks)
# }
# 
# # with an unvectorized function use map (or use rowwise())
# articles <- articles %>% 
#   mutate(pdf_as_string = map_chr(path, my_extract)) 

# articles %>% filter(article == "PMC1747177")

# known_problematic = c("PMC5039120_SK02", "PMC5039120_BB02",
#                       # interspersed columns
#                       "PMC3198286_MD03", 
#                       # interspersed headers
#                       "PMC4926940_MS01", "PMC4926940_SK01",
#                       # interspersed columns
#                       "PMC5039120_BB01", "PMC5039120_SK01",
#                       # interspersed columns
#                       "PMC5238813_RA08", "PMC5238813_SK28",
#                       # article has extraction issues, these two don't match
#                       "PMC5080194_MD02", "PMC5080194_RA01",
#                       # below here are those that find two matches
#                       # These two are found twice in the same article)
#                       "PMC5339831_BB01", "PMC5339831_SK01",
#                       # These two match but are same text as next two
#                       "PMC5339831_BB02", "PMC5339831_SK02",
#                       # These two match but are same as last two.
#                       "PMC3309518_RA09")
#                       # This one shows up twice.

```

Do matching using Biostrings.

```{r}
library(Biostrings)
library(tidyverse)
library(clipr)

top_codes <- top_codes %>% 
  mutate(norm_full_quote = norm_text(full_quote))

# for each article, run a function
match_in_article <- function(df) {
  #df <- top_codes[36, ]
  art <- df[[1, "article"]] # grouping variable
  pdf_as_string = (articles %>% filter(article == art))$pdf_as_string[[1]] 
  # ddply(df, .(selection), match_selection, BString(pdf_as_string))
  df %>% group_by(selection) %>% do(match_selection(., BString(pdf_as_string)))
}

match_selection <- function(df, pdf_as_xstring) {
  norm_full_quote <- df[[1, "norm_full_quote"]]
  # Attempt exact match (without indels)
  results <- matchPattern(norm_full_quote, pdf_as_xstring)
  if (length(results) == 0) {
      mismatch_allow <- floor(str_length(norm_full_quote) / 10)
      # mismatch_allow <- 10
      results <- matchPattern(norm_full_quote, pdf_as_xstring,
                          with.indels = T, max.mismatch = mismatch_allow)
  }
  
  if (length(results) == 0) {
    # warning(paste0("Not found: ", df[[1, "selection"]], "\n"))
    df %>% mutate(found = F, num_found = 0, start = NA, end = NA)
  } else {
    results <- list(results)
    df %>% mutate(found = T,
                 num_found = map_int(results, length),
                 start_val = map_int(results, start),
                 end_val = map_int(results, end))
  }
}

achievable <- top_codes # %>% filter(!(selection %in% known_problematic) )

# the dot here is a pronoun for the group, ie the split up dataframe.
found <- achievable %>% group_by(article) %>% do(match_in_article(.))

found %>% group_by(found) %>% tally()

found %>% filter(found == F)

# %>% pull(full_quote) %>% write_clip()

# articles %>% filter(article == "PMC5042035") %>% pull(pdf_as_string) %>% write_clip()

# fileoutput <- articles %>% filter(article == "PMC1635254") %>% pull(pxdf_as_string)

# write_file(fileoutput, "/Users/howison/Documents/UTexas/Projects/SloanSoftCite/softcite-dataset/docs/pdf-files/pmc_oa_files/PMC1635254/PMC1635254.txt")
```



```{r}
# Now need to figure out which overlap with which.

# Selections can only match selections in the same article. 
# Can only match selections by another coder as well.
# Can identify matches then drop those by self (which would include matching own range)
# write_csv(found, "found.csv")

found <- read_csv("found.csv")

# this needed a non-equi join. forcing via a cartesian product
# then a filter. Worked back to an inner join by article.

 # overlap definition
    # from http://wiki.c2.com/?TestIfDateRangesOverlap
    # https://stackoverflow.com/questions/3269434/
    # whats-the-most-efficient-way-to-test-two-integer-ranges-for-overlap
    # ( start1 <= end2 and start2 <= end1 )

matching <- inner_join(x = found, y = found, by = "article") %>% 
  filter(coder.x != coder.y,
         start_val.x <= end_val.y,
         start_val.y <= end_val.x) %>% 
  select(selection.x, selection.y)

# sensible check
# matching %>%
#   select(full_quote.x, full_quote.y) 

# left join back to found to leave NAs for those that didn't match
matched_found <- found %>% 
  left_join(matching, by = c("selection" = "selection.x")) %>% 
  dplyr::rename(matching_selection = selection.y) %>% 
  mutate(matched = if_else(is.na(matching_selection),F,T))

# summarize situation.
simple_agree_counts <- matched_found %>%
  group_by(article) %>% 
  summarize(coder_count          = n_distinct(coder),
            sel_count            = n(),
            match_count          = sum(matched==T),
            simple_agree_percent = round(match_count / sel_count, 2))
  

simple_agree_counts %>% 
  filter(coder_count > 1) %>% 
  summarize(total_selections = sum(sel_count),
            total_match = sum(match_count),
            overall_agree_perc = total_match / total_selections,
            overall_agree_perc = round(overall_agree_perc, 2))

# per coder is a little more problematic
agree_counts_per_coder <- matched_found %>% 
  group_by(coder) %>% 
  summarize(article_count        = n_distinct(article),
            sel_count            = n(),
            match_count          = sum(matched==T),
            simple_agree_percent = round(match_count / sel_count, 2)) %>% 
  arrange(desc(simple_agree_percent))
```


```{r}


found

View(found)

found %>% 
  group_by(article) %>%
  mutate(matches = interval_overlap(interval, interval))

# for each article, split by coder then findOverlaps 
found %>% ungroup() %>% select(start, end) %>% as.matrix()

# get_matches_in_article <- function(df_article) {
#   # df_article <- found %>% ungroup()
#   starts <- df_article %>% pull(start)
#   ends <- df_article %>% pull(end)
#   intervals <- Intervals(as.matrix(data.frame(starts, ends)))
#   interval_overlap(intervals, intervals)
# }

get_matches_in_article <- function(s, e) {
  intervals <- Intervals(as.matrix(data.frame(s, e)))
  interval_overlap(intervals, intervals)
}

found %>% 
  group_by(article) %>%
  do(matches_for_article = get_matches_in_article(start_val, end_val))
         
# Probable issues in passing in found, which is grouped.
get_possible_matches <- function(df, found) {
  # filter the found dataframe to find possible matches
  curr_article <- df[[1,"article"]]
  curr_coder <- df[[1,"coder"]]
  others <- found %>% 
    filter(article == curr_article, coder != curr_coder ) %>% 
    transmute(possible_match = selection, 
              possible_range = range)
  
  df %>% rowwise() %>% mutate(poss_match = list(others))
}

# return_group <- function(df) {
#   # head(df, 2)
#   m = tibble(new_col = "hat")
#   m$selection <- df$selection
#   m
# }
# 
# found %>% group_by(article) %>% do(return_group(.))

poss_matches <- found %>%
  select(article, coder, selection, range) %>% 
  group_by(article, coder) %>%
  do(get_possible_matches(., found)) %>% 
  unnest(poss_match)

# poss_matches %>% unnest(poss_match)

# poss_matches[[300,"range"]]

# Now have a row for every possible pair.
# Must have found available
does_overlap <- function(sel_range, poss_overlap_range) {
 # sel_range_list <- (found %>% filter(selection == sel))[[1, "range"]]
#  sel_range <- sel_range_list[[1]] # unlist
#  poss_overlap_range_list <- found %>% 
#  filter(selection == poss_overlap) %>% pull(range)
 # poss_overlap_range <- poss_overlap_range_list[[1]] # unlist
  num_overlap = nrow(as.matrix(findOverlaps(sel_range, poss_overlap_range)))
  return(num_overlap != 0)
}

# does_overlap("PMC2529246_BB02", "PMC2529246_MS02", found)

poss_matches <- poss_matches %>% 
  mutate(match = map2(range, possible_range, does_overlap))

poss_matches %>% group_by(match) %>% tally()

final_matches <- poss_matches %>% filter(match == T) %>% 
  rename(matching_selection = poss_match) %>% 
  select(selection, matching_selection)

top_codes_with_matches <- left_join(top_codes, final_matches, by = "selection") %>% mutate(matched = !is.na(matching_selection))

top_codes_with_matches %>% group_by(matched) %>% tally()

# Should add in manual matches from list above.

```

```{r}
library(IRanges)
self <- tribble(
  ~start, ~end,
  1,   6,
  6,   8
)

other <- tribble(
  ~start, ~end,
  1,   14,
  7,   14
)

self_r <-  self %>% mutate(range = map2(start, end, IRanges))
other_r <- other %>% mutate(range = map2(start, end, IRanges))

run_match <- function(df, other_list) {
  self_list <-  RangesList(df[[1, "range"]])
  return(overlapsAny(self_list, other_list))
}

dlply(self_r, .(start), run_match, RangesList(other_r$range))


# Hmmm, even using RangesLists it still just matches each pair
```
```{r}
# take found and nest each coder into a column?
nest_coders <- function(df) {
  coders <- split(df, df$coder)
  coders[1] <- coders[1] %>% select(-article)
  coders[2] <- coders[2] %>% select(-article)
  
  tibble(coder1 = coders[1], coder2 = coders[2])
}

(found %>% group_by(article) %>% do(nest_coders(.)))[[1,"coder1"]]
(found %>% group_by(article) %>% do(nest_coders(.)))[[10,"coder1"]]



test_ranges <- tribble(
  ~id, ~start, ~end,
  1,  1,   6,
  2,  7,   8,
  3,  1,  14,
  4,  7,  14
)



test_ranges <-  test_ranges %>% mutate(range = map2(start, end, IRanges))

ranges_array <- test_ranges %>% pull(range)

ranges_list <- as(ranges_array, "RangesList")

as.matrix(findOverlaps(query=ranges_list, select = "all", type = "any"))

range_match <- function(query, subject) {
  # match defined as any overlap.
  case_when(
    query$start > subject$end ~ FALSE,
    
  )
}

vals <- Intervals(test_ranges %>% select(start, end) %>% as.matrix)

test_ranges$matches <- interval_overlap(vals, vals)

test_ranges %>% 
  unnest() %>% 
  filter(id != matches)

```
# ~id, ~overlapping
# 1, 3
# 2, 3; 4 
# 3, 1; 2; 4
# 4, 2; 3
