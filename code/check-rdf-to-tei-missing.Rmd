---
title: "Checking missing data from RDF to TEI"
author: Fan Du
date: 9/11/2020
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

library(tidyverse)
library(here)
library(xml2)
```

_September 18, 2020. Fri_  
**Problem statement:**  
I started from checking positive articles in the corpus. id.csv contains 12 original articles with duplicated ids. -> should be that 14 ids in the table point to the wrong article -> should be a quick fix  
Besides, currently I have found 8 positive articles in RDF annotations that are not contained in TEI corpus. I suspect it belongs to the small set of missing articles due to the different starting point of document processing.  

# 1. Compare CSV and TEI summaries

```{r, echo=T, results='hide', message=F}
# load the article ID table
included_article <- read_csv("https://raw.githubusercontent.com/ourresearch/software-mentions/master/resources/dataset/software/corpus/ids.csv")
# 4,971 articles in total, uniquely identified

# check id table
included_article %>% distinct(origin)
# 4,958 distinct original file names

# identified duplicated value of `origin` in id table
duplicated_origin <- included_article %>% 
  group_by(origin) %>% 
  filter(n()>1) %>% 
  distinct(origin) %>% pull
```

```{r, echo=TRUE}
included_article %>% 
  filter(origin %in% duplicated_origin) 
  # write to "data/duplicated_origin_in_id_tbl.csv")
```

```{r, echo=TRUE, results='hide'}
# construct artile filter
article_list <- included_article %>% 
  distinct(origin) %>% pull(origin)
```

```{r, echo=TRUE, results='hide', message=F}
# check top annotator results from csv files
csv_articles <- read_csv(here("data/csv_dataset/softcite_articles.csv"))
csv_mentions <- read_csv(here("data/csv_dataset/softcite_in_text_mentions.csv"))
csv_codes <- read_csv(here("data/csv_dataset/softcite_codes_applied.csv"))
csv_references <- read_csv(here("data/csv_dataset/softcite_references.csv")) 

rdf_software <- csv_articles %>% 
  filter(article_set %in% c("pmc_article", "econ_article")) %>% 
  filter(no_selections_found == F) %>% 
  # only check articles annotated with positive results
  left_join(csv_mentions, by=c("article", "coder")) %>% 
  filter(mention_type == "software") %>% 
  select(-codable, -standard_article, -quote, -tei_quote, -memo, -page) %>% 
  # TODO: there might be duplicated values in the fields dropped
  # make sure that every record is uniquely identified
  distinct()
  # 6,207 selections of software in 1,495 articles

rdf_sw_names <- rdf_software %>% 
  filter(article %in% article_list) %>% 
  distinct() %>% 
  # 6,144 software mentions in 1,483 articles
  left_join(csv_codes, by=c("selection", "coder")) %>% 
  filter(code == "software_name") %>% 
  # 6,147 software mentions after joining
  # TODO: duplicated selections in csv_codes
  distinct() %>% 
  filter(was_code_present == T)
  # 5,607 software mentions with names

rdf_sw_attributes <- rdf_software %>% 
  filter(article %in% article_list) %>% 
  distinct() %>% 
  left_join(csv_codes, by=c("selection", "coder")) %>% 
  filter(code %in% c("software_was_used", "creator", "version_date", "version_number", "url") & was_code_present == T) %>% 
  distinct()
  # 9,472 software attributes codes

rdf_mention_pairs <- rdf_sw_names %>% 
  left_join(rdf_sw_attributes, by=c("selection", "coder")) 
  # 9,433 matched software name-attribute pairs
  # TODO: might missed non-named software -> check backward from TEI corpus

rdf_sw_codes <- rdf_software %>% 
  filter(article %in% article_list) %>% 
  distinct() %>% 
  left_join(csv_codes, by=c("selection", "coder")) %>% 
  filter(code %in% c("software_name", "software_was_used", "creator", "version_number", "version_date", "url")) %>% 
  distinct() 
  # 36,832 codes of software mentions (in pmc and econ articles)

top_annotators <- rdf_sw_codes %>% 
  group_by(article, coder) %>% 
  summarise(n_anno_per_coder=n()) %>% 
  # 1710 article-coder pair (only for recognized software mentions)
  top_n(n=1, wt=n_anno_per_coder)
  # 1,619 top annotators in single articles
  # TODO: here used a slightly different way to construct the table -> check software-processing-summary.Rmd for consistency

top_annotator_filter <- top_annotators %>% 
  select(article, coder) 

missing_top_annotator <- top_annotator_filter %>% 
  left_join(rdf_mention_pairs, by=c("article" = "article.x", "coder")) %>% 
  filter(is.na(article_set.x))
  # TODO: check this subset. why missing? triangulate with the TEI data too

# this should be the baseline for curation including propagation
# possible next steps:
# identify curated annotations and propagated annotations 
# compare baseline?
 
```

```{r, echo=TRUE}
# start with the TEI corpus
# first compare positive articles
positive_corpus <- read_xml("https://raw.githubusercontent.com/howisonlab/softcite-dataset/master/data/corpus/softcite_corpus.tei.xml") %>% xml_ns_strip()

tei_pos_articles <- positive_corpus %>% 
  xml_find_all(xpath = "//fileDesc[@xml:id]") %>% 
  xml_attrs() %>% unlist() %>% 
  tibble::enframe(name=NULL) %>% 
  left_join(included_article, by=c("value"="id")) %>% 
  rename(id=value)
  # 1,240 positive articles in the corpus

tei_pos_article_filter <- tei_pos_articles %>% 
  distinct(origin) %>% pull()
  # only 1,228 distinct file name

rdf_filtered_pos_article <- rdf_sw_codes %>% 
  distinct(article) %>%  
  filter(article %in% tei_pos_article_filter) %>% pull
  # get 1,220 articles. 

# TODO: why are these positive articles not in TEI corpus?
# possible loss during document processing (i.e. different starting point)
setdiff(tei_pos_article_filter, rdf_filtered_pos_article)
```

```{r, echo=TRUE}
# then compare negative articles
# TODO: check consistency with softcite-processing-summary.Rmd

```

# 2. Check `publisher` loss due to the drop of reference

```{r}
# a quick way would be to strip all the ref
# perhaps better check all fields

#xml_find_num(tei_doc, xpath="count(//rs[.//text('ref')])")
```


# 3. Review `</ab>`