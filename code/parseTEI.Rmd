---
title: "parse-tei"
author: "Fan Du and James Howison"
date: "4/9/2020"
output: html_document
---

```{r}
library(xml2)
library(tidyverse)

xml_doc = read_xml("https://raw.githubusercontent.com/ourresearch/software-mentions/master/resources/dataset/software/corpus/all_clean_post_processed.tei.xml")

# start with rs tags
rs_tags <- xml_find_all(xml_doc, 
                        ns = c("tei" = "http://www.tei-c.org/ns/1.0"),
                        xpath = "//tei:rs") 

# extract the text
xml_text_col <- rs_tags %>% 
  map(xml_text)

# extract the xml_attrs (comes as named vector)
xml_attr_col <- rs_tags %>% 
  map(xml_attrs)
         
extracted <- tibble(xml_text_col,
                    xml_attr_col) %>% # named vector becomes a tibble.
  # groovy tidyr function to unnest that tibble column
  unnest_wider(xml_attr_col) %>% unnest(cols = c(xml_text_col)) %>% 
  mutate(corresp = str_sub(corresp, 2, -1)) %>% 
  mutate(id = if_else(is.na(id), corresp, id)) %>% 
  select(-corresp)

extracted %>% select(id, type, xml_text_col) %>% 
  group_by(id) %>% 
  summarise(n_field = n_distinct(type)) %>% 
  filter(n_field == 4)
# only 11 software mentions come along with publisher, version, and url all 3 fields

extracted <- extracted %>% 
  group_by(id) %>%
  mutate(row = row_number()) %>% ungroup() %>% 
  # assign a unique identifier within each mention cluster as pivot_wider does not take duplicated observations
  pivot_wider(names_from = "type", values_from = "xml_text_col") %>% 
  select(-row) %>% 
  separate(id, into = c("article", "drop1", "drop2"), sep = "-", remove = F) %>% 
  select(-drop1, -drop2)
# need to check here again later
```

Descriptive statistics of rs tags

```{r}
library(here)

# 6,695 annotations
top10 <- extracted %>% filter(!is.na(software)) %>% 
  # 4,131 software mentions
  group_by(software) %>% 
  # 1,506 distinct pieces of software
  summarise(n_mention = n()) %>% 
  arrange(desc(n_mention)) %>% head(10)  
top10
  # write_csv(here::here("output/top10-software-mentions.csv"))
  
extracted %>% right_join(top10, by="software") %>% 
  select(id) %>% left_join(extracted, by="id") %>% 
  filter(!is.na(url)) %>% distinct(id)
# 842 top 10 software mentions:
# 355 with publisher, 498 with version, 17 with url

extracted %>% filter(!is.na(version)) %>% 
# 1,274 version tags
  distinct(id)
# 1,259 software mentions with version

extracted %>% filter(!is.na(publisher)) %>%
  group_by(id) %>% 
  summarise(n_publisher = n()) %>% 
  arrange(desc(n_publisher))

extracted %>% filter(!is.na(publisher)) %>%
  group_by(publisher) %>% 
  summarise(n_publisher = n()) %>% 
  arrange(desc(n_publisher))
# 1,119 publisher tags
# 1,115 software mentioned with publisher
# 451 distinct publishers

extracted %>% filter(!is.na(url)) %>% 
# 171 url tags
  group_by(id) %>% 
  summarise(n_url = n()) %>% 
  arrange(desc(n_url))

extracted %>% filter(id == "PMC3886109-software-0")

extracted %>% filter(resp == "#curator")
# 1,217 curated tags

extracted <- extracted %>% 
  mutate(annotation_id = row_number()) 

extracted %>% 
  group_by(resp) %>% 
  summarise(n_annotation = n_distinct(annotation_id)) %>% 
  arrange(desc(n_annotation))

extracted %>%  
  filter(!is.na(software)) %>% 
  group_by(article) %>%
  summarise(n_annotation = n_distinct(annotation_id)) %>%
  summarise(avg_annotation = mean(n_annotation)) 
# 5.5 annotations per article
# 3.4 software mentions per article

annotation_per_article <- extracted %>% 
  group_by(article) %>% 
  summarise(n_annotation = n_distinct(annotation_id)) %>% 
  mutate(category = "annotations in all fields")

mention_per_article <- extracted %>%
  filter(!is.na(software)) %>% 
  group_by(article) %>% 
  summarise(n_annotation = n_distinct(annotation_id)) %>% 
  mutate(category = "software mention")

bind_rows(mention_per_article, annotation_per_article) %>% 
  as_tibble() %>% 
  ggplot(aes(x=n_annotation)) +
  geom_histogram(bins = 100) + 
  facet_grid(category ~ .) +
  labs(x="Number of annotation", y="count of articles",
  title="Distribution of article counts over number of annotations per article")


extracted %>% filter(is.na(resp))

extracted %>% group_by(article) %>% 
# 1,216 articles in the corpus contain rs tags
  summarise(n_mention = n()) %>% 
  arrange(n_mention)
# article counts may not be accurate to be counted from rs tags
# need to query the xml again
```

Getting statistics of articles and paragraphs 

```{r}
library(xml2)

read_xml("https://raw.githubusercontent.com/ourresearch/software-mentions/master/resources/dataset/software/corpus/all_clean_post_processed.tei.xml") %>% 
  xml_find_all(ns = c("tei" = "http://www.tei-c.org/ns/1.0"),
               xpath = "//tei:fileDesc/@xml:id") %>% 
  xml_text %>% 
  length()
```


```{r}
articles <- read_csv("~/softcite-dataset/data/csv_dataset/softcite_articles.csv")

articles %>% 
  select(-coder) %>% 
  distinct() %>% 
  group_by(article_set, no_selections_found) %>% 
  tally()

```



```{r}
para <- xml_find_all(xml_doc, 
                           ns = c("tei" = "http://www.tei-c.org/ns/1.0"),
                           xpath = "//tei:body/tei:p") %>% xml_text
para %>% head(2)
str(para)
# 2,317 paragraphs in 1,247 articles


# below is incomplete experimental code
# might need the pluck option
# currently cannot extract the article-paragraph-rs tags hierarchy
# so cannot uniquely identify negatively annotated paragraphs


# if use xml_ns_strip can avoiding repeated adding namespace arg?
full_excerpts <- xml_find_all(xml_doc, 
                           ns = c("tei" = "http://www.tei-c.org/ns/1.0"),
                           xpath = "//tei:TEI") %>% xml_text
full_excerpts %>% head(2)

all_article_attributes <- xml_find_all(xml_doc, 
                           ns = c("tei" = "http://www.tei-c.org/ns/1.0"),
                           xpath = "//tei:TEI") %>% xml_attrs
all_article_attributes %>% head(2)

# get article set

  tibble(text = map_chr(., xml_text),
         # attr as a tibble column, pluck needed to unpack one
         # level of list.
         xml_attr_col = map(., xml_attrs) %>% map(pluck,1)) %>% 
   # drop original node column somehow created by tibble
  select(-1) %>%
  # groovy function to unnest that tibble column
  unnest_wider(xml_attr_col) %>% 
  View()
```
April 10, 2020.

Tried routes:

Route 1:
# rs_tags_test <- rs_tags %>% head(10)
# rs_tibble <- rs_tags_test %>% map(xml_attrs) %>% map_df(bind_rows)

# getting tag value
# xml_find_chr(xml_doc, ns = c("tei" = "http://www.tei-c.org/ns/1.0"),
#              xpath = "string(//tei:rs[@xml:id='PMC4750616-software-0']/text())")

# tried separate as alternative to extract
# rs_tibble %>% 
#   separate(id, into = c("article", "drop1", "drop2"), sep = "-", remove = F) %>% 
#   select(-drop1, -drop2)


rs_tibble <- rs_tags %>%
          # see https://stackoverflow.com/questions/40036207/
          # tidyverse-prefered-way-to-turn-a-named-vector-into-a-data-frame-tibble
             map(xml_attrs) %>%
             map_df(bind_rows) %>%
             extract(id, into = c("article"), regex = "(.*?)-", remove = F) %>%
             filter(! is.na(id)) %>%
             mutate(xpath_string = str_glue("string(//tei:rs[@xml:id='{id}']/text())")) %>%
             mutate(software = map_chr(xpath_string, xml_find_chr, x = xml_doc,
                             ns = c("tei" = "http://www.tei-c.org/ns/1.0"))) %>%
             select(-corresp)
# 4,131 software mentions


rs_details <- rs_tags %>%
  map(xml_attrs) %>%
  map_df(bind_rows) %>%
  filter(is.na(id)) %>%
  select(-subtype, -id, -cert) %>%
  mutate(id = str_sub(corresp, 2, -1),
         property = type,
         property_resp = resp) %>%
  # changing column name in prep for joining with another table
  select(-type, -resp, -corresp) %>%
  mutate(xpath_string_2 = str_glue("string(//tei:rs[@type='{property}']/text())")) %>%
  mutate(property_text = map_chr(xpath_string_2, xml_find_chr, x = xml_doc,
                             ns = c("tei" = "http://www.tei-c.org/ns/1.0")))

rs_details %>%
  select(-xpath_string_2) %>%
  pivot_wider(names_from = property, values_from = property_text)
# interestingly pivot_wider does not work well with glued data
# seems some clues here: https://github.com/tidyverse/tidyr/issues/676

Route 2:
# tibbling tags
# rs_tibble <- rs_tags %>% head(10) %>% map(my_xml_extract) %>% map_df(bind_rows)
# 
# my_xml_extract <- function(rs_tag) {
#   # rs_tag <- rs_tags %>% head(1)
#   rs_tag %>% 
#     map(xml_attrs) %>% 
#     map_df(bind_rows) %>% 
#     mutate(text = xml_text(rs_tag))
# }

# my_xml_extract <- function(xml_doc) {
#   xml_doc %>%
#     map(xml_find_all, ns = c("tei" = "http://www.tei-c.org/ns/1.0"),
#                         xpath = "//tei:rs") %>%
#     map(xml_attrs) %>%
#     map_df(bind_rows) %>%
#     mutate(text = xml_text(rs_tag))
# }
# map() does not work on non-lists/-vectors


```{r}

txt <- c('<node attrA="1A" attrB="1B">text1</node>',
         '<node attrA="2A" attrB="2B">text2</node>')

txt %>% 
  map(read_xml) %>% 
  map(xml_find_all, xpath = "//node") %>% 
  tibble(text = map_chr(., xml_text),
         # attr as a tibble column, pluck needed to unpack one
         # level of list.
         xml_attr_col = map(., xml_attrs) %>% map(pluck,1)) %>% 
   # drop original node column somehow created by tibble
  select(-1) %>%
  # groovy function to unnest that tibble column
  unnest_wider(xml_attr_col) %>% 
  View()

  map(as_list) %>% 
  tibble(text = map(.,pluck,1,1,1),
         values = map(., pluck, 1, attributes, 1),
         attr_names = map(., pluck, 1, attributes, names)) %>% View

as_lists <- txt %>% 
  map(read_xml) %>% 
  map(xml_find_all, xpath = "//node") %>% 
  map(as_list)

text <- as_lists %>% 
  map(pluck,1,1,1)

  
as_lists %>% 
  map(pluck, 1, attributes, 2)

attr_names <- as_lists %>% 
  map(pluck, 1, attributes, names)

rm(names)
```

```{r}
library(holepunch)
write_compendium_description(package = "Softcite Dataset", 
                             description = "Summary of Softcite Dataset")
```

