---
title: "parse-tei"
author: "Fan Du and James Howison"
date: "4/9/2020"
output: html_document
---

```{r}
library(xml2)
library(tidyverse)

xml_doc <- read_xml("https://raw.githubusercontent.com/ourresearch/software-mentions/master/resources/dataset/software/corpus/all_clean_post_processed.tei.xml")

# start with rs tags
rs_tags <- xml_find_all(xml_doc, 
                        ns = c("tei" = "http://www.tei-c.org/ns/1.0"),
                        xpath = "//tei:rs") 

# extract the text
xml_text_col <- rs_tags %>% 
  map(xml_text)

# extract the xml_attrs (comes as named vector)
xml_attr_col <- rs_tags %>% 
  map(xml_attrs)
         
extracted <- tibble(xml_text_col,
                    xml_attr_col) %>% # named vector becomes a tibble.
  # groovy tidyr function to unnest that tibble column
  unnest_wider(xml_attr_col) %>% unnest(cols = c(xml_text_col)) %>% 
  mutate(corresp = str_sub(corresp, 2, -1)) %>% 
  mutate(id = if_else(is.na(id), corresp, id)) %>% 
  select(-corresp)

extracted %>% select(id, type, xml_text_col) %>%
  group_by(id) %>%
  summarise(n_field = n_distinct(type)) %>%
  filter(n_field == 4)
# only 11 software mentions come along with publisher, version, and url all 3 fields

extracted <- extracted %>% 
  group_by(id) %>%
  mutate(row = row_number()) %>% ungroup() %>% 
  # assign a unique identifier within each mention cluster as pivot_wider does not take duplicated observations
  pivot_wider(names_from = "type", values_from = "xml_text_col") %>% 
  select(-row) %>% 
  extract(id, into = "article", "^(.+)(?=-software)" , remove = F)

# check if any failed extraction
extracted %>% filter(is.na(article)) %>% select(id)
```

Descriptive statistics of rs tags

```{r}
# 6,695 annotations
top10 <- extracted %>% filter(!is.na(software)) %>% 
  # 4,131 software mentions
  group_by(software) %>% 
  # 1,506 distinct pieces of software
  summarise(n_mention = n()) %>% 
  arrange(desc(n_mention)) %>% head(10)  
top10
  # write_csv(here::here("output/top10-software-mentions.csv"))
  
extracted %>% right_join(top10, by="software") %>% 
  select(id) %>% distinct(id) %>% 
  inner_join(extracted, by="id") %>% 
  filter(!is.na(url))
# 736 top 10 software mentions:
# 357 with publisher, 501 with version, 17 with url

extracted %>% filter(!is.na(version)) %>% 
# 1,273 version tags
  distinct(id)
# 1,260 software mentions with version

extracted %>% filter(!is.na(url)) %>%
  group_by(id) %>% 
  summarise(n_url = n()) %>% 
  arrange(desc(n_url))
# check software mention with more than 1 publisher/version/url
extracted %>% filter(id == "PMC3886109-software-0")

extracted %>% filter(!is.na(publisher)) %>% #distinct(id)
  group_by(publisher) %>% 
  summarise(n_publisher = n()) %>% 
  arrange(desc(n_publisher)) 
# 1,119 publisher tags
# 1,117 software mentioned with publisher
# 451 distinct publishers

extracted %>% filter(!is.na(url)) %>% 
# 172 url tags
  group_by(id) %>% 
  summarise(n_url = n()) %>% 
  arrange(desc(n_url))

extracted %>% filter(resp == "#curator")
# 1,217 curated tags

extracted <- extracted %>% 
  mutate(annotation_id = row_number()) 

extracted %>% 
  group_by(resp) %>% 
  summarise(n_annotation = n_distinct(annotation_id)) %>% 
  arrange(desc(n_annotation))
# individually produced annotations range from 1 to 939 

extracted %>%  
  #filter(!is.na(software)) %>% 
  group_by(article) %>%
  summarise(n_annotation = n_distinct(annotation_id)) %>%
  # 1,233 articles with software mentions
  arrange(desc(n_annotation)) %>% 
  # software mentions per article range from 1 to 137
  # annotations per article range from 1 to 144
  summarise(avg_annotation = mean(n_annotation)) 
# 3.35 software mentions per article
# 5.43 annotations per article
```

Viz annotation stats

```{r}
annotation_per_article <- extracted %>% 
  group_by(article) %>% 
  summarise(n_annotation = n_distinct(annotation_id)) %>% 
  mutate(category = "annotations in all fields")

mention_per_article <- extracted %>%
  filter(!is.na(software)) %>% 
  group_by(article) %>% 
  summarise(n_annotation = n_distinct(annotation_id)) %>% 
  mutate(category = "software mention")

bind_rows(mention_per_article, annotation_per_article) %>% 
  as_tibble() %>% 
  ggplot(aes(x=n_annotation)) +
  geom_histogram(bins = 100) + 
  facet_grid(category ~ .) +
  labs(x="Number of annotation", y="count of articles",
  title="Distribution of article counts over number of annotations per article")


extracted %>% filter(is.na(resp))

extracted %>% group_by(article) %>% 
# 1,233 articles in the corpus contain rs tags
  summarise(n_mention = n()) %>% 
  arrange(n_mention)
# article counts may not be accurate to be counted from rs tags
# need to query the xml again
```

Getting statistics of articles and paragraphs 

```{r}
library(xml2)

read_xml("https://raw.githubusercontent.com/ourresearch/software-mentions/master/resources/dataset/software/corpus/all_clean_post_processed.tei.xml") %>% 
  xml_find_all(ns = c("tei" = "http://www.tei-c.org/ns/1.0"),
               xpath = "//tei:fileDesc/@xml:id") %>% 
  xml_text %>% 
  length()

# 1,247 articles with mentions found
```


```{r}
library(here)
articles <- read_csv(here("data/csv_dataset/softcite_articles.csv"))

articles %>% 
  select(-coder) %>% 
  distinct() %>% 
  group_by(article_set, no_selections_found) %>% 
  tally()
# 3,431 articles without mentions found
```



```{r}
para <- xml_find_all(xml_doc, 
                           ns = c("tei" = "http://www.tei-c.org/ns/1.0"),
                           xpath = "//tei:body/tei:p") %>% xml_text
para %>% head(2)
str(para)
# 2,317 paragraphs in 1,247 articles


# below is incomplete experimental code
# might need the pluck option
# currently cannot extract the article-paragraph-rs tags hierarchy
# so cannot uniquely identify negatively annotated paragraphs

full_excerpts <- xml_find_all(xml_doc, 
                           ns = c("tei" = "http://www.tei-c.org/ns/1.0"),
                           xpath = "//tei:TEI") %>% xml_text
full_excerpts %>% head(2)

all_article_attributes <- xml_find_all(xml_doc, 
                           ns = c("tei" = "http://www.tei-c.org/ns/1.0"),
                           xpath = "//tei:TEI") %>% xml_attrs
all_article_attributes %>% head(2)

# get article set

  tibble(text = map_chr(., xml_text),
         # attr as a tibble column, pluck needed to unpack one
         # level of list.
         xml_attr_col = map(., xml_attrs) %>% map(pluck,1)) %>% 
   # drop original node column somehow created by tibble
  select(-1) %>%
  # groovy function to unnest that tibble column
  unnest_wider(xml_attr_col) %>% 
  View()
```

April 10, 2020.

# Tried routes:

# Route 1:
# rs_tags_test <- rs_tags %>% head(10)
# rs_tibble <- rs_tags_test %>% map(xml_attrs) %>% map_df(bind_rows)

# getting tag value
# xml_find_chr(xml_doc, ns = c("tei" = "http://www.tei-c.org/ns/1.0"),
#              xpath = "string(//tei:rs[@xml:id='PMC4750616-software-0']/text())")

# tried separate as alternative to extract
# rs_tibble %>% 
#   separate(id, into = c("article", "drop1", "drop2"), sep = "-", remove = F) %>% 
#   select(-drop1, -drop2)


# rs_tibble <- rs_tags %>%
#           # see https://stackoverflow.com/questions/40036207/
#           # tidyverse-prefered-way-to-turn-a-named-vector-into-a-data-frame-tibble
#              map(xml_attrs) %>%
#              map_df(bind_rows) %>%
#              extract(id, into = c("article"), regex = "(.*?)-", remove = F) %>%
#              filter(! is.na(id)) %>%
#              mutate(xpath_string = str_glue("string(//tei:rs[@xml:id='{id}']/text())")) %>%
#              mutate(software = map_chr(xpath_string, xml_find_chr, x = xml_doc,
#                              ns = c("tei" = "http://www.tei-c.org/ns/1.0"))) %>%
#              select(-corresp)
# # 4,131 software mentions


# rs_details <- rs_tags %>%
#   map(xml_attrs) %>%
#   map_df(bind_rows) %>%
#   filter(is.na(id)) %>%
#   select(-subtype, -id, -cert) %>%
#   mutate(id = str_sub(corresp, 2, -1),
#          property = type,
#          property_resp = resp) %>%
#   # changing column name in prep for joining with another table
#   select(-type, -resp, -corresp) %>%
#   mutate(xpath_string_2 = str_glue("string(//tei:rs[@type='{property}']/text())")) %>%
#   mutate(property_text = map_chr(xpath_string_2, xml_find_chr, x = xml_doc,
#                              ns = c("tei" = "http://www.tei-c.org/ns/1.0")))
# 
# rs_details %>%
#   select(-xpath_string_2) %>%
#   pivot_wider(names_from = property, values_from = property_text)
# # interestingly pivot_wider does not work well with glued data
# # seems some clues here: https://github.com/tidyverse/tidyr/issues/676
# 
# Route 2:
# tibbling tags
# rs_tibble <- rs_tags %>% head(10) %>% map(my_xml_extract) %>% map_df(bind_rows)
# 
# my_xml_extract <- function(rs_tag) {
#   # rs_tag <- rs_tags %>% head(1)
#   rs_tag %>% 
#     map(xml_attrs) %>% 
#     map_df(bind_rows) %>% 
#     mutate(text = xml_text(rs_tag))
# }

# my_xml_extract <- function(xml_doc) {
#   xml_doc %>%
#     map(xml_find_all, ns = c("tei" = "http://www.tei-c.org/ns/1.0"),
#                         xpath = "//tei:rs") %>%
#     map(xml_attrs) %>%
#     map_df(bind_rows) %>%
#     mutate(text = xml_text(rs_tag))
# }
# map() does not work on non-lists/-vectors


```{r}

txt <- c('<node attrA="1A" attrB="1B">text1</node>',
         '<node attrA="2A" attrB="2B">text2</node>')

txt %>% 
  map(read_xml) %>% 
  map(xml_find_all, xpath = "//node") %>% 
    tibble(text = map_chr(., xml_text),
         # attr as a tibble column, pluck needed to unpack one
         # level of list.
         xml_attr_col = map(., xml_attrs) %>% map(pluck,1)) %>% 
   # drop original node column somehow created by tibble
  select(-1) %>%
  # groovy function to unnest that tibble column
  unnest_wider(xml_attr_col) %>% 
  View()
# work for parallel nodesets with consistent structures

  map(as_list) %>% 
  tibble(text = map(.,pluck,1,1,1),
         values = map(., pluck, 1, attributes, 1),
         attr_names = map(., pluck, 1, attributes, names)) %>% View

as_lists <- txt %>% 
  map(read_xml) %>% 
  map(xml_find_all, xpath = "//node") %>% 
  map(as_list)

text <- as_lists %>% 
  map(pluck,1,1,1)
  
as_lists %>% 
  map(pluck, 1, attributes, 2)

attr_names <- as_lists %>% 
  map(pluck, 1, attributes, names)

rm(names)
```

```{r}
library(holepunch)
write_compendium_description(package = "Softcite Dataset", 
                             description = "Summary of Softcite Dataset")
```

May 7, 2020.

Now with the updated TEI XML corpus containing articles analyzed with no mentions.

```{r}
library(xml2)
library(tidyverse)

xml_doc <- read_xml("https://raw.githubusercontent.com/ourresearch/software-mentions/master/resources/dataset/software/corpus/all_clean_post_processed_with_no_mention.tei.xml") %>% xml_ns_strip()

pdf_title <- xml_find_all(xml_doc, xpath = "//title") %>% map(xml_text) 
title_length <- pdf_title %>% map(str_length) %>% unlist 

# check articles missing titles in TEI XML
empty_title_location <- which(title_length == 0)
# 123 missing titles
```

Document-level:
(Have to be grouped by xml_text?)
- table 1: <fileDesc>. article id and title.
- table 2: <profileDesc>. post-processing annotation.
(All positive and negative articles have these two fields.)
- table 3: paragraphs

Instance-level:
- table 4: software annotations

```{r}
library(xml2)
library(tidyverse)

xml_doc <- read_xml("https://raw.githubusercontent.com/ourresearch/software-mentions/master/resources/dataset/software/corpus/all_clean_post_processed_with_no_mention.tei.xml") %>% xml_ns_strip()

fileDesc <- xml_doc %>% xml_find_all(xpath="//fileDesc") 
fileDesc <- fileDesc[-1] # remove the header

title <- fileDesc %>% xml_text
article_id <- fileDesc %>% xml_attrs %>% map(pluck,1) %>% unlist()

fileDesc <- tibble(article_id, title)

fileDesc %>%   
  tibble(title = map(., xml_text), 
         attrs = map(., xml_attrs) %>% map(pluck,1) %>% unlist())

```

Seems that the dot pronoun does not work with xml nodeset well.
Have to work with the "least parsable unit" one by one?
Can Xquery to get several field values at the same time? (if cannot pluck to them at the same level)
