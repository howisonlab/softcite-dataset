{
    "level": "paragraph",
    "abstract": [
        {
            "text": "The LHCb collaboration has redesigned its trigger to enable the full offline detector reconstruction to be performed in real time. Together with the real-time alignment and calibration of the detector, and a software infrastructure to make persistent the high-level physics objects produced during real-time processing, this redesign enabled the widespread deployment of real-time analysis during Run 2. We describe the design of the Run 2 trigger and real-time reconstruction, and present data-driven performance measurements for a representative sample of LHCb's physics programme.",
            "paragraph_rank": 0,
            "section_rank": 1
        }
    ],
    "body_text": [
        {
            "text": "Introduction",
            "section_rank": 2
        },
        {
            "section": "Introduction",
            "text": "The LHCb experiment is a dedicated heavy-flavour physics experiment at the LHC, focused on the reconstruction of particles containing c and b quarks. During Run 1, the LHCb physics programme was extended to electroweak, soft QCD and even heavy-ion physics. This was made possible in large part due to a versatile real-time reconstruction and trigger system, which is responsible for reducing the rate of collisions saved for offline analysis by three orders of magnitude. The trigger used by LHCb in Run 1 [1] executed a simplified two-stage version of the full offline reconstruction. In the first stage, only charged particles with at least \u223c 1 GeV/c of transverse momentum (p T ) and displaced from the primary vertex (PV) were available; the p T threshold was somewhat lower for muons, which in addition were not required to be displaced. This first stage reconstruction enabled the bunch crossing rate to be reduced efficiently by roughly one order of magnitude. In the following second stage, most charged particles with p T 300 MeV/c were available to classify the bunch crossings (hereafter \"events\"). Particle-identification information and neutral particles such as photons or \u03c0 0 mesons were available on-demand to specific classification algorithms. Although this trigger enabled the majority of the LHCb physics programme, the lack of low-momentum charged particles at the first stage and full particle identification at the second stage limited the performance for c-hadron physics in particular. In addition, resolution differences between the online and offline reconstructions made it difficult to precisely understand absolute trigger efficiencies.",
            "paragraph_rank": 1,
            "section_rank": 2,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b0",
                    "start": 506,
                    "text": "[1]",
                    "end": 509
                }
            ]
        },
        {
            "section": "Introduction",
            "text": "For these reasons, the LHCb trigger system was redesigned during 2013-2015 to perform the full offline event reconstruction. The entire data processing framework was redesigned to enable a single coherent real-time detector alignment and calibration, as well as real-time analyses using information directly from the trigger system. The key objectives of this redesign were twofold: firstly, to enable the full offline reconstruction to run in the trigger, greatly increasing the efficiency with which charm-and strange-hadron decays could be selected; and secondly, to achieve the same quality of alignment and calibration within the trigger as was achieved offline in Run 1, enabling the final signal selection to be performed at the trigger level.",
            "paragraph_rank": 2,
            "section_rank": 2
        },
        {
            "section": "Introduction",
            "text": "A schematic diagram showing the trigger data flow in Run 2 is depicted in figure 1. The LHCb trigger is designed to allow datataking with minimal deadtime at the full LHC bunch crossing rate of 40 MHz. The maximum rate at which all LHCb subdetectors can be read out is imposed by the bandwidth and frequency of the front-end electronics, and corresponds to around 1.1 MHz when running at the designed rate of visible interactions per bunch crossing in LHCb of \u00b5 = 0.4. During Run 2 LHCb operated at \u00b5 = 1.1 in order to collect a greater integrated luminosity, which limited the actual readout rate to about 1 MHz. A system of field-programmable gate arrays with a fixed latency -1 -  of 4 \u00b5s (the L trigger) determines which events are kept. Information from the electromagnetic calorimeter, hadronic calorimeter, and muon stations is used in separate L trigger lines.",
            "paragraph_rank": 3,
            "section_rank": 2
        },
        {
            "text": "JINST 14 P04013",
            "section_rank": 3
        },
        {
            "section": "JINST 14 P04013",
            "text": "The High Level trigger (HLT) is divided into two stages, HLT1 and HLT2. The first level of the software trigger performs an inclusive selection of events based on one-or two-track signatures, on the presence of muon tracks displaced from the PVs, or on dimuon combinations in the event. Events selected by the HLT1 trigger are buffered to disk storage in the online system. This is done for two purposes: events can be processed further during inter-fill periods, and the detector can be calibrated and aligned run-by-run before the HLT2 stage. Once the detector is aligned and calibrated, events are passed to HLT2, where a full event reconstruction is performed. This allows for a wide range of inclusive and exclusive final states to trigger the event and obviates the need for further offline processing.",
            "paragraph_rank": 4,
            "section_rank": 3
        },
        {
            "section": "JINST 14 P04013",
            "text": "This paper describes the design and performance of the Run 2 LHCb trigger system, including the real-time reconstruction which runs in the HLT. The software framework enabling real-time analysis (\"TURBO\") has been described in detail elsewhere. The initial proof-of-concept deployed in 2015 [2] allowed offline-quality signal candidates selected in the trigger to be written to permanent storage. It also allowed physics analysts to use the offline analysis tools when working with these candidates, which was crucial in enabling LHCb to rapidly produce a number of publications proving that real-time analysis was possible without losing precision or introducing additional systematics. Subsequent developments [3] generalized this approach to allow not only the signal candidate but also information about other, related, particles in the event to be saved. These developments also transformed the proof-of-concept implementation into a scalable solution which will now form the basis of LHCb's upgrade computing model [4]. ",
            "paragraph_rank": 5,
            "section_rank": 3,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b1",
                    "start": 291,
                    "text": "[2]",
                    "end": 294
                },
                {
                    "type": "bibr",
                    "ref_id": "b2",
                    "start": 712,
                    "text": "[3]",
                    "end": 715
                },
                {
                    "type": "bibr",
                    "ref_id": "b3",
                    "start": 1021,
                    "text": "[4]",
                    "end": 1024
                }
            ]
        },
        {
            "text": "The LHCb detector",
            "section_rank": 4
        },
        {
            "section": "The LHCb detector",
            "text": "The LHCb detector [5,6] is a single-arm forward spectrometer covering the pseudorapidity range 2 < \u03b7 < 5. The detector coordinate system is such that z is along the beam line and x is the direction in which charged particle trajectories are deflected by the magnetic field. The detector includes a high-precision tracking system consisting of a silicon-strip vertex detector (VELO) surrounding the pp interaction region [7], a large-area silicon-strip detector (TT) located upstream of a dipole magnet with a bending power of about 4 Tm, and three stations of silicon-strip detectors (IT) and straw drift tubes [8] (OT) placed downstream of the magnet. These are collectively referred to as the T-stations. The tracking system provides a measurement of momentum, p, of charged particles with a relative uncertainty that varies from 0.5% at low momentum to 1.0% at 200 GeV/c. A sketch of the various track types relevant in LHCb is shown in figure 2.",
            "paragraph_rank": 6,
            "section_rank": 4,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b4",
                    "start": 18,
                    "text": "[5,",
                    "end": 21
                },
                {
                    "type": "bibr",
                    "ref_id": "b5",
                    "start": 21,
                    "text": "6]",
                    "end": 23
                },
                {
                    "type": "bibr",
                    "ref_id": "b6",
                    "start": 420,
                    "text": "[7]",
                    "end": 423
                },
                {
                    "type": "bibr",
                    "ref_id": "b7",
                    "start": 611,
                    "text": "[8]",
                    "end": 614
                }
            ]
        },
        {
            "section": "The LHCb detector",
            "text": "The minimum distance of a track to a PV, the impact parameter, is measured with a resolution of (15 + [29 GeV/c]/p T ) \u00b5m. Different types of charged hadrons are distinguished using information from two ring-imaging Cherenkov detectors [9]. Photons, electrons and hadrons are identified by a calorimeter system consisting of scintillating-pad (SPD) and preshower detectors (PS), an electromagnetic calorimeter (ECAL) and a hadronic calorimeter (HCAL). Muons are identified by a system composed of alternating layers of iron and multiwire proportional chambers (MUON) [10].",
            "paragraph_rank": 7,
            "section_rank": 4,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b8",
                    "start": 236,
                    "text": "[9]",
                    "end": 239
                },
                {
                    "type": "bibr",
                    "ref_id": "b9",
                    "start": 567,
                    "text": "[10]",
                    "end": 571
                }
            ]
        },
        {
            "section": "The LHCb detector",
            "text": "The LHCb detector data taking is divided into fills and runs. A fill is a single period of collisions delimited by the announcement of stable beam conditions and the dumping of the beam by the LHC, and typically lasts around twelve hours. A fill is subdivided into runs, each of which lasts a maximum of one hour. The downtime associated with run changes is negligible compared to other sources of downtime.",
            "paragraph_rank": 8,
            "section_rank": 4
        },
        {
            "section": "The LHCb detector",
            "text": "Detector simulation has been used in the tuning of most reconstruction and selection algorithms discussed in this paper. In simulated LHCb events, pp collisions are generated using P [11,12] with a specific LHCb configuration [13]. Decays of hadronic particles are described by E G [14], in which final-state radiation is generated using P [15]. The interaction of the generated particles with the detector, and its response, are implemented using the G 4 toolkit [16,17] as described in ref. [18].",
            "paragraph_rank": 9,
            "section_rank": 4,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b10",
                    "start": 183,
                    "text": "[11,",
                    "end": 187
                },
                {
                    "type": "bibr",
                    "ref_id": "b11",
                    "start": 187,
                    "text": "12]",
                    "end": 190
                },
                {
                    "type": "bibr",
                    "ref_id": "b12",
                    "start": 226,
                    "text": "[13]",
                    "end": 230
                },
                {
                    "type": "bibr",
                    "ref_id": "b13",
                    "start": 282,
                    "text": "[14]",
                    "end": 286
                },
                {
                    "type": "bibr",
                    "ref_id": "b14",
                    "start": 340,
                    "text": "[15]",
                    "end": 344
                },
                {
                    "type": "bibr",
                    "ref_id": "b15",
                    "start": 464,
                    "text": "[16,",
                    "end": 468
                },
                {
                    "type": "bibr",
                    "ref_id": "b16",
                    "start": 468,
                    "text": "17]",
                    "end": 471
                },
                {
                    "type": "bibr",
                    "ref_id": "b17",
                    "start": 493,
                    "text": "[18]",
                    "end": 497
                }
            ]
        },
        {
            "section": "The LHCb detector",
            "text": "-3 -",
            "paragraph_rank": 10,
            "section_rank": 4
        },
        {
            "text": "JINST 14 P04013",
            "section_rank": 5
        },
        {
            "text": "Data acquisition and the LHCb trigger",
            "section_rank": 6
        },
        {
            "section": "Data acquisition and the LHCb trigger",
            "text": "All trigger systems consist of a set of algorithms that classify events (or parts thereof) as either interesting or uninteresting for further analysis, so that the data rate can be reduced to a manageable level by keeping only interesting events or interesting parts of them. It is conventional to refer to a single trigger classification algorithm as a \"line\", so that a trigger consists of a set of trigger lines.",
            "paragraph_rank": 11,
            "section_rank": 6
        },
        {
            "text": "Hardware trigger",
            "section_rank": 7
        },
        {
            "section": "Hardware trigger",
            "text": "The energies deposited in the SPD, PS, ECAL and HCAL are used in the L -calorimeter system to trigger the selection of events. All detector components are segmented transverse to the beam axis into cells of different size. The decision to trigger an event is based on the transverse energy deposited in clusters of 2 \u00d7 2 cells in the ECAL and HCAL. The transverse energy of a cluster is defined as",
            "paragraph_rank": 12,
            "section_rank": 7
        },
        {
            "section": "Hardware trigger",
            "text": "where E i is the energy deposited in cell i and \u03b8 i is the angle between the z-axis and a line from the cell centre to the average pp interaction point (for more details, see ref. [1]). Additionally, information from the SPD and PS systems is used to distinguish between hadron, photon and electron candidates. The L -muon trigger searches for straight-line tracks in the five muon stations. Each muon station is sub-divided into logical pads in the x-y plane. The pad size scales with the distance to the beam line. The track direction is used to estimate the p T of a muon candidate, assuming that the particle originated from the interaction point and received a single kick from the magnetic field. The p T resolution of the L -muon trigger is about 25% averaged over the relevant p T range. The trigger decision is based on the two muon candidates with the largest p T : either the largest p T must be above the L Muon threshold, or the product of the largest and second largest p T values must be above the L DiMuon threshold. In addition there are special trigger lines that select events with low particle multiplicity to study central exclusive production and inclusive jet trigger lines for QCD measurements.",
            "paragraph_rank": 13,
            "section_rank": 7,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b0",
                    "start": 180,
                    "text": "[1]",
                    "end": 183
                }
            ]
        },
        {
            "section": "Hardware trigger",
            "text": "To reduce the complexity of events and, hence, to enable a faster reconstruction in the subsequent software stage, a requirement is placed on the maximum number of SPD hits in most L trigger lines. The L DiMuon trigger accepts a low rate of events, and therefore, only a loose SPD requirement is applied, while no SPD requirement is applied in the high p T L Muon trigger used for electroweak production analyses in order to avoid systematic uncertainties associated with the determination of the corresponding efficiency. The thresholds used to take the majority of the data are listed in table 1 as a function of the year of data taking. Note that while the use of SPD requirements selects simpler and faster-to-reconstruct events, it does not result in a significant loss of absolute signal efficiency compared to a strategy using only E T and p T requirements. This is because the L signal-background discrimination deteriorates rapidly with increasing event complexity for all but the dimuon and electroweak trigger lines. Note that the 2017 thresholds are looser than the 2016 thresholds because the maximum number of colliding bunches, and hence, the collision rate of the LHC was significantly lower in 2017, due to difficulties with part of the injection chain. The optimization of the L criteria is described in more detail in section 6. Table 1. The L thresholds for the different trigger lines used to take the majority of the data for each indicated year. Technical trigger lines and those used for special areas of the physics programme are excluded for brevity. The Hadron, Photon, and Electron trigger lines select events based on the E T of reconstructed ECAL and HCAL clusters. The Muon, Muon High, and Dimon trigger lines select events based on the p T reconstruced MUON stubs, where the Dimuon selection is based on the product of the largest and second largest p T stubs found in the event. As some of the subdetectors also read out hits associated to other bunch crossings, the use of bandwidth is further optimised in most of the L0 lines by rejecting events with a large E T (> 24 GeV) for the previous bunch crossing [19]. ",
            "paragraph_rank": 14,
            "section_rank": 7,
            "ref_spans": [
                {
                    "type": "table",
                    "start": 1348,
                    "text": "Table 1",
                    "end": 1355
                },
                {
                    "type": "bibr",
                    "ref_id": "b18",
                    "start": 2142,
                    "text": "[19]",
                    "end": 2146
                }
            ]
        },
        {
            "text": "High level trigger",
            "section_rank": 8
        },
        {
            "section": "High level trigger",
            "text": "Events selected by L are transferred to the Event Filter Farm (EFF) for further selection. The EFF consists of approximately 1700 nodes, 800 of which were added for Run 2, with 27000 physical cores. The EFF can accommodate \u2248 50000 single-threaded processes using hyper-threading technology. The HLT is written in the same framework as the software used in the offline reconstruction of events for physics analyses. This allows for offline software to be easily incorporated into the trigger. As detailed later, the increased EFF capacity and improvements in the software allowed the offline reconstruction to be performed in the HLT in Run 2.",
            "paragraph_rank": 15,
            "section_rank": 8
        },
        {
            "section": "High level trigger",
            "text": "The total disk buffer of the EFF is 10 PB, distributed such that farm nodes with faster processors get a larger portion of the disk buffer. At an average event size of 55 kB passing HLT1, this buffer allows for up to two weeks of consecutive HLT1 data taking before HLT2 has to be executed. Therefore, it is large enough to accommodate both regular running (where, as we will see, the alignment and calibration is completed in a matter of minutes) and to serve as a safety mechanism to delay HLT2 processing in case of problems with the detector or calibration.",
            "paragraph_rank": 16,
            "section_rank": 8
        },
        {
            "section": "High level trigger",
            "text": "Around 40% of the trigger output rate is dedicated to inclusive topological trigger lines, another 40% is dedicated to exclusive c-hadron trigger lines, with the rest divided among dimuon lines, trigger lines for electroweak physics, searches for exotic new particles, and other exclusive trigger lines for specific analyses. There are in total around 20 HLT1 and 500 HLT2 trigger lines.",
            "paragraph_rank": 17,
            "section_rank": 8
        },
        {
            "text": "Real-time alignment and calibration",
            "section_rank": 9
        },
        {
            "section": "Real-time alignment and calibration",
            "text": "The computing power available in the Run 2 EFF allows for automated alignment and calibration tasks, providing offline quality information to the trigger reconstruction and selections, as described in ref. [20,21]. A more detailed description of this real-time alignment and calibration procedure will be the topic of a separate publication.",
            "paragraph_rank": 18,
            "section_rank": 9,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b19",
                    "start": 206,
                    "text": "[20,",
                    "end": 210
                },
                {
                    "type": "bibr",
                    "ref_id": "b20",
                    "start": 210,
                    "text": "21]",
                    "end": 213
                }
            ]
        },
        {
            "section": "Real-time alignment and calibration",
            "text": "-5 - Dedicated samples selected by HLT1 are used to align and calibrate the detector in real time. The alignment and calibrations are performed at regular intervals, and the resulting alignment and calibration constants are updated only if they differ significantly from the current values.",
            "paragraph_rank": 19,
            "section_rank": 9
        },
        {
            "section": "Real-time alignment and calibration",
            "text": "The major detector alignment and calibration tasks consist of:",
            "paragraph_rank": 20,
            "section_rank": 9
        },
        {
            "section": "Real-time alignment and calibration",
            "text": "\u2022 the VELO alignment, followed by the alignment of the tracking stations;",
            "paragraph_rank": 21,
            "section_rank": 9
        },
        {
            "section": "Real-time alignment and calibration",
            "text": "\u2022 the MUON alignment;",
            "paragraph_rank": 22,
            "section_rank": 9
        },
        {
            "section": "Real-time alignment and calibration",
            "text": "\u2022 alignment of the rotations around various local axes in both RICH detectors of the primary and secondary mirrors;",
            "paragraph_rank": 23,
            "section_rank": 9
        },
        {
            "section": "Real-time alignment and calibration",
            "text": "\u2022 global time calibration of the OT;",
            "paragraph_rank": 24,
            "section_rank": 9
        },
        {
            "section": "Real-time alignment and calibration",
            "text": "\u2022 RICH gas refractive-index calibration;",
            "paragraph_rank": 25,
            "section_rank": 9
        },
        {
            "section": "Real-time alignment and calibration",
            "text": "\u2022 RICH Hybrid Photon Detectors calibration;",
            "paragraph_rank": 26,
            "section_rank": 9
        },
        {
            "section": "Real-time alignment and calibration",
            "text": "\u2022 ECAL LED (relative) and \u03c0 0 (absolute) calibrations.",
            "paragraph_rank": 27,
            "section_rank": 9
        },
        {
            "section": "Real-time alignment and calibration",
            "text": "Each of these tasks has a dedicated HLT1 trigger line which supplies it with the types of events required. When the required sample sizes have been collected, the selected events are saved to the disk buffer of the EFF, and calibration and alignment tasks are performed in parallel within the EFF. A schematic view of the alignment and calibration procedure is shown in figure 3, together with the time when the tasks are launched and the typical time taken to complete them.",
            "paragraph_rank": 28,
            "section_rank": 9,
            "ref_spans": [
                {
                    "type": "figure",
                    "ref_id": "fig_3",
                    "start": 370,
                    "text": "figure 3",
                    "end": 378
                }
            ]
        },
        {
            "section": "Real-time alignment and calibration",
            "text": "-6 -2019 JINST 14 P04013",
            "paragraph_rank": 29,
            "section_rank": 9
        },
        {
            "text": "HLT1 partial event reconstruction",
            "section_rank": 10
        },
        {
            "section": "HLT1 partial event reconstruction",
            "text": "HLT1 reconstructs the trajectories of charged particles traversing the full LHCb tracking system, called long tracks, with a p T larger than 500 MeV/c. In addition, a precise reconstruction of the PV is performed. The details of both steps are presented in section 4.1. Tight timing constraints in HLT1 mean that most particle-identification algorithms cannot be executed. The exception is muon identification, which due to a clean signature produced by muons in the LHCb detector can be performed already in HLT1, as described in section 4.2. As discussed in section 6.7.3, a subset of specially selected HLT1 events serves as input to the alignment and calibrations tasks.",
            "paragraph_rank": 30,
            "section_rank": 10
        },
        {
            "text": "Track and vertex reconstruction in HLT1",
            "section_rank": 11
        },
        {
            "section": "Track and vertex reconstruction in HLT1",
            "text": "The sequence of HLT1 algorithms which reconstruct vertices and long tracks is shown in figure 4. The pattern recognition deployed in HLT1 consists of three main steps: reconstructing the VELO tracks, extrapolating them to the TT stations to form upstream tracks, and finally extending them further to the T stations to produce long tracks. Next, the long tracks are fitted using a Kalman Filtering and the fake trajectories are rejected. The set of fitted VELO tracks is re-used to determine the positions of the PVs.",
            "paragraph_rank": 31,
            "section_rank": 11,
            "ref_spans": [
                {
                    "type": "figure",
                    "ref_id": "fig_4",
                    "start": 87,
                    "text": "figure 4",
                    "end": 95
                }
            ]
        },
        {
            "text": "Pattern recognition of high-momentum tracks",
            "section_rank": 12
        },
        {
            "section": "Pattern recognition of high-momentum tracks",
            "text": "The hits in the VELO are combined to form straight lines loosely pointing towards the beam line [22]. Next, at least three hits in the TT are required in a small region around a straight-line extrapolation from the VELO [23] to form so-called upstream tracks. The TT is located in the fringe field of the LHCb dipole magnet, which allows the momentum to be determined with a relative resolution of about 20%. This momentum estimate is used to reject low p T tracks. Matching long tracks with TT hits additionally reduces the number of fake VELO tracks. Due to the limited acceptance of the TT, VELO tracks pointing to the region around the beampipe do not deposit charge in the TT; therefore, they are passed on without requiring TT hits.",
            "paragraph_rank": 32,
            "section_rank": 12,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b21",
                    "start": 96,
                    "text": "[22]",
                    "end": 100
                },
                {
                    "type": "bibr",
                    "ref_id": "b22",
                    "start": 220,
                    "text": "[23]",
                    "end": 224
                }
            ]
        },
        {
            "section": "Pattern recognition of high-momentum tracks",
            "text": "The search window in the IT and OT is defined by the maximum possible deflection of charged particles with p T larger than 500 MeV/c. The search is also restricted to one side of the straight-line extrapolation by the charge estimate of the upstream track. The use of the charge estimate is new in Run 2 and enabled the p T threshold to be lowered from 1.2 GeV/c to 500 MeV/c with respect to Run 1. For a given slope and position upstream of the magnet and a single hit in the tracking detectors downstream of the magnet, IT and OT, the momentum is fixed and hits are projected along this trajectory into a common plane. A search is made for clusters of hits in this plane which are then used to define the final long track [24]. In 2016 two artificial neural nets were implemented to increase the purity and the efficiency of the track candidates in the pattern recognition [25].",
            "paragraph_rank": 33,
            "section_rank": 12,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b23",
                    "start": 724,
                    "text": "[24]",
                    "end": 728
                },
                {
                    "type": "bibr",
                    "ref_id": "b24",
                    "start": 875,
                    "text": "[25]",
                    "end": 879
                }
            ]
        },
        {
            "text": "Track fitting and fake-track rejection",
            "section_rank": 13
        },
        {
            "section": "Track fitting and fake-track rejection",
            "text": "Subsequently, all tracks are fitted with a Kalman filter to obtain the optimal parameter estimate. The settings of the online and offline reconstruction are harmonised in Run 2 to obtain identical results for a given track. Previously, the online Kalman filter performend only a single iteration which did not allow the ambiguities from the drift-time measurement of OT hits on a track to be resolved. In -  Run 2 the online fit runs until convergence or maximally 10 iterations. Furthermore the possibility to remove up to two outliers has been added in the online reconstruction. The offline reconstruction is changed to use clusters which are faster to decode but have less information, and to employ a Kalman filter that utilizes a simplified geometry description of the LHCb detector. This significantly speeds up the calculation of material corrections in the filtering procedure. For Run 2 the calculation of material corrections due to multiple scattering has been improved. The new description is additive for many small scatterers resulting in more standard normal pull distributions. The changes made to the offline Kalman filter enable running the same algorithm in both the HLT and offline. These changes neither affect the impact parameter resolution nor the momentum resolution as shown in section 5.1.2.",
            "paragraph_rank": 34,
            "section_rank": 13
        },
        {
            "section": "Track fitting and fake-track rejection",
            "text": "Since 2016 the fake track rejection, described in details in section 5.1, has been used in HLT1 reducing the rate of events passing this stage by 4%.",
            "paragraph_rank": 35,
            "section_rank": 13
        },
        {
            "text": "Primary vertex reconstruction",
            "section_rank": 14
        },
        {
            "section": "Primary vertex reconstruction",
            "text": "Many LHCb analyses require a precise knowledge of the PV position and this information is used early in the selection of displaced particles. The full set of VELO tracks is available in HLT1. Therefore, the PVs in Run 2 are reconstructed using VELO tracks only, neglecting the additional momentum information on long tracks which is only available later. This does not result in a degradation in resolution compared to using a mixture of VELO and long tracks. Furthermore, this approach produces a consistent PV position from the beginning to the end of the analysis chain which reduces systematic effects. As there is no magnetic field in the VELO, the Kalman filter for VELO tracks uses a linear propagation, allowing for a single scattering at each detector plane, tuned using simulation. This simplification results in no loss of precision compared to a more detailed material description, but significantly reduces the amount of time spent in the filtering phase, as no expensive computations are necessary. A byproduct of this simpler track fit is that the PV covariance matrix is more accurate than that used offline in Run 1, with pull distributions more compatible with unit widths in all three dimensions.",
            "paragraph_rank": 36,
            "section_rank": 14
        },
        {
            "section": "Primary vertex reconstruction",
            "text": "The PV resolution is obtained by randomly splitting the input VELO tracks into two subsets. The PV algorithm is executed independently on each subset, and the PVs found in each subset are matched based on the distance between them. The width of the distribution of the difference of matched PV positions in a given dimension, corrected by a factor of \u221a 2, gives the corresponding PV position resolution. The resolution of the PV reconstruction for Run 2 is shown in figure 5 compared to the Run 1 (2012) offline reconstruction algorithm. The new algorithm performs equally well for the x (y) coordinate, while with respect to Run 1 the resolution on the z coordinate is improved by about 10%.",
            "paragraph_rank": 37,
            "section_rank": 14
        },
        {
            "section": "Primary vertex reconstruction",
            "text": "Additionally, the parameters of the PV reconstruction have been retuned to give a higher efficiency and smaller fake rate [26]. The resulting improvement in efficiency of reconstructing PVs is 0.5% for PVs associated with the production of a b quark pair, 1.3% for those associated with the production of a c quark pair, and 6.6% for light quarks production. Simultaneously, the fraction of fake PVs, for example due to material interactions or the decay vertices of long-lived particles, is reduced from 3.5% to 1%.",
            "paragraph_rank": 38,
            "section_rank": 14,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b25",
                    "start": 122,
                    "text": "[26]",
                    "end": 126
                }
            ]
        },
        {
            "text": "Muon identification",
            "section_rank": 15
        },
        {
            "section": "Muon identification",
            "text": "The muon identification starts with fully fitted tracks. Hits in the MUON stations are searched for in momentum-dependent regions of interest around the track extrapolation. Tracks with p < 3 GeV/c cannot be identified as muons, as they would not be able to reach the MUON stations. Below a momentum of 6 GeV/c the muon identification algorithm requires hits in the first two stations after the calorimeters. Between 6 and 10 GeV/c an additional hit is required in one of the last two stations. Above 10 GeV, hits are required in all the four MUON stations. This same algorithm is used in HLT1, HLT2 and offline.",
            "paragraph_rank": 39,
            "section_rank": 15
        },
        {
            "section": "Muon identification",
            "text": "-9 - In HLT1, the track reconstruction is only performed for tracks with p T above 500 MeV/c. For particles with lower p T , a complementary muon-identification algorithm has been devised, which is more similar to the HLT1 muon identification performed in Run 1. Upstream track segments are extrapolated directly to the MUON stations, where hits are searched for around the track extrapolation. The regions of interest used in this search are larger than those used for otherwise-reconstructed long tracks. If hits are found in the muon system, the VELO-TT segment is extrapolated through the magnetic field using the momentum estimate and matched to hits not already used in the HLT1 long-track reconstruction. This procedure extends the muon-identification capabilities down to a p T of 80 MeV/c for a small additional resource cost, significantly improving the performance for lower-momentum muons which are important in several measurements [27].",
            "paragraph_rank": 40,
            "section_rank": 15,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b26",
                    "start": 945,
                    "text": "[27]",
                    "end": 949
                }
            ]
        },
        {
            "section": "Muon identification",
            "text": "The muon identification code has been reoptimized for Run 2, gaining significant efficiency, in particular at small p T . This is demonstrated using LHCb simulation in figure 6. The performance of the muon identification in HLT1 is shown in figure 7 (left) as determined from unbiased J/\u03c8 \u2192 \u00b5 + \u00b5 \u2212 decays using the tag-and-probe method. This performance is obtained by studying the efficiency of the single-muon HLT1 trigger, which includes requirements on the displacement of the muon and on the minimum momentum (6 GeV/c) and p T (1.1 GeV/c). Analogously in figure 7 (right) the misidentification efficiency with the same criteria is shown for pions from D 0 \u2192 K \u2212 \u03c0 + decays. The muon identification efficiency of the single-muon HLT1 trigger is slightly reduced by the displacement and (transverse) momentum requirements, at the benefit of a lower misidentification probability.",
            "paragraph_rank": 41,
            "section_rank": 15
        },
        {
            "text": "HLT2 full event reconstruction",
            "section_rank": 16
        },
        {
            "section": "HLT2 full event reconstruction",
            "text": "The HLT2 full event reconstruction consists of three major steps: the track reconstruction of charged particles, the reconstruction of neutral particles, and particle identification (PID). The HLT2 track reconstruction exploits the full information from the tracking sub-detectors, performing additional steps of the pattern recognition which are not possible in HLT1 due to strict time constraints. the HLT2 reconstruction. Finally, in addition to the muon identification available in HLT1, HLT2 exploits the full particle identification from the RICH detectors and calorimeter system. The code of all reconstruction algorithms has been optimized for Run 2 to better exploit the capabilities of modern CPUs. Together with the algorithmic changes described in the following sections, this results in a two times faster execution time while delivering the same or in several cases better physics performance than that achieved offline in Run 1.",
            "paragraph_rank": 42,
            "section_rank": 16
        },
        {
            "text": "The track reconstruction of charged particles",
            "section_rank": 17
        },
        {
            "section": "The track reconstruction of charged particles",
            "text": "A sketch of the track and vertex reconstruction sequence in HLT2 is shown in figure 8. The goal is to reconstruct all tracks without a minimal p T requirement. This is particularly important for the study of the decays of lighter particles, such as charmed or strange hadrons, whose abundance means that only some of the fully reconstructed and exclusively selected final states fit into the available trigger bandwidth. Often, not all of the decay products of a charm-or strange-hadron decay pass the 500 MeV/c p T requirement of HLT1, particularly for decays into three or more final-state particles. Therefore, to efficiently trigger these decays, it is necessary to also reconstruct the lower-momentum tracks within the LHCb acceptance.",
            "paragraph_rank": 43,
            "section_rank": 17,
            "ref_spans": [
                {
                    "type": "figure",
                    "ref_id": "fig_8",
                    "start": 77,
                    "text": "figure 8",
                    "end": 85
                }
            ]
        },
        {
            "section": "The track reconstruction of charged particles",
            "text": "In a first step, the track reconstruction of HLT1 is repeated. A second step is then used to reconstruct the lower-momentum tracks which had not been found in HLT1 due to the kinematic thresholds in the reconstruction. Those VELO tracks and T-station clusters used to reconstruct long tracks with a good fit quality in the first step are disregarded for this second step. A similar procedure as in HLT1 is employed: the remaining VELO tracks are extrapolated through the magnet to the T-stations using the same algorithm, where the search window is now defined by the maximal possible deflection of a particle with p T larger than 80 MeV/c. No TT hits are required for the second step to avoid the loss of track efficiency induced by acceptance gaps in the TT. The new Run 2 track finding optimization results in 27% fewer fake tracks and a reconstruction efficiency gain of 0.5% for long tracks. In addition, a standalone search for tracks in the T stations is performed [28], and these standalone tracks are then combined with VELO tracks to form long tracks [29,30]. The redundancy of the two long-track algorithms increases the efficiency by a few percent. Tracks produced in the decays of long-lived particles like \u039b or K 0 S that decay outside the VELO are reconstructed using T-station segments that are extrapolated backwards through the magnetic field and combined with hits in the TT. For Run 2, a new algorithm was used to reconstruct these tracks [31]. It uses two multivariate classifiers, one to reject fakes, and another to select the final set of hits in the TT in case several sets are compatible with the same T-station segment. In combination with other improvements, this results in a higher efficiency and a lower fake rate compared to the corresponding Run 1 algorithm. The next step in the reconstruction chain is the rejection of fake tracks. These fakes result from random combinations of hits or a mismatch of track segments upstream and downstream of the magnet. They are reduced using two techniques. First, all tracks are fitted using the same Kalman filter. In Run 1, the only selection to reject fake tracks was based on a reduced \u03c7 2 . For Run 2, the upper limit on this \u03c7 2 selection was increased to allow for a better overall track reconstruction efficiency. To offset the corresponding increase in the number of fake tracks, a neural network was trained using the TMVA [32,33] package to efficiently remove these tracks [34]. Its input variables are the overall \u03c7 2 of the Kalman filter, the \u03c7 2 values of the fits for the different track segments, the numbers of hits in the different tracking detectors, and the p T of the track. the Run 2 trigger without any significant impact on the execution time. The evaluation uses only about 0.2% of the total CPU budget. Furthermore, the better performance of this fake track rejection in both stages of the HLT leads to 16% less CPU consumption in the entire software trigger. The neural network was trained on simulated tracks. The working point was chosen such that it rejects 60% of all fake tracks, while maintaining an efficiency of about 99%.",
            "paragraph_rank": 44,
            "section_rank": 17,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b27",
                    "start": 972,
                    "text": "[28]",
                    "end": 976
                },
                {
                    "type": "bibr",
                    "ref_id": "b28",
                    "start": 1061,
                    "text": "[29,",
                    "end": 1065
                },
                {
                    "type": "bibr",
                    "ref_id": "b29",
                    "start": 1065,
                    "text": "30]",
                    "end": 1068
                },
                {
                    "type": "bibr",
                    "ref_id": "b30",
                    "start": 1459,
                    "text": "[31]",
                    "end": 1463
                },
                {
                    "type": "bibr",
                    "ref_id": "b31",
                    "start": 2405,
                    "text": "[32,",
                    "end": 2409
                },
                {
                    "type": "bibr",
                    "ref_id": "b32",
                    "start": 2409,
                    "text": "33]",
                    "end": 2412
                },
                {
                    "type": "bibr",
                    "ref_id": "b33",
                    "start": 2456,
                    "text": "[34]",
                    "end": 2460
                }
            ]
        },
        {
            "section": "The track reconstruction of charged particles",
            "text": "The performance of the fake track removal was validated on first collision data in 2015 to ensure a uniform response over a large area of the phase space. As an example, the performance for D 0 \u2192 K \u2212 \u03c0 + and K 0 S \u2192 \u03c0 \u2212 \u03c0 decays is shown in figure 9. After the removal of fake tracks, the remaining tracks are filtered to remove so-called clones. Clones can be created inside a single pattern-recognition algorithm or, more commonly, originate from the redundancy in the pattern-recognition algorithms. Two tracks are defined as clones of each other if they share enough hits in each subdetector. Only the subdetectors where both tracks have hits are considered. The track with more hits in total is kept and the other is discarded. This final list of tracks is subsequently used to select events as discussed in section 6.",
            "paragraph_rank": 45,
            "section_rank": 17
        },
        {
            "text": "Tracking efficiency",
            "section_rank": 18
        },
        {
            "section": "Tracking efficiency",
            "text": "The track reconstruction efficiency is determined using a tag-and-probe method on J/\u03c8 \u2192 \u00b5 + \u00b5 \u2212 decays that originate from the decays of b-hadrons [35]. One muon is reconstructed using the full reconstruction, while the other muon is reconstructed using only specific subdetectors, making it possible to probe the others. For Run 2, the track reconstruction efficiency is determined in HLT2 using the data collected by specific trigger lines, see section 6.8.3. The performance compared to Run 1 is shown in figure 10. Given that the OT has a readout window which is larger than 25 ns and therefore is prone to spillover effects when reducing the bunch spacing from 50 ns to 25 ns, a small reduction in the track reconstruction efficiency is observed in 2015.",
            "paragraph_rank": 46,
            "section_rank": 18,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b34",
                    "start": 147,
                    "text": "[35]",
                    "end": 151
                },
                {
                    "type": "figure",
                    "ref_id": "fig_1",
                    "start": 508,
                    "text": "figure 10",
                    "end": 517
                }
            ]
        },
        {
            "text": "Invariant mass resolution",
            "section_rank": 19
        },
        {
            "section": "Invariant mass resolution",
            "text": "The invariant mass resolution is determined on a sample of J/\u03c8 \u2192 \u00b5 + \u00b5 \u2212 decays, where the J/\u03c8 originates from a b-hadron decay. The dimuon invariant mass distribution is modelled using a double Crystal Ball function [36], where the weighted mean of the standard deviations of the two Gaussian components is used to estimate the resolution. The distributions for subsamples of the  2012 and 2016 data can be seen in figure 11, the resolutions are 12.4 MeV/c 2 for the 2012 data sample and 12.7 MeV/c 2 for the 2016 data sample. The difference comes from a slightly highermomentum spectrum in 2016, due to the larger beam energy in Run 2, and a small degradation in the performance due to the use of a simplified description of the detector geometry throughout Run 2.",
            "paragraph_rank": 47,
            "section_rank": 19,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b35",
                    "start": 217,
                    "text": "[36]",
                    "end": 221
                }
            ]
        },
        {
            "text": "Impact parameter and decay-time resolutions",
            "section_rank": 20
        },
        {
            "section": "Impact parameter and decay-time resolutions",
            "text": "The impact parameter and decay-time resolutions are extracted with data-driven methods which are described in more detail elsewhere [6]. The impact parameter is defined as the distance between a particle trajectory and a given PV. It is one of the main discriminants between particles produced directly in the primary interaction and particles originating from the decays of long-lived hadrons. The impact parameter resolution as a function of 1/p T is shown in figure 12. Only events with one reconstructed PV are used, and the PV fit is rerun excluding each track in turn. The resulting PV is required to have at least 25 tracks to minimise the contribution from the PV resolution. Multiple scattering induces a linear dependence on 1/p T . For high p T particles, the impact parameter resolution is roughly 12 \u00b5m in both the x and y directions. The observed improvement of about 1 \u00b5m in 2017 data taking is due to the use of an updated VELO error parametrisation.",
            "paragraph_rank": 48,
            "section_rank": 20,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b5",
                    "start": 132,
                    "text": "[6]",
                    "end": 135
                },
                {
                    "type": "figure",
                    "ref_id": "fig_1",
                    "start": 462,
                    "text": "figure 12",
                    "end": 471
                }
            ]
        },
        {
            "section": "Impact parameter and decay-time resolutions",
            "text": "The decay time of a particle is determined from the distance between the PV and the secondary decay vertex. An excellent decay-time resolution is a key ingredient of time-dependent mixing -14 - and CP violation measurements. The resolution is determined from J/\u03c8 decays combined with two random tracks which mimic B 0 s \u2192 J/\u03c8 \u03c6 decays. In the absence of any impact parameter requirements these combinations come mainly from prompt particles and, therefore, the expected decay time is zero. The width of the distribution is thus a measure of the decay-time resolution. A comparison of the decay-time resolution as a function of momentum for Run 1, 2015, and 2016 data taking is shown in figure 13. For Run 2 the average resolution is about 45 fs for a 4-track vertex.",
            "paragraph_rank": 49,
            "section_rank": 20,
            "ref_spans": [
                {
                    "type": "figure",
                    "ref_id": "fig_1",
                    "start": 686,
                    "text": "figure 13",
                    "end": 695
                }
            ]
        },
        {
            "text": "Muon reconstruction",
            "section_rank": 21
        },
        {
            "section": "Muon reconstruction",
            "text": "As mentioned in section 4.2, the same muon identification algorithm is used in HLT2 and HLT1, apart from the fact that the HLT2 algorithm takes as its input the full set of fitted tracks available after the HLT2 reconstruction.",
            "paragraph_rank": 50,
            "section_rank": 21
        },
        {
            "text": "RICH reconstruction",
            "section_rank": 22
        },
        {
            "section": "RICH reconstruction",
            "text": "The identification of different particle species is crucial across LHCb's physics programme. The RICH detectors provide the main discrimination between deuterons, kaons, pions, and protons.  Figure 14. Efficiency and fake rate of the RICH identification for the 2012 (left) and the 2016 (right) data.",
            "paragraph_rank": 51,
            "section_rank": 22,
            "ref_spans": [
                {
                    "type": "figure",
                    "ref_id": "fig_1",
                    "start": 191,
                    "text": "Figure 14",
                    "end": 200
                }
            ]
        },
        {
            "text": "Efficiency",
            "section_rank": 23
        },
        {
            "section": "Efficiency",
            "text": "Cherenkov light is emitted in a cone around the flight direction of a charged particle, where the cone width depends on the velocity of the particle. The photon yields, expected Cherenkov angles, and estimates of the per-track Cherenkov angle resolution are computed under each of the deuteron, proton, kaon, pion, muon and electron mass hypotheses. The RICH reconstruction considers simultaneously all reconstructed tracks and all Cherenkov photons in RICH1 and RICH2 in each event. The reconstruction algorithm provides a likelihood for each mass hypothesis. As the RICH reconstruction consumes significant computing power, it could not be run for every track in the Run 1 real-time reconstruction. Improvements in the HLT and in the RICH reconstruction itself made it possible, however, to run the full algorithm in the Run 2 HLT2. The performance of the RICH particle identification is shown in figure 14 for the 2012 and 2016 data. A small improvement is obtained in Run 2 for particles below 15 GeV/c of momentum.",
            "paragraph_rank": 52,
            "section_rank": 23
        },
        {
            "text": "Calorimeter reconstruction",
            "section_rank": 24
        },
        {
            "section": "Calorimeter reconstruction",
            "text": "The reconstruction of electromagnetic particles (photons, electrons, and \u03c0 0 mesons) is performed by the calorimeters. A cellular automaton algorithm is used to build clusters from the energy deposits in the different calorimeter subsystems, which are combined to determine the total energy of each particle [37]. Neutral particles are then identified according to their isolation with respect to the reconstructed tracks. Electron identification is also provided by combining information from the isolation of clusters in the ECAL, the presence of clusters in the PS, the energy deposited in the HCAL, and the position of possible Bremsstrahlung photons.",
            "paragraph_rank": 53,
            "section_rank": 24,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b36",
                    "start": 308,
                    "text": "[37]",
                    "end": 312
                }
            ]
        },
        {
            "section": "Calorimeter reconstruction",
            "text": "High-E T \u03c0 0 mesons and photons are indistinguishable at the trigger level, as they both appear as a single cluster, while low-E T \u03c0 0 mesons are built by combining resolved pairs of well-separated photons. The neutral-cluster reconstruction algorithm run in HLT2 is the same as that run offline.",
            "paragraph_rank": 54,
            "section_rank": 24
        },
        {
            "section": "Calorimeter reconstruction",
            "text": "The identification of these clusters as either neutral objects or electrons uses information from both the PS/SPD detectors, and a matching between reconstructed tracks and calorimeter clusters. Early in Run 2 this online identification was not identical to the offline version because the HLT did not reconstruct T-tracks (see figure 2), since these are not directly used by physics analyses. They are, however, relevant for neutral-particle identification. This misalignment was gradually reduced as Run 2 progressed, first by adding the reconstruction of T-tracks and then by subsequently applying a Kalman filter to them to align the algorithm to the offline reconstruction sequence. A fully automated ECAL calibration was introduced in 2018. The automatic LED calibration is performed for fills longer than 3.5 hours as indicated in figure 3, while the absolute \u03c0 0 calibration is processed once per month when sufficient data (amounting to 300M events) is collected. The performance of the calorimeter reconstruction is shown in figure 15 using B 0 \u2192 (K + \u03c0 \u2212 )\u03b3 decays. The invariant mass resolution has been improved with respect to Run 1 from about 91 MeV/c 2 to 87 MeV/c 2 .",
            "paragraph_rank": 55,
            "section_rank": 24,
            "ref_spans": [
                {
                    "type": "figure",
                    "ref_id": "fig_2",
                    "start": 328,
                    "text": "figure 2)",
                    "end": 337
                },
                {
                    "type": "figure",
                    "ref_id": "fig_1",
                    "start": 1035,
                    "text": "figure 15",
                    "end": 1044
                }
            ]
        },
        {
            "text": "Trigger performance",
            "section_rank": 25
        },
        {
            "section": "Trigger performance",
            "text": "The LHCb trigger performance is optimized around two key metrics: the L kinematic and occupancy thresholds for each of the main trigger lines (muon, dimuon, electron, photon, and hadron); and the optimization of the HLT timing budget, which defines the maximum allowed HLT1 output rate. An automated procedure is used to divide the L bandwidth among a set of representative signal channels. It has been significantly improved with respect to Run 1 and is described here. The procedure for determining the HLT processing budget is also described, and the L and HLT performance is evaluated using a tag-and-probe approach on Run 2 data.",
            "paragraph_rank": 56,
            "section_rank": 25
        },
        {
            "text": "L bandwidth division",
            "section_rank": 26
        },
        {
            "section": "L bandwidth division",
            "text": "The relative simplicity of the information available for the L trigger decision means that the trigger lines listed in table 1 cover the majority of the LHCb physics programme. Out of these, the high-p T muon trigger consumes a relatively negligible rate and is insensitive to the running conditions. The remaining five trigger lines: hadron, muon, electron, photon and dimuon, must have their p T and E T thresholds tuned to maximize signal efficiency under different LHC conditions. In particular, during the luminosity ramp-up of the LHC, signal efficiencies can be improved by reducing the thresholds to maintain an L output rate close to the maximal readout rate of 1 MHz.",
            "paragraph_rank": 57,
            "section_rank": 26
        },
        {
            "section": "L bandwidth division",
            "text": "Once the LHC reaches its nominal number of colliding bunches in a given year, determining the optimal division of rate between the L channels is important for achieving the physics goals of the experiment. This so-called \"bandwidth division\" is performed using a genetic algorithm to minimise -17 -",
            "paragraph_rank": 58,
            "section_rank": 26
        },
        {
            "text": "JINST 14 P04013",
            "section_rank": 27
        },
        {
            "section": "JINST 14 P04013",
            "text": "the following pseudo-\u03c7 2 for a broad range of simulated signal samples that are representative of the LHCb physics programme:",
            "paragraph_rank": 59,
            "section_rank": 27
        },
        {
            "section": "JINST 14 P04013",
            "text": "The sum is over the N signal samples, \u03b5(r) i is the efficiency including detector dead time of the i th data set, and \u03b5 max is the efficiency including detector dead time when all of the bandwidth is allocated to this data set. The ratio of dead-time-corrected efficiencies is designed to ensure that inefficient signal samples contribute more to the \u03c7 2 , i.e. the algorithm prioritizes improving the efficiency of signal samples which start with a low absolute efficiency over making identical absolute efficiency improvements for signals with high efficiencies to begin with. The weight assigned to each data set, w i , is predetermined by the LHCb collaboration and is designed to grant more bandwidth to higher-priority physics channels. The dead-time correction to the signal efficiency acts as a rate limiter and is dependent upon the filling scheme:",
            "paragraph_rank": 60,
            "section_rank": 27
        },
        {
            "section": "JINST 14 P04013",
            "text": "Here is the overall L signal efficiency and r is the retention of collisions collected using random trigger lines (henceforth \"nobias\"). The physics dead time \u03b4 phys (r) is zero below r limit = 1.1 MHz, which is the maximum HLT1 throughput, and r/r limit above this. The technical dead time, \u03b4 tech (r) is determined from a model trained on a filling-scheme dependent emulation of the detector readout dead time.",
            "paragraph_rank": 61,
            "section_rank": 27
        },
        {
            "section": "JINST 14 P04013",
            "text": "The results of the L bandwidth optimization are shown in figure 16 for the 2016 and 2017 data-taking conditions. The different optimal points are mostly connected to the different LHC running conditions in the two years, in particular to problems in 2017 which limited the maximum possible number of bunches in the LHC and hence led to lower trigger thresholds.",
            "paragraph_rank": 62,
            "section_rank": 27,
            "ref_spans": [
                {
                    "type": "figure",
                    "ref_id": "fig_1",
                    "start": 57,
                    "text": "figure 16",
                    "end": 66
                }
            ]
        },
        {
            "text": "Measuring the HLT processing speed",
            "section_rank": 28
        },
        {
            "section": "Measuring the HLT processing speed",
            "text": "Trigger configurations are tested for processing speed and memory usage on 13 TeV nobias collected at the same average number of visible interactions per bunch crossing as in regular data taking. As the L trigger conditions affect the complexity of events processed by the HLT, these tests are repeated for each L configuration. Nobias events passing the L configuration are processed by a dedicated \"average\" EFF node loaded with the same number of total tasks as in the online data-taking configuration. The timing is measured separately for HLT1 running on events passing L , and HLT2 running on events passing both L and HLT1. Each HLT1 and HLT2 task processes an independent sample of around 10,000 events during this test, with the number of events chosen to balance robustness and turnaround speed. The processing speed of each of the individual HLT1 and HLT2 tasks is then averaged in order to remove fluctuations due to the limited number of test events, and these values are compared to the available budget. In addition, the memory usage is plotted as a function of the event number to verify that there are no memory leaks.",
            "paragraph_rank": 63,
            "section_rank": 28
        },
        {
            "section": "Measuring the HLT processing speed",
            "text": "These tests give confidence that the HLT is running within its budget, and they are particularly important for spotting problems after any major changes are made to a stable configuration. They do not, however, give a perfect reflection of the performance expected on the full farm. In particular, -18 -the effect of calibration and alignment tasks which run in parallel with the HLT1 and HLT2 tasks is neglected, as are the I/O issues associated with sending events from L to the HLT1 tasks, the buffering of HLT1 events and the action of reading them back into HLT2 tasks, and the overhead from sending the events accepted by HLT2 to the offline storage. For this reason it makes little sense to quote detailed performance numbers for the HLT from these tests.",
            "paragraph_rank": 64,
            "section_rank": 28
        },
        {
            "text": "Optimization of the HLT timing and disk buffers",
            "section_rank": 29
        },
        {
            "section": "Optimization of the HLT timing and disk buffers",
            "text": "The timing budget of the HLT is defined as the average time available for each HLT task to process an event, when the processing farm is fully loaded.1 With a traditional single-stage HLT, as was the case in Run 1, the timing budget is easily determined because the events must always be processed as they arrive during the collider runtime. Therefore, it is simply the number of HLT tasks divided by the input event rate, which amounted to about 50 ms for a farm with around 50000 logical cores as was available in 2015. The calculation is more complicated for the two-stage HLT used in Run 2. Since the second stage is deferred and can occur during LHC downtime there are two timing budgets, one for the HLT1 stage and one for the HLT2 stage. These budgets depend on the assumptions made about the length and distribution of the LHC runtime and downtime periods.",
            "paragraph_rank": 65,
            "section_rank": 29
        },
        {
            "section": "Optimization of the HLT timing and disk buffers",
            "text": "The LHC downtime is not uniformly distributed throughout the year. Most occurs during a winter shutdown, lasting several months, and \"technical stops\" lasting approximately two weeks each and distributed throughout the year. The runtime is consequently also concentrated, with a peak structure of repeated 10-15 hour-long collision periods with inter-fill gaps of 2-3 hours between them. The timing budget is determined by simulating the rate at which the disk buffer fills up and empties, using the processing speed measurements and the most recent LHC fill structure as a guide.2 The objective is to ensure that the disk buffer will never exceed more than 80% capacity at any point throughout the remaining data taking period, and is evaluated every two weeks using the actual disk occupancy at the time as the starting point. The output rate of HLT1 is adjusted to keep the disk buffer usage within the desired limits. This output rate is controlled by switching between two HLT1 configurations, where the tighter configuration sacrifices some rate and efficiency for the inclusive general purpose trigger lines while protecting the trigger lines used for specific areas of the physics programme. The buffer usage is monitored throughout the year, and biweekly simulations are made using the present buffer capacity, HLT1 output rate and HLT2 throughput to determine the projected disk usage until the end of the year. Should a significant fraction of these simulations exceed the 80% usage threshold, the HLT1 configuration is tightened. An example simulation and the disk usage throughout 2017 are shown in figure 17.",
            "paragraph_rank": 66,
            "section_rank": 29
        },
        {
            "text": "Efficiency measurement method",
            "section_rank": 30
        },
        {
            "section": "Efficiency measurement method",
            "text": "All efficiencies are measured on background-subtracted data using the so-called TISTOS method described in the Run 1 performance paper [1] and briefly recapped here. Offline-selected signal events are divided into the following categories:",
            "paragraph_rank": 67,
            "section_rank": 30,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b0",
                    "start": 135,
                    "text": "[1]",
                    "end": 138
                }
            ]
        },
        {
            "section": "Efficiency measurement method",
            "text": "1The processing farm consists of processors with a certain number of physical cores, but a single task will not generally fully load a single physical core. For this reason the number of logical HLT tasks which are launched for each physical CPU core is optimized by measuring the overall throughput of events in the farm. For the 2015 HLT farm, this number is typically around 1.6, depending on the node in question.",
            "paragraph_rank": 68,
            "section_rank": 30
        },
        {
            "section": "Efficiency measurement method",
            "text": "2In 2015 this optimization used the 2012 fill information.",
            "paragraph_rank": 69,
            "section_rank": 30
        },
        {
            "section": "Efficiency measurement method",
            "text": "-19 - Figure 16. Efficiencies per signal mode for (top) 2016 and (bottom) 2017 data-taking periods measured in simulation. Red (left-slanted) hatched plots are when the entire L bandwidth is granted to this signal mode, whereas blue (right-slanted) hatched plots are following the bandwidth division. Signals which appear only in blue are used for performance validation and are not part of the optimization itself. Channels followed by \"(S)\" are selected in a kinematic and geometric volume which is particularly important for spectroscopy studies.",
            "paragraph_rank": 70,
            "section_rank": 30,
            "ref_spans": [
                {
                    "type": "figure",
                    "ref_id": "fig_1",
                    "start": 6,
                    "text": "Figure 16",
                    "end": 15
                }
            ]
        },
        {
            "section": "Efficiency measurement method",
            "text": "-20 - During data taking, simulations (red, left) are used every two weeks to determine the probability of exceeding the 80% usage threshold. In 2017, the loose HLT1 configuration was used for the entire year leading to a maximum buffer capacity of 48% (black, right). LHC Technical Stops and Machine Development (MD) periods are shown in dark and light grey, respectively. The schedule changed between when this simulation was run in week 32 and the end of the year. An MD period was removed and the duration of the data taking was reduced.",
            "paragraph_rank": 71,
            "section_rank": 30
        },
        {
            "text": "JINST 14 P04013",
            "section_rank": 31
        },
        {
            "section": "JINST 14 P04013",
            "text": "\u2022 TIS: events which are triggered independently of the presence of the signal decay. These are unbiased by the trigger selection except for correlations between the signal decay and the rest of the event. (For example when triggering on the \"other\" B in the event and subsequently looking at the momentum distribution of the \"signal\" B, the correlation in their momenta is caused by the fact that they both originate in the same fragmentation chain.)",
            "paragraph_rank": 72,
            "section_rank": 31
        },
        {
            "section": "JINST 14 P04013",
            "text": "\u2022 TOS: events which are triggered on the signal decay independently of the presence of the rest of the event.",
            "paragraph_rank": 73,
            "section_rank": 31
        },
        {
            "section": "JINST 14 P04013",
            "text": "All efficiencies quoted in this paper are TOS efficiencies, given by = N(TOS and TIS) N(TIS) , (6.3) where N(TIS) is the number of signal TIS events in the sample, while N(TOS and TIS) is the number of signal events which are both TOS and TIS. The number of signal events passing and failing the TOS criterion is measured using a histogram sideband subtraction, as described in ref. [38]. In order to reduce the correlations between TOS and TIS events, the efficiency is plotted as a function of the p T and, where appropriate, decay time of the signal particle.",
            "paragraph_rank": 74,
            "section_rank": 31,
            "ref_spans": [
                {
                    "type": "bibr",
                    "start": 95,
                    "text": "(6.3)",
                    "end": 100
                },
                {
                    "type": "bibr",
                    "ref_id": "b37",
                    "start": 383,
                    "text": "[38]",
                    "end": 387
                }
            ]
        },
        {
            "text": "Samples used for performance measurements",
            "section_rank": 32
        },
        {
            "section": "Samples used for performance measurements",
            "text": "The performance of the L and HLT trigger selections is evaluated using samples ",
            "paragraph_rank": 75,
            "section_rank": 32
        },
        {
            "text": "JINST 14 P04013",
            "section_rank": 33
        },
        {
            "section": "JINST 14 P04013",
            "text": "and therefore, they can be used to measure the L , HLT1, and HLT2 efficiencies. The exception is the B 0 \u2192 K * 0 \u03b3 decay, where requiring TIS at HLT1 or HLT2 results in a signal yield and purity which are too small to be usable. Therefore, this mode is only used to study the L photon and electron trigger performance.",
            "paragraph_rank": 76,
            "section_rank": 33
        },
        {
            "text": "L performance",
            "section_rank": 34
        },
        {
            "section": "L performance",
            "text": "The efficiencies of the L trigger lines in Run 2 are shown in figure 20 for c-hadrons and figure 21 for b-hadrons, respectively, as a function of the p T and the data-taking period. The L is optimized to fill the available \u2248 1 MHz bandwidth for a given set of LHC running conditions, and so the L efficiency evolves as a function of those conditions. In particular, if the LHC has to run with a reduced number of colliding bunches, the required rejection factor to reach 1 MHz output rate is smaller and the L criteria can be correspondingly loosened, which is the cause of the jumps visible in the bottom plots. In the case of the B 0 \u2192 K * 0 \u03b3 decay, the low efficiency of the dedicated L photon trigger is due to the limited information available to separate electrons and photons within the L system. This identification relies on the amount of electromagnetic showering observed in the preshower detector, before the photons or electrons reach the ECAL, and whether there is a hit or not in the SPD detector. The chosen working point is such that the L photon trigger has a high purity but relatively low efficiency. However, many genuine photons are selected by the L electron trigger, which is also efficient for photons. In addiction, a significant amount of photons convert in the detector material between the magnet and the SPD plane. These converted photons are reconstructed as neutral clusters offline, but leave a hit in the SPD detector and are therefore triggered as electrons. In practice the B 0 \u2192 K * 0 \u03b3 signal is selected using both electron and photon L trigger lines to account for these effects.",
            "paragraph_rank": 77,
            "section_rank": 34,
            "ref_spans": [
                {
                    "type": "figure",
                    "ref_id": "fig_1",
                    "start": 62,
                    "text": "figure 20 for c-hadrons and figure 21",
                    "end": 99
                }
            ]
        },
        {
            "section": "L performance",
            "text": "The efficiency of each L trigger is measured with respect to events where the corresponding SPD criterion from table 1 has already been applied. The distribution of SPD hits for B + \u2192 D 0 \u03c0 + signal candidates in Run 2 data is shown in figure 22, and is representative of typical heavy-flavour signals in LHCb. The efficiency of the SPD thresholds is generally around 90% for the L DiMuon and 50% for the other heavy-flavour L trigger lines. The advantage of these SPD requirements is that they allow looser L kinematic thresholds.",
            "paragraph_rank": 78,
            "section_rank": 34,
            "ref_spans": [
                {
                    "type": "figure",
                    "ref_id": "fig_2",
                    "start": 236,
                    "text": "figure 22",
                    "end": 245
                }
            ]
        },
        {
            "section": "L performance",
            "text": "The L trigger efficiencies as functions of the hadron p T and \u03b7 are shown in figure 23, except for the photon trigger where the signal yields are too small. The efficiency is relatively flat in \u03b7 for any given p T bin, although the calorimeter-based trigger lines do have a slightly better efficiency at high pseudorapidities.",
            "paragraph_rank": 79,
            "section_rank": 34
        },
        {
            "text": "HLT1 performance",
            "section_rank": 35
        },
        {
            "section": "HLT1 performance",
            "text": "The HLT1 trigger stage processes approximately 1 MHz of events that pass the L trigger, and reduces the event rate to around 110 kHz, which are further processed by HLT2. The HLT1 reconstruction sequence was described in section 4, while this section describes the performance of the HLT1 trigger lines.",
            "paragraph_rank": 80,
            "section_rank": 35
        },
        {
            "text": "Inclusive lines",
            "section_rank": 36
        },
        {
            "section": "Inclusive lines",
            "text": "HLT1 has two inclusive trigger lines which select events containing a particle whose decay vertex is displaced from the PV: a line which selects a single displaced track with high p T , and a line -23 - , Hadron",
            "paragraph_rank": 81,
            "section_rank": 36
        },
        {
            "section": "Inclusive lines",
            "text": ", Photon \u03b3  -25 -which selects a displaced two-track vertex with high p T . The single track line is a reoptimization of the Run 1 inclusive single track trigger [39], while the displaced two-track vertex trigger is a new development for Run 2. Both lines start by selecting good quality tracks that are inconsistent with originating from the PV. The single-track trigger then selects events based on a hyperbolic requirement in the 2D plane of the track displacement and p T .3 The two-track displaced vertex trigger selects events based on a MatrixNet classifier [40] whose input variables are the vertex-fit quality, the vertex displacement, the scalar sum of the p T of the two tracks, and the displacement of the tracks making up the vertex.",
            "paragraph_rank": 82,
            "section_rank": 36,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b38",
                    "start": 162,
                    "text": "[39]",
                    "end": 166
                },
                {
                    "type": "bibr",
                    "ref_id": "b39",
                    "start": 565,
                    "text": "[40]",
                    "end": 569
                }
            ]
        },
        {
            "section": "Inclusive lines",
            "text": "These trigger lines were primarily optimized for inclusively selecting the decays of b and c hadrons, and were trained using 26 different b-and c-hadron decays in order to make them as efficient as possible on the full spectrum of possible decay topologies. Care was taken, however, to make sure that these trigger lines would also be efficient for more exotic displaced signatures, for example hypothetical supersymmetric particles. The performance of these trigger lines is shown in figures 24 and 25. The two-track line is more efficienct at low p T , whereas the single track line performs best at high p T , such that combined they provide high efficiency over the full p T range.",
            "paragraph_rank": 83,
            "section_rank": 36
        },
        {
            "section": "Inclusive lines",
            "text": "Unlike the L trigger configurations, which changed frequently in response to varying LHC conditions, the HLT1 trigger configuration was kept largely stable, with some updates at the end of each data-taking year. The variation in the total HLT1 efficiency as a function of the data-taking period is shown in figure 26. The b-hadron efficiencies have been stable throughout the Run 2 data taking. The c-hadron efficiencies decreased midway through 2016, when a tighter HLT1 configuration was used to prevent the disk buffer from overflowing due to unexpectedly high LHC efficiency and availability. The improvements seen for some of the c-hadron channels in 2017 with respect to 2016 are caused by changes in the corresponding reference offline selections leading to a different average p T and displacement of the c-hadron, not any intrinsic variation in the HLT1 performance or thresholds.",
            "paragraph_rank": 84,
            "section_rank": 36,
            "ref_spans": [
                {
                    "type": "figure",
                    "ref_id": "fig_2",
                    "start": 307,
                    "text": "figure 26",
                    "end": 316
                }
            ]
        },
        {
            "text": "Muon lines",
            "section_rank": 37
        },
        {
            "section": "Muon lines",
            "text": "The HLT1 muon lines select muonic decays of b and c hadrons, as well muons originating from decays of W and Z bosons. As muons have an intrinsically cleaner signature than hadrons, the muon lines make use of simple rectangular selection criteria as opposed to the multivariate inclusive lines. There are four primary lines: one line that selects a single displaced muon with high p T for flavour physics; a second single muon line that selects very high p T muons, without displacement criteria, for electroweak physics; a third line that selects a dimuon pair compatible with originating from the decay of a charmonium or bottomonium resonance, or from Drell-Yan production; and a fourth line that selects displaced dimuons with no requirement on the dimuon mass. The efficiencies of the lines relevant for b-hadron decays are shown in figure 27 as obtained from data with the TISTOS method. Note that because these HLT1 muon trigger lines only run on events selected by L Muon and L DiMuon trigger lines, their absolute efficiency is lower than that of the inclusive single-track HLT1 trigger, which runs on all L -selected events. In addition to these lines, for Run 2 a new line dedicated to lower-p T dimuons has been developed which has tighter criteria on the displacement 3More complicated multivariate selection criteria, for example boosted decision trees using track quality information in addition to the displacement and p T , were tried but gave no significant increase in performance. of the dimuon but runs on all L -selected events, rather than just the muon ones. This line is particularly important for selecting rare decays of strange hadrons, that are not triggered by the L muon lines, increasing their HLT1 efficiency up to a factor three [41].",
            "paragraph_rank": 85,
            "section_rank": 37,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b42",
                    "start": 1762,
                    "text": "[41]",
                    "end": 1766
                }
            ]
        },
        {
            "text": "Calibration trigger lines",
            "section_rank": 38
        },
        {
            "section": "Calibration trigger lines",
            "text": "HLT1 contains two primary types of calibration trigger lines: a line which selects D 0 \u2192 K \u2212 \u03c0 + candidates with significant displacement from the PV, and a line which selects J/\u03c8 \u2192 \u00b5 + \u00b5 \u2212 -29 - candidates. The former is used for providing a pure sample of good tracks (the D 0 decay products) for the alignment of the tracking system, while the latter is used to provide a pure sample of muons for the alignment of the muon chambers. In addition, other trigger lines select events enriched in offaxis VELO tracks or tracks which populate the lower-occupancy regions of the RICH detectors, for use in the VELO and RICH alignment, respectively. The purity and yield of the calibration trigger lines is illustrated in figure 28, which shows the D 0 and J/\u03c8 candidates reconstructed online in their respective lines for a specific fill corresponding to approximately 18.5 pb \u22121 of integrated luminosity.",
            "paragraph_rank": 86,
            "section_rank": 38,
            "ref_spans": [
                {
                    "type": "figure",
                    "ref_id": "fig_2",
                    "start": 717,
                    "text": "figure 28",
                    "end": 726
                }
            ]
        },
        {
            "text": "Low multiplicity event and exclusive trigger lines",
            "section_rank": 39
        },
        {
            "section": "Low multiplicity event and exclusive trigger lines",
            "text": "Special trigger lines for low-multiplicity events are needed to enable the study of central exclusive production (CEP). This kind of process takes place by colourless, low-p T t-channel exchange between protons and can result in particle production in the central rapidity region. The protons remain intact and are deflected only slightly, so such production is typically accompanied by large ranges of rapidity with little detector activity, known as \"rapidity gaps\". The trigger development initially focussed on acquiring large samples of exclusively-produced dimuon candidates, but evolved to cover final states involving hadrons and calorimeter objects.",
            "paragraph_rank": 87,
            "section_rank": 39
        },
        {
            "section": "Low multiplicity event and exclusive trigger lines",
            "text": "Since low levels of activity are anticipated for CEP, events with more than 30 hits in the SPD are rejected at the hardware level. Lower-bounds are also placed on relevant detector activity measurements as appropriate for each final state. These criteria indirectly favour the selection of events with exactly one pp interaction, as opposed to either multiple-or zero-interaction events. At the HLT1 stage, the low-multiplicity events containing muons or electromagnetic calorimeter objects occur at a low enough rate that can be selected with no additional requirements. but low-multiplicity events containing hadrons are required to have at least two tracks reconstructed in the VELO.",
            "paragraph_rank": 88,
            "section_rank": 39
        },
        {
            "section": "Low multiplicity event and exclusive trigger lines",
            "text": "In addition, the low p T thresholds implemented in the Run 2 HLT1 tracking allowed several special exclusive HLT1 trigger selections to be implemented for the first time, notably trigger lines that select two-body beauty and charm hadron decays without biasing their decay times [42]. In 2018 the HeRSCheL detector [43] is employed in the L selection of CEP events, allowing for a reduction of the p T thresholds.",
            "paragraph_rank": 89,
            "section_rank": 39,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b43",
                    "start": 279,
                    "text": "[42]",
                    "end": 283
                },
                {
                    "type": "bibr",
                    "ref_id": "b44",
                    "start": 315,
                    "text": "[43]",
                    "end": 319
                }
            ]
        },
        {
            "section": "Low multiplicity event and exclusive trigger lines",
            "text": "-30 - Figure 29. Rates of the main groups of HLT1 trigger lines and the total HLT1 rate as a function of the year of data taking, shown for the trigger configuration used to take most of the luminosity in each year.",
            "paragraph_rank": 90,
            "section_rank": 39,
            "ref_spans": [
                {
                    "type": "figure",
                    "ref_id": "fig_2",
                    "start": 6,
                    "text": "Figure 29",
                    "end": 15
                }
            ]
        },
        {
            "text": "HLT1 bandwidth division",
            "section_rank": 40
        },
        {
            "section": "HLT1 bandwidth division",
            "text": "The HLT1 bandwidth is preferentially allocated to the inclusive and muon trigger lines which, by selecting b-and c-hadron decays, cover most of the LHCb physics programme. The large disk buffer available in Run 2 also makes it possible to allow generous rates for other trigger lines, however, with a total HLT1 output rate of 150 kHz which is around two times the Run 1 average. The HLT1 rates and the overlaps in the events selected by the different HLT1 trigger lines are shown in figure 29.",
            "paragraph_rank": 91,
            "section_rank": 40
        },
        {
            "text": "HLT2 performance",
            "section_rank": 41
        },
        {
            "section": "HLT2 performance",
            "text": "The HLT2 trigger stage reduces the event rate to around 12.5 kHz, at which point the remaining events are saved to permanent storage for further analysis. The HLT2 reconstruction sequence was described in section 5, while this section describes the performance of a representative set of HLT2 trigger lines.",
            "paragraph_rank": 92,
            "section_rank": 41
        },
        {
            "text": "Inclusive b-hadron trigger lines",
            "section_rank": 42
        },
        {
            "section": "Inclusive b-hadron trigger lines",
            "text": "The HLT2 inclusive b-hadron trigger lines look for a two-, three-, or four-track vertex with sizable p T , significant displacement from the PV, and a topology compatible with the decay of a b hadron. As in Run 1 [44,45], these \"topological\" trigger lines rely on a multivariate selection of the displaced vertex. This selection is implemented in a MatrixNet classifier whose inputs have been discretized [45] in order to minimize the variation in selection performance with varying detector conditions and speed up the evaluation time. The efficiency of the topological trigger lines is increased for decays involving muons by relaxing the requirement on the multivariate discriminant whenever one or more of the tracks associated with the topological vertex is positively identified as a muon or electron. The topological trigger lines are trained to separate signal b-hadron decays which can be fully reconstructed inside the detector acceptance from those which cannot, as well as from displaced vertices formed from the decays of c hadrons originating from the PV. The displaced vertices from c hadrons are the most numerous background. Harder to discriminate against, however, are the backgrounds from b-hadron decays that are only partially contained in the detector acceptance, or b-hadron decays in which much of the energy is taken by neutral particles. The selection has been reoptimized [46] for Run 2, taking advantage of the full offline reconstruction now available in HLT2 to loosen the selection criteria when building vertex candidates, and fully relying on the multivariate algorithm to discriminate between signal and background. The resulting efficiencies are shown in figure 30 and figure 31 for a specific decay mode, while the evolution of the efficiency as a function of the data-taking conditions is shown in figure 32.",
            "paragraph_rank": 93,
            "section_rank": 42,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b45",
                    "start": 213,
                    "text": "[44,",
                    "end": 217
                },
                {
                    "type": "bibr",
                    "ref_id": "b46",
                    "start": 217,
                    "text": "45]",
                    "end": 220
                },
                {
                    "type": "bibr",
                    "ref_id": "b46",
                    "start": 405,
                    "text": "[45]",
                    "end": 409
                },
                {
                    "type": "bibr",
                    "ref_id": "b47",
                    "start": 1399,
                    "text": "[46]",
                    "end": 1403
                }
            ]
        },
        {
            "text": "Muon and dimuon trigger lines",
            "section_rank": 43
        },
        {
            "section": "Muon and dimuon trigger lines",
            "text": "The HLT2 muon and dimuon trigger lines select a wide spectrum of signals: low-mass Drell-Yan dimuons for electroweak physics, dimuons originating from the PV for production measurements, dimuons with displacement from the PV for the study of b-, c-, and s-hadron decays and heavy dimuons for exotic particle searches and electroweak physics. As mentioned in section 4.2, in Run 2 the HLT2 and offline muon-identification procedures are identical. Owing to this improvement and because muons provide a relatively rare and clean event signature, the dimuon trigger lines generally have a high efficiency which is only limited in some cases by the rate of the selected signal, most notably for production measurements. This is illustrated in figure 33 where the efficiency of the HLT2 muon trigger lines is shown for B + \u2192 J/\u03c8 K + decays. Note that the muon topological trigger lines have a lower absolute efficiency compared to the hadron topological trigger lines because they -32 - only process events passing the HLT1 single-muon selection. In addition to the standard inclusive muon lines used in Run 1, for Run 2 new lines have been developed in particular for dimuons with lower p T for exotic particle searches (e.g. dark photons) and for rare strange-hadron decays [41].",
            "paragraph_rank": 94,
            "section_rank": 43,
            "ref_spans": [
                {
                    "type": "figure",
                    "ref_id": "fig_3",
                    "start": 739,
                    "text": "figure 33",
                    "end": 748
                },
                {
                    "type": "bibr",
                    "ref_id": "b42",
                    "start": 1271,
                    "text": "[41]",
                    "end": 1275
                }
            ]
        },
        {
            "text": "Exclusive and calibration trigger lines",
            "section_rank": 44
        },
        {
            "section": "Exclusive and calibration trigger lines",
            "text": "In addition to the inclusive trigger lines, the full offline reconstruction performed at the start of HLT2 means that it is possible to fully reconstruct certain decays of interest and select them using  Figure 33. The TOS efficiency of the HLT2 muon trigger lines as a function of the (left) B + p T and (right) units of the average B + decay time. The decay time plot is drawn such that the x-axis is binned in units of the B + lifetime in its rest frame. The efficiency of the inclusive topological (\"any topological\") trigger lines and topological trigger lines requiring one track identified as a muon (\"any muon topological\") are plotted for reference. dedicated trigger lines without any loss in efficiency compared to the offline analysis. This is especially important for c-hadron trigger lines because around 10% of all 13 TeV proton-proton collisions produce a cc pair, and it is not possible to write all c-hadron signals to offline storage. In order to reduce the necessary disk space, the LHCb exclusive c-hadron trigger lines make extensive use of the TURBO stream. All events selected by those trigger lines, except those containing neutral particles, are sent to the TURBO stream. The selection criteria of these trigger lines are usually a slightly looser version of those used in the offline analysis, enabling the candidates saved in the TURBO stream to be directly used by the analysts. In total, over 200 different exclusive trigger lines which select the decays of c hadrons are deployed in Run 2. They are generally tuned to have S/B ratios well in excess of 1 already at the output of the trigger, with the final selection performed offline using information reconstructed in the trigger and tuned to minimize systematic uncertainties. The purity achievable using the trigger-level information has already been illustrated in figure 18 for a representative sample of c-hadron decays.",
            "paragraph_rank": 95,
            "section_rank": 44,
            "ref_spans": [
                {
                    "type": "figure",
                    "ref_id": "fig_3",
                    "start": 204,
                    "text": "Figure 33",
                    "end": 213
                },
                {
                    "type": "figure",
                    "ref_id": "fig_1",
                    "start": 1851,
                    "text": "figure 18",
                    "end": 1860
                }
            ]
        },
        {
            "section": "Exclusive and calibration trigger lines",
            "text": "In addition, HLT2 contains a suite of calibration trigger lines, which are used to measure the performance of the track-finding and particle-identification algorithms in a data-driven way. These trigger lines select high-yield charm, charmonium, and K 0 S decays using a tag-and-probe approach, where the probe particle is kept unbiased with respect to either the tracking or particle-identification information. There are around 50 such lines in total, and they select around 500 Hz of calibration signals.",
            "paragraph_rank": 96,
            "section_rank": 44
        },
        {
            "text": "Low multiplicity event trigger lines",
            "section_rank": 45
        },
        {
            "section": "Low multiplicity event trigger lines",
            "text": "At the HLT2 stage there are dedicated selections for each relevant final state with a low track multiplicity. There are 32 lines: two to select exclusive dimuon production, three to select exclusive -34 - production of photons or electrons, and the remainder to select various hadronic final states, dominated by lines that select low-p T hadrons.",
            "paragraph_rank": 97,
            "section_rank": 45
        },
        {
            "text": "JINST 14 P04013",
            "section_rank": 46
        },
        {
            "section": "JINST 14 P04013",
            "text": "The HLT2 trigger efficiencies have been determined in data and are shown in figure 34 for two channels of particular interest: dimuon and dihadron. The dimuon HLT2 trigger efficiency is determined using a sample of independently triggered candidates reconstructed in events containing exactly two muon tracks inside the detector acceptance. The dimuon candidate is required to have satisfied the relevant low-multiplicity L trigger. The efficiency is shown as a function of dimuon mass, where the rise at 800 MeV/c 2 results from the 400 MeV/c p T requirement for each muon. In the case of exclusive production, where the candidate is expected to be produced with low p T , this leads to an implicit lower bound on the mass of the exclusively-produced object at m(\u00b5 + \u00b5 \u2212 ) \u2248 800 MeV/c 2 . The non-zero efficiency for candidates with m(\u00b5 + \u00b5 \u2212 ) 800 MeV/c 2 arises from candidates with higher p T .",
            "paragraph_rank": 98,
            "section_rank": 46
        },
        {
            "section": "JINST 14 P04013",
            "text": "The dihadron HLT2 trigger efficiency, which includes the effect of a 50% prescale, is determined using \u03c6(1020) \u2192 K + K \u2212 candidates reconstructed in low-multiplicity events and triggered independently of the signal candidate. The \u03c6(1020) candidate is required to pass the relevant lowmultiplicity L and HLT1 trigger lines, and the background from misidentified pions is reduced using information from the RICH sub-detectors. The efficiency is shown as a function of the p T of the \u03c6(1020) meson.",
            "paragraph_rank": 99,
            "section_rank": 46
        },
        {
            "text": "HLT2 bandwidth division",
            "section_rank": 47
        },
        {
            "section": "HLT2 bandwidth division",
            "text": "The HLT2 bandwidth is divided into the full stream, containing inclusive trigger lines, and the TURBO stream, which contains exclusive trigger lines that fully reconstruct relevant decays. Most of the full stream rate is taken up by the topological b-hadron, inclusive c-hadron, and dimuon trigger lines, while the TURBO stream rate is divided among several hundred exclusive c-hadron trigger lines. As the TURBO stream trigger lines perform a full selection of high-purity signals, their rates are generally proportional to the signal abundance. The HLT2 rates and the overlaps in the events selected by the different HLT2 trigger lines are shown in figure 35, where the exclusive trigger lines are counted as one item for brevity.",
            "paragraph_rank": 100,
            "section_rank": 47,
            "ref_spans": [
                {
                    "type": "figure",
                    "ref_id": "fig_3",
                    "start": 651,
                    "text": "figure 35",
                    "end": 660
                }
            ]
        },
        {
            "section": "HLT2 bandwidth division",
            "text": "- 35 -2019 JINST 14 P04013 Figure 35. Rates of the main categories of HLT2 trigger lines and the total HLT2 rate for each year of data taking, shown for the trigger configuration used to take most of the luminosity in the given year. TURBO, CALIBRATION, and FULL refer to different output data streams as discussed in ref. [2].",
            "paragraph_rank": 101,
            "section_rank": 47,
            "ref_spans": [
                {
                    "type": "bibr",
                    "start": 2,
                    "text": "35 -",
                    "end": 6
                },
                {
                    "type": "figure",
                    "ref_id": "fig_3",
                    "start": 27,
                    "text": "Figure 35",
                    "end": 36
                },
                {
                    "type": "bibr",
                    "ref_id": "b1",
                    "start": 323,
                    "text": "[2]",
                    "end": 326
                }
            ]
        },
        {
            "text": "Conclusions",
            "section_rank": 48
        },
        {
            "section": "Conclusions",
            "text": "The design and performance of the LHCb Run 2 reconstruction and High Level Trigger have been presented. The use of real-time alignment and calibration and improvements in the reconstruction software allows for events to be fully reconstructed in the High Level Trigger with equivalent quality to the Run 1 offline performance, and enables signals to be selected with a purity close to that achievable offline. This in turn enables physics analysis to be performed directly with the output of the reconstruction in the trigger. To this end, a significant fraction of triggered events is saved in a reduced \"real-time analysis\" format, saving only higher-level reconstructed objects relevant to physics analysis and not the full raw detector data. The successful deployment of this full real-time reconstruction and analysis during Run 2 is a critical stepping stone towards the LHCb upgrade, whose software trigger will have to deal with roughly 100 times greater data rates while maintaining a high acceptance over the same broad range of physics channels.",
            "paragraph_rank": 102,
            "section_rank": 48
        },
        {
            "text": "Figure 1 .",
            "section_rank": 49
        },
        {
            "section": "Figure 1 .",
            "text": "Figure 1. Overview of the LHCb trigger system.",
            "paragraph_rank": 103,
            "section_rank": 49
        },
        {
            "text": "Figure 2 .",
            "section_rank": 50
        },
        {
            "section": "Figure 2 .",
            "text": "Figure 2. Sketch of the different types of tracks within LHCb.",
            "paragraph_rank": 104,
            "section_rank": 50
        },
        {
            "text": "Figure 3 .",
            "section_rank": 51
        },
        {
            "section": "Figure 3 .",
            "text": "Figure 3. Schematic view of the real-time alignment and calibration procedure starting at the beginning of each fill, as used for 2018 data taking.",
            "paragraph_rank": 105,
            "section_rank": 51
        },
        {
            "text": "Figure 4 .",
            "section_rank": 52
        },
        {
            "section": "Figure 4 .",
            "text": "Figure 4. Sketch of the HLT1 track and vertex reconstruction.",
            "paragraph_rank": 106,
            "section_rank": 52
        },
        {
            "text": "Figure 5 .",
            "section_rank": 53
        },
        {
            "section": "Figure 5 .",
            "text": "Figure 5. The PV x (left) and z (right) resolution as a function of the number of tracks in the PV for the Run 1 offline and Run 2 (used both offline and online) PV reconstruction algorithms.",
            "paragraph_rank": 107,
            "section_rank": 53
        },
        {
            "text": "Figure 6 .",
            "section_rank": 54
        },
        {
            "section": "Figure 6 .",
            "text": "Figure 6. HLT1 dimuon efficiency as a function of the minimum p T of the two muons. A large gain, especially at low p T , can be seen from the comparison of the Run 1 and Run 2 algorithms.",
            "paragraph_rank": 108,
            "section_rank": 54
        },
        {
            "text": "Figure 7 .",
            "section_rank": 55
        },
        {
            "section": "Figure 7 .",
            "text": "Figure 7. HLT1 muon identification efficiency for (left) muons from J/\u03c8 \u2192 \u00b5 + \u00b5 \u2212 decays and (right) pions from D 0 \u2192 K \u2212 \u03c0 + decays. Green circles show only the identification efficiency (HLT1 Muon ID) while red squares show the efficiency of the additional trigger line (named HLT1TrackMuon) requirements (see text).",
            "paragraph_rank": 109,
            "section_rank": 55
        },
        {
            "text": "Figure 8 .",
            "section_rank": 56
        },
        {
            "section": "Figure 8 .",
            "text": "Figure 8. Sketch of the HLT2 track and vertex reconstruction sequence.",
            "paragraph_rank": 110,
            "section_rank": 56
        },
        {
            "text": "AFigure 9 .",
            "section_rank": 57
        },
        {
            "section": "AFigure 9 .",
            "text": "Figure 9. Performance of the fake-track classifier on (left) D \u2192 K \u2212 \u03c0 + and (right) K 0 S \u2192 \u03c0 \u2212 \u03c0 + decays. For these plots, the clones have been removed.",
            "paragraph_rank": 111,
            "section_rank": 57
        },
        {
            "text": "Figure 10 .Figure 11 .",
            "section_rank": 58
        },
        {
            "section": "Figure 10 .Figure 11 .",
            "text": "Figure 10. Comparison of the track reconstruction efficiency in 2015 and 2012 data as a function of the momentum (left) and pseudorapidity (right).",
            "paragraph_rank": 112,
            "section_rank": 58
        },
        {
            "text": "Figure 12 .Figure 13 .",
            "section_rank": 59
        },
        {
            "section": "Figure 12 .Figure 13 .",
            "text": "Figure 12. Resolution of the x (left) and y (right) components of the impact parameter comparing the 2012 (blue), 2015 (orange), 2016 (red) and 2017 (green) data-taking periods. The resolution as a function of p T is given in the bottom right corner.",
            "paragraph_rank": 113,
            "section_rank": 59
        },
        {
            "text": "Figure 15 .",
            "section_rank": 60
        },
        {
            "section": "Figure 15 .",
            "text": "Figure 15. Invariant mass of B 0 \u2192 (K + \u03c0 \u2212 )\u03b3 candidates in Run 1 (left) and Run 2 (right). The fit model includes the (red) signal component, (dashed green) combinatorial background, (dot-dashed turqoise) misidentified physics backgrounds (e.g. B 0s \u2192 \u03c6\u03b3 where a kaon is misidentified as a pion) and (dotted magenta) partially reconstructed physics backgrounds.",
            "paragraph_rank": 114,
            "section_rank": 60
        },
        {
            "text": "Figure 17 .",
            "section_rank": 61
        },
        {
            "section": "Figure 17 .",
            "text": "Figure 17. Disk buffer usage projections during (left) and at the end of (right) the 2017 data-taking period. During data taking, simulations (red, left) are used every two weeks to determine the probability of exceeding the 80% usage threshold. In 2017, the loose HLT1 configuration was used for the entire year leading to a maximum buffer capacity of 48% (black, right). LHC Technical Stops and Machine Development (MD) periods are shown in dark and light grey, respectively. The schedule changed between when this simulation was run in week 32 and the end of the year. An MD period was removed and the duration of the data taking was reduced.",
            "paragraph_rank": 115,
            "section_rank": 61
        },
        {
            "text": "Figure 18 .Figure 19 .",
            "section_rank": 62
        },
        {
            "section": "Figure 18 .Figure 19 .",
            "text": "Figure 18. Charm candidates used for the evaluation of the trigger performance.",
            "paragraph_rank": 116,
            "section_rank": 62
        },
        {
            "text": "Figure 20 .",
            "section_rank": 63
        },
        {
            "section": "Figure 20 .",
            "text": "Figure 20. Efficiencies of the L trigger lines in Run 2 data for c-hadron decays. The left plot shows the efficiency as a function of the hadron p T , while the right plot shows the evolution of the efficiency as a function of the different trigger configurations used during data taking. The three blocks visible in the plot, separated by vertical gaps, correspond to the three years of data taking (2015-2017). The L hadron efficiency is shown.",
            "paragraph_rank": 117,
            "section_rank": 63
        },
        {
            "text": "Figure 21 .Figure 22 .Figure 23 .",
            "section_rank": 64
        },
        {
            "section": "Figure 21 .Figure 22 .Figure 23 .",
            "text": "Figure 21. Efficiencies of the L trigger lines in Run 2 data for b-hadron decays. The left plot shows the efficiency as a function of the hadron p T , while the right plot shows the evolution of the efficiency as a function of the different trigger configurations used during data taking. The three blocks visible in the plot, separated by vertical gaps, correspond to the three years of data taking (2015-2017). The plotted L efficiency for each b-hadron is described in the legend above the plots.",
            "paragraph_rank": 118,
            "section_rank": 64
        },
        {
            "text": "Figure 24 .Figure 25 .Figure 26 .Figure 27 .",
            "section_rank": 65
        },
        {
            "section": "Figure 24 .Figure 25 .Figure 26 .Figure 27 .",
            "text": "Figure 24. Efficiency of the HLT1 inclusive trigger lines as a function of (left) c-hadron p T and (right) decay time. The decay time plots are drawn such that the x-axis is binned in units of the lifetime for each hadron in its rest frame. The plots in each column show, from top to bottom, the single-track, two-track, and combined HLT1 inclusive performance.",
            "paragraph_rank": 119,
            "section_rank": 65
        },
        {
            "text": "Figure 28 .",
            "section_rank": 66
        },
        {
            "section": "Figure 28 .",
            "text": "Figure 28. The D 0 (left) and J/\u03c8 (right) candidates selected by the HLT1 calibration lines. Both plots show candidates reconstructed online.",
            "paragraph_rank": 120,
            "section_rank": 66
        },
        {
            "text": "Figure 30 .",
            "section_rank": 67
        },
        {
            "section": "Figure 30 .",
            "text": "Figure 30. Efficiency of the HLT2 topological trigger lines as a function of the (left) b-hadron p T and (right) in units of the average b-hadron decay time. The decay time plots are drawn such that the x-axis is binned in units of the lifetime for each hadron in its rest frame. The plots show the combined efficiency of the topological trigger lines for each b-hadron decay mode.",
            "paragraph_rank": 121,
            "section_rank": 67
        },
        {
            "text": "Figure 31 .Figure 32 .",
            "section_rank": 68
        },
        {
            "section": "Figure 31 .Figure 32 .",
            "text": "Figure 31. Efficiency of the HLT2 topological trigger lines as a function of the (left) b-hadron p T and (right) in units of the average b-hadron decay time. The decay time plots are drawn such that the x-axis is binned in units of the lifetime for each hadron in its rest frame. The plots show the different contributions of the 2-, 3-, and 4-body topological trigger lines to a specific decay.",
            "paragraph_rank": 122,
            "section_rank": 68
        },
        {
            "text": "Figure 34 .",
            "section_rank": 69
        },
        {
            "section": "Figure 34 .",
            "text": "Figure 34. HLT2 trigger efficiencies of the dedicated selections for low-multiplicity events: (left) for dimuon candidates as a function of dimuon mass, and (right) for \u03c6(1020) candidates as a function of candidate p T .",
            "paragraph_rank": 123,
            "section_rank": 69
        },
        {
            "text": "RECONSTRUCTION STEP OUTPUT OBJECTS EXECUTION ORDER VELO tracking with simplified Kalman Filter Primary Vertex finding VELO \u2192 TT tracking inital momentum estimate TT \u2192 T stations tracking with p T > 500 MeV/c Full Kalman Filtering Fake track rejection VELO tracks Primary Vertices (PV) upstream tracks long tracks fitted long tracks",
            "section_rank": 70
        },
        {
            "text": "Acknowledgments",
            "section_rank": 72
        },
        {
            "section": "Acknowledgments",
            "text": "We express our gratitude to our colleagues in the CERN accelerator departments for the excellent performance of the LHC. We thank the technical and administrative staff at the LHCb institutes. We acknowledge support from CERN and from the national agencies: CAPES, CNPq, ",
            "paragraph_rank": 125,
            "section_rank": 72
        }
    ]
}