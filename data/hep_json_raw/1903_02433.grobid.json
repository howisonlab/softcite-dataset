{
    "level": "paragraph",
    "abstract": [
        {
            "text": "A Generative-Adversarial Network (GAN) based on convolutional neural networks is used to simulate the production of pairs of jets at the LHC. The GAN is trained on events generated using MadGraph5, Pythia8, and Delphes3 fast detector simulation. We demonstrate that a number of kinematic distributions both at Monte Carlo truth level and after the detector simulation can be reproduced by the generator network.",
            "paragraph_rank": 1,
            "section_rank": 1
        },
        {
            "text": "The code can be checked out or forked from the publicly accessible online repository https://gitlab.cern.ch/disipio/DiJetGAN.",
            "paragraph_rank": 2,
            "section_rank": 1
        }
    ],
    "body_text": [
        {
            "text": "Introduction",
            "section_rank": 2
        },
        {
            "section": "Introduction",
            "text": "In the forthcoming years, experiments at the Large Hadron Collider (LHC) are expected to cope with a deluge of data. At the same time, the strategy to produce reliable and statistically large samples of simulated proton-proton (pp) inelastic collisions will be facing both technological limitations and new opportunities.",
            "paragraph_rank": 3,
            "section_rank": 2
        },
        {
            "section": "Introduction",
            "text": "In the context of Machine Learning (ML) and specifically Deep Neural Networks (DNN), an unsupervised learning technique called Generative-Adversarial Networks (GAN) has been proposed recently [1] to create a sample based on a set of unlabelled training examples. In practice, GANs have been widely used to generate photorealistic portraits [2], music [3], and recently also to simulate the response of a calorimeter to the passage of particles [4][5][6][7][8][9][10]. Other deep learning methodologies such as variational autoencoders (VAE) [12] have been applied to reproduce several kinematic distributions learned from examples taken from Monte Carlo simulations. The same study also reports a less satisfactory performance when a GAN based on fully-connected deep networks was used [13]. In another work [11], authors employed a similar architecture, but also taking into account the symmetries of the process under consideration. The events used for the training are pre-processed so that the azimuthal angle of the leading charged lepton is always zero, which leads to a reduction of the degrees of freedom of the problem. Despite the fact that a substantial improvement is observed in terms of agreement with the testing sample, the distribution of the azimuthal angle is not uniform, indicating that the network failed to learn the left-right symmetry. In",
            "paragraph_rank": 4,
            "section_rank": 2,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b0",
                    "start": 192,
                    "text": "[1]",
                    "end": 195
                },
                {
                    "type": "bibr",
                    "ref_id": "b1",
                    "start": 340,
                    "text": "[2]",
                    "end": 343
                },
                {
                    "type": "bibr",
                    "ref_id": "b2",
                    "start": 351,
                    "text": "[3]",
                    "end": 354
                },
                {
                    "type": "bibr",
                    "ref_id": "b3",
                    "start": 444,
                    "text": "[4]",
                    "end": 447
                },
                {
                    "type": "bibr",
                    "ref_id": "b5",
                    "start": 447,
                    "text": "[5]",
                    "end": 450
                },
                {
                    "type": "bibr",
                    "ref_id": "b6",
                    "start": 450,
                    "text": "[6]",
                    "end": 453
                },
                {
                    "type": "bibr",
                    "ref_id": "b7",
                    "start": 453,
                    "text": "[7]",
                    "end": 456
                },
                {
                    "type": "bibr",
                    "ref_id": "b8",
                    "start": 456,
                    "text": "[8]",
                    "end": 459
                },
                {
                    "type": "bibr",
                    "ref_id": "b9",
                    "start": 459,
                    "text": "[9]",
                    "end": 462
                },
                {
                    "type": "bibr",
                    "ref_id": "b10",
                    "start": 462,
                    "text": "[10]",
                    "end": 466
                },
                {
                    "type": "bibr",
                    "ref_id": "b12",
                    "start": 541,
                    "text": "[12]",
                    "end": 545
                },
                {
                    "type": "bibr",
                    "ref_id": "b13",
                    "start": 786,
                    "text": "[13]",
                    "end": 790
                },
                {
                    "type": "bibr",
                    "ref_id": "b11",
                    "start": 808,
                    "text": "[11]",
                    "end": 812
                }
            ]
        },
        {
            "text": "JHEP08(2019)110",
            "section_rank": 3
        },
        {
            "section": "JHEP08(2019)110",
            "text": "this work, we successfully trained a GAN to reproduce kinematic distributions, improving on such previous attempts, thanks to a careful consideration of a larger number of implicit symmetries of the physics process under study, and the employment of deep convolutional layers [14] following the example of [15].",
            "paragraph_rank": 5,
            "section_rank": 3,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b14",
                    "start": 276,
                    "text": "[14]",
                    "end": 280
                },
                {
                    "type": "bibr",
                    "ref_id": "b15",
                    "start": 306,
                    "text": "[15]",
                    "end": 310
                }
            ]
        },
        {
            "section": "JHEP08(2019)110",
            "text": "In a GAN, a generative network G transforms a vector of random numbers (input noise) z \u223c p z into a sample carrying some physical meaning, which in this case are the fourmomenta of the two jets. In practical applications, p z is usually a N -dimensional uniform distribution in the range [0, 1] N . Subsequently, a discriminative network D estimates the probability that a given sample comes either from the training data or the generator. The two samples are distributed with probability density functions (pdf ) p data and p fake respectively, with p data fixed and usually estimated using a Monte Carlo method. The Nash equilibrium (min-max game) is reached when D is unable to distinguish fake examples from real data, hence the generator has been trained to be a good approximator of the data pdf, i.e. p fake \u223c p data .",
            "paragraph_rank": 6,
            "section_rank": 3
        },
        {
            "section": "JHEP08(2019)110",
            "text": "In this article, we introduce the architecture of a GAN that aims to simulate the production of pairs of particles in pp interactions at the LHC. In particular, this approach is applied to dijet production, which is a ubiquitous source of background to Standard Model precision measurements [16][17][18] and searches for physics beyond the Standard Model [19][20][21][22].",
            "paragraph_rank": 7,
            "section_rank": 3,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b16",
                    "start": 291,
                    "text": "[16]",
                    "end": 295
                },
                {
                    "type": "bibr",
                    "ref_id": "b17",
                    "start": 295,
                    "text": "[17]",
                    "end": 299
                },
                {
                    "type": "bibr",
                    "ref_id": "b18",
                    "start": 299,
                    "text": "[18]",
                    "end": 303
                },
                {
                    "type": "bibr",
                    "ref_id": "b19",
                    "start": 355,
                    "text": "[19]",
                    "end": 359
                },
                {
                    "type": "bibr",
                    "ref_id": "b20",
                    "start": 359,
                    "text": "[20]",
                    "end": 363
                },
                {
                    "type": "bibr",
                    "ref_id": "b21",
                    "start": 363,
                    "text": "[21]",
                    "end": 367
                },
                {
                    "type": "bibr",
                    "ref_id": "b23",
                    "start": 367,
                    "text": "[22]",
                    "end": 371
                }
            ]
        },
        {
            "section": "JHEP08(2019)110",
            "text": "Currently, both the ATLAS [24] and CMS [25] experiments of the LHC at CERN deploy Monte Carlo (MC) event generators such as MadGraph5 [26], POWHEG-BOX [27], MC@NLO [28], Pythia8 [29], Herwig7 [30] and Sherpa [31] to simulate the hardscattering (HS) and the parton-shower (PS) processes, and a GEANT4 [33] simulation of the actual detector for the response of the experimental apparatus. Best estimates suggest that the simulation of a single event takes already several minutes [34], with O(10 9 ) events to be generated for each simulation campaign, leading to a huge computational footprint both in terms of CPU usage and disk space. The situation is particularly critical for processes that require the matching of fixed-order next-to-leading matrix element calculations to parton shower (\"multijet merging\"), as implemented for example in the FxFx [35],",
            "paragraph_rank": 8,
            "section_rank": 3,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b25",
                    "start": 26,
                    "text": "[24]",
                    "end": 30
                },
                {
                    "type": "bibr",
                    "ref_id": "b26",
                    "start": 39,
                    "text": "[25]",
                    "end": 43
                },
                {
                    "type": "bibr",
                    "ref_id": "b27",
                    "start": 134,
                    "text": "[26]",
                    "end": 138
                },
                {
                    "type": "bibr",
                    "ref_id": "b28",
                    "start": 151,
                    "text": "[27]",
                    "end": 155
                },
                {
                    "type": "bibr",
                    "ref_id": "b29",
                    "start": 164,
                    "text": "[28]",
                    "end": 168
                },
                {
                    "type": "bibr",
                    "ref_id": "b30",
                    "start": 178,
                    "text": "[29]",
                    "end": 182
                },
                {
                    "type": "bibr",
                    "ref_id": "b31",
                    "start": 192,
                    "text": "[30]",
                    "end": 196
                },
                {
                    "type": "bibr",
                    "ref_id": "b32",
                    "start": 208,
                    "text": "[31]",
                    "end": 212
                },
                {
                    "type": "bibr",
                    "ref_id": "b34",
                    "start": 300,
                    "text": "[33]",
                    "end": 304
                },
                {
                    "type": "bibr",
                    "ref_id": "b35",
                    "start": 478,
                    "text": "[34]",
                    "end": 482
                },
                {
                    "type": "bibr",
                    "ref_id": "b36",
                    "start": 852,
                    "text": "[35]",
                    "end": 856
                }
            ]
        },
        {
            "section": "JHEP08(2019)110",
            "text": "MePSNLO [36] and UNLoPS [37] methods. Currently, such advanced MC generators cannot be used as the standard generators by the LHC collaborations due to the large time required to generate the billions of events normally required. Hence the experiments need to rely on simpler, but less accurate, generators. Providing a solution to extend the simulated events to the requirements of the LHC experiments will significantly enhance a wide range of measurements. Similarly, detailed simulation based on GEANT4 will not be an affordable solution due to the large time required to simulate an event [34]. Both experiments are already using fast simulation and are developing new tools exploiting ML and other advanced statistical techniques. We will demonstrate that the same GAN used for reproducing the generator output can be also used to reproduce a simulation of a detector response with a significant time gain with respect to full simulation.",
            "paragraph_rank": 9,
            "section_rank": 3,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b37",
                    "start": 8,
                    "text": "[36]",
                    "end": 12
                },
                {
                    "type": "bibr",
                    "ref_id": "b38",
                    "start": 24,
                    "text": "[37]",
                    "end": 28
                },
                {
                    "type": "bibr",
                    "ref_id": "b35",
                    "start": 594,
                    "text": "[34]",
                    "end": 598
                }
            ]
        },
        {
            "section": "JHEP08(2019)110",
            "text": "The code is publicly accessible on the online repository https://gitlab.cern.ch/disipio/ DiJetGAN.",
            "paragraph_rank": 10,
            "section_rank": 3
        },
        {
            "text": "JHEP08(2019)110 2 Physics of QCD dijet events",
            "section_rank": 4
        },
        {
            "section": "JHEP08(2019)110 2 Physics of QCD dijet events",
            "text": "At hadron colliders such as the LHC, the most abundant kind of interaction between the two colliding protons is the scattering between quarks and gluons (collectively referred to as partons). According to calculations based on the SM, these parton-scattering processes via strong interactions described by quantum chromodynamics (QCD) result in the overwhelming majority of cases in two outgoing partons which carry a net color charge and evolve from high to low virtuality producing parton showers, which eventually hadronize into collimated highly-energetic clusters of particles called jets. Hence, 2\u21922 parton scattering processes with a pair of jets in the final state are called dijet events. The relationship between the clusters and the original partons is revealed by the execution of a clustering algorithm [32]. One can think of a jet approximately as a cone of radius R whose axis correspond to the direction of flight of the initial parton. The size of the radius can be controlled by setting a distance parameter in the clustering algorithm.",
            "paragraph_rank": 11,
            "section_rank": 4,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b33",
                    "start": 816,
                    "text": "[32]",
                    "end": 820
                }
            ]
        },
        {
            "section": "JHEP08(2019)110 2 Physics of QCD dijet events",
            "text": "For most analyses, the most relevant jets are produced with a large transverse momentum (p T ) and large angle with respect to the incoming partons. The jet mass, defined as the norm of the four-momentum sum of constituents inside a jet, is only loosely related to the mass of the originating parton, and comes mostly from the dynamics of strong interactions. Programs such as Pythia8 [29] and Herwig7 [30] implement such calculations with beyond the leading-logarithm (LL) accuracy in what are called a Parton Shower algorithms. The jet mass also plays a key role in the identification of Lorentz-boosted hadronically decaying massive particles such as top quarks [16,21], vector (W and Z) and Higgs bosons [23]. Finally, QCD predicts a characteristically smooth and monotonically decreasing distribution for the dijet invariant mass (m jj ), and small production angle. Instead, many theories beyond the SM predict the presence of additional massive particles decaying to dijet pairs, whose hypothetical presence would distort both the dijet invariant mass and production angle distributions in model-dependent ways [19,20]. It is therefore extremely important to be able to reproduce these distributions in a simulation, with particular emphasis on the region of the phase space with m jj > 1 TeV, where signs of physics beyond the Standard Model may become evident.",
            "paragraph_rank": 12,
            "section_rank": 4,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b30",
                    "start": 385,
                    "text": "[29]",
                    "end": 389
                },
                {
                    "type": "bibr",
                    "ref_id": "b31",
                    "start": 402,
                    "text": "[30]",
                    "end": 406
                },
                {
                    "type": "bibr",
                    "ref_id": "b16",
                    "start": 665,
                    "text": "[16,",
                    "end": 669
                },
                {
                    "type": "bibr",
                    "ref_id": "b21",
                    "start": 669,
                    "text": "21]",
                    "end": 672
                },
                {
                    "type": "bibr",
                    "start": 708,
                    "text": "[23]",
                    "end": 712
                },
                {
                    "type": "bibr",
                    "ref_id": "b19",
                    "start": 1118,
                    "text": "[19,",
                    "end": 1122
                },
                {
                    "type": "bibr",
                    "ref_id": "b20",
                    "start": 1122,
                    "text": "20]",
                    "end": 1125
                }
            ]
        },
        {
            "section": "JHEP08(2019)110 2 Physics of QCD dijet events",
            "text": "In the following sections, the agreement between MC calculations and the output of the GAN is evaluated by comparing the individual jets' and dijet system's transverse momentum, pseudo-rapidity 1 (\u03b7) and mass distributions. The \u03c7 2 between the MC and the GAN distributions is used as figure of merit.",
            "paragraph_rank": 13,
            "section_rank": 4
        },
        {
            "text": "Monte Carlo sample",
            "section_rank": 5
        },
        {
            "section": "Monte Carlo sample",
            "text": "A sample of 10 million dijet events has been generated using MadGraph5 and Pythia8, corresponding to an integrated luminosity of about 0.5 fb \u22121 . The response of the detector was simulated by a Delphes3 [38] fast simulation, using settings that resemble the ATLAS 1 Pseudorapidity is a commonly spatial coordinate describing the angle of a particle relative to the beam axis defined as \u03b7",
            "paragraph_rank": 14,
            "section_rank": 5,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b39",
                    "start": 204,
                    "text": "[38]",
                    "end": 208
                },
                {
                    "type": "bibr",
                    "ref_id": "b0",
                    "start": 265,
                    "text": "1",
                    "end": 266
                }
            ]
        },
        {
            "section": "Monte Carlo sample",
            "text": "where E is the energy and pL is the longitudinal component of the momentum. It is related to the other components of the momentum via the relationship |p| = pT cosh \u03b7.",
            "paragraph_rank": 15,
            "section_rank": 5
        },
        {
            "text": "JHEP08(2019)110",
            "section_rank": 6
        },
        {
            "section": "JHEP08(2019)110",
            "text": "detector. An average of 25 additional soft-QCD pp collisions (pile-up) were overlaid to reproduce more realistic data-taking conditions.",
            "paragraph_rank": 16,
            "section_rank": 6
        },
        {
            "section": "JHEP08(2019)110",
            "text": "Electrons, muons, jets and missing transverse energy are reconstructed by Delphes3 algorithms before and after the detector simulation. These two levels of reconstruction are referred to in the following sections as particle-and reco-level respectively. At particlelevel, only stable final-state particles, i.e. particles that are not decayed further by the generator, and unstable particles 2 that are to be decayed later by the detector simulation, are considered. Jets were reconstructed using the anti-k T algorithm [39] as implemented in FastJet [40], with a distance parameter R = 1.0.",
            "paragraph_rank": 17,
            "section_rank": 6,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b40",
                    "start": 520,
                    "text": "[39]",
                    "end": 524
                },
                {
                    "type": "bibr",
                    "ref_id": "b41",
                    "start": 551,
                    "text": "[40]",
                    "end": 555
                }
            ]
        },
        {
            "section": "JHEP08(2019)110",
            "text": "To increase the number of events with both jets with p T > 250 GeV, a cut on the scalar sum of the transverse momenta of the outgoing partons H T > 500 GeV was applied to the hard-scattering process. Approximately 7,5 million events passed this selection at particle level, and about 4 million at reco level. The difference in efficiency between the two levels of the simulation can be understood in terms of distortions introduced by the detector, which smear the jets' transverse momentum distributions, hence lowering the number of events passing the final selection. These events were used to train the network in the subsequent steps.",
            "paragraph_rank": 18,
            "section_rank": 6
        },
        {
            "text": "Network architecture",
            "section_rank": 7
        },
        {
            "section": "Network architecture",
            "text": "The overall architecture of the network, summarized in figure 1, is composed of two main blocks: a generator (G) and a discriminator (D), both based on convolutional layers. All layers have LeakyReLU activation functions [41] except the last layers that have either tanh or sigmoid for the generator and the discriminator respectively. The generator transforms a vector of 128 random numbers drawn from a uniform distribution in the [0, 1] range into a vector of 7 elements representing the p T , \u03b7 and mass of the leading jet, and the p T , \u03b7, \u03c6 and mass of the second-leading jet. The discriminator takes as input the array of 7 elements described above and gives as output a number d between 0 and 1 that is interpreted as the likelihood of the event being drawn from \"real\" MadGraph5 events (d=1) or from \"fakes\" generated by G (d=0).",
            "paragraph_rank": 19,
            "section_rank": 7,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b42",
                    "start": 221,
                    "text": "[41]",
                    "end": 225
                }
            ]
        },
        {
            "section": "Network architecture",
            "text": "The network is implemented and trained using Keras v2.2.4 [42] with Tensorflow v1.12 [43] back-end. Input features are scaled in the range [\u22121, 1] and pre-and postprocessed using the scikit-learn [44] and Pandas [45] libraries. The loss function of the generator is mean squared error, while that of the discriminator is the binary cross-entropy. The optimizer is in both cases Adam [46] with learning rate lr = 10 \u22125 , \u03b2 1 = 0.5 and \u03b2 2 = 0.9. The parameters described above are those that provide the best results among many values and configurations tested. Having reached a satisfactory performance, no further parameter optimisation was carried out.",
            "paragraph_rank": 20,
            "section_rank": 7,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b43",
                    "start": 58,
                    "text": "[42]",
                    "end": 62
                },
                {
                    "type": "bibr",
                    "ref_id": "b44",
                    "start": 85,
                    "text": "[43]",
                    "end": 89
                },
                {
                    "type": "bibr",
                    "ref_id": "b45",
                    "start": 196,
                    "text": "[44]",
                    "end": 200
                },
                {
                    "type": "bibr",
                    "ref_id": "b46",
                    "start": 212,
                    "text": "[45]",
                    "end": 216
                },
                {
                    "type": "bibr",
                    "ref_id": "b47",
                    "start": 383,
                    "text": "[46]",
                    "end": 387
                }
            ]
        },
        {
            "text": "Training",
            "section_rank": 8
        },
        {
            "section": "Training",
            "text": "For the purpose of the training, all MC events were rotated so that the azimuthal \u03c6 angle of the leading jet is always zero. A significant performance improvement was achieved by JHEP08(2019)110 exploiting the intrinsic \u03c6 symmetry in dijet events; the \u03c6 of the leading jet is set to zero while the \u03c6 of the other jet is set to the absolute value of the difference in \u03c6 between the two generated jets. This transformation is reversed when events are generated. In order to further deploy the symmetries of dijet kinematics, every event is used twice: first in its original configuration, and then with the sign of the pseudorapidity of each jet reversed (\u03b7-flip). During event generation, the \u03b7 of the jets is randomly flipped to remove any nonphysical effects that could be introduced by the GAN.",
            "paragraph_rank": 21,
            "section_rank": 8
        },
        {
            "section": "Training",
            "text": "The network was then trained for 500,000 iterations with mini-batches of 128 events each, drawn from the original distribution and from the noise-generated fakes. It took about five hours to complete the training on a GPU NVIDIA Quadro P6000. For each iteration, we first trained the discriminator to distinguish between real and fake events. Then, the discriminator weights are fixed and the generator is trained. At the end of the training, the discriminator will be unable to distinguish between the two sets. In such balanced set-up, equilibrium is reached when the generator can fool the discriminator in 50% of cases. From the definition of the cross-entropy loss this corresponds to a value of \u2212 ln p = \u2212 ln(0.5) = 0.693, where p is the probability attributed to a given class. Figures 2 and 3 show the discriminator and the generator loss as a function of the training epoch at particle and reco level respectively. The stationary state between the generator and the discriminator, also known as Nash equilibrium, is reached after a few thousands epochs. However, the agreement between the MC-and GAN-generated distributions improves in terms of \u03c7 2 as the number of iterations increases, and levels out after about 100,000 epochs, as shown in figure 6. This is true for both particle and reco level, and can be easily understood as a consequence of the stabilization of the generator losses at around the same epoch.  ",
            "paragraph_rank": 22,
            "section_rank": 8,
            "ref_spans": [
                {
                    "type": "figure",
                    "ref_id": "fig_1",
                    "start": 785,
                    "text": "Figures 2 and 3",
                    "end": 800
                },
                {
                    "type": "figure",
                    "ref_id": "fig_5",
                    "start": 1252,
                    "text": "figure 6",
                    "end": 1260
                }
            ]
        },
        {
            "text": "JHEP08(2019)110",
            "section_rank": 9
        },
        {
            "text": "Event generation and final results",
            "section_rank": 10
        },
        {
            "section": "Event generation and final results",
            "text": "During the training, the weights of the generator model are saved into a file every 5000 epochs and used subsequently to generate an arbitrary number of events. To match the size of the MadGraph5 samples, 10 million events were generated with the GAN. On average, it takes about 80 seconds to generate 1 million events on a GPU NVIDIA Quadro P6000. After the generation, events are filtered by applying the same kinematic cuts we applied to the real MC events, i.e. both jets with p T > 250 GeV, ordered by decreasing p T . Approximately 90% fulfill these requirements and are used to fill the histograms. Figures 4 and 5 show the comparison of the two leading jets and dijet system kinematics at particle-and reco-level respectively, as they appear at the iteration that yields the best agreement in terms of overall \u03c7 2 over degrees of freedom. Overall, the level of agreement is satisfactory over a large range of the kinematic regime.",
            "paragraph_rank": 23,
            "section_rank": 10,
            "ref_spans": [
                {
                    "type": "figure",
                    "ref_id": "fig_3",
                    "start": 606,
                    "text": "Figures 4 and 5",
                    "end": 621
                }
            ]
        },
        {
            "section": "Event generation and final results",
            "text": "We further investigated the agreement in regions of the phase space with low crosssection, in particular where the dijet invariant mass is in the multi-TeV regime. This kinematic region is of particularly interest for searches of physics beyond the SM. A very common approach is to fit the MC sample with the following four-parameters (4p) analytic function:",
            "paragraph_rank": 24,
            "section_rank": 10
        },
        {
            "text": "JHEP08(2019)110",
            "section_rank": 11
        },
        {
            "section": "JHEP08(2019)110",
            "text": "where x = m jj / \u221a s and p 0 , p 1 , p 2 , p 3 are the free parameters of the fit. Such function is motivated by the structure of parton distribution functions and has been widely used by Tevatron and LHC experiments [51].",
            "paragraph_rank": 25,
            "section_rank": 11,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b52",
                    "start": 217,
                    "text": "[51]",
                    "end": 221
                }
            ]
        },
        {
            "section": "JHEP08(2019)110",
            "text": "We trained the GAN using only a small fraction of the available events, about 15% corresponding to about 150,000 events with m jj > 1.5 TeV. Then, we used the trained model to generate a sample of about 11 million events, a number much larger than that of events in the MadGraph5+Pythia8 training sample. A difference between the two approaches can be seen for m jj > 8 TeV, where the number of MC events is very small. The difference between the two methods can be interpreted as a source of systematic uncertainty.   As shown in figure 7, limiting the fit in the region between 2.5 and 10 TeV, the 4p analytic function can predict the shape of the MC distribution with a \u03c7 2 /NDF = 0.98. In the same kinematic region, the sample generated with the GAN shows and agreement with comparable \u03c7 2 /NDF. Besides the agreement in a single variable, one has to take in mind that the 4p fit does not allow the user to generate event, but only to make an estimate of the background due to multijet production in that particular kinematic region and only for that specific observable. Therefore, events produced with our GAN can significantly expand the techniques used by analysis teams in determining their background.",
            "paragraph_rank": 26,
            "section_rank": 11
        },
        {
            "text": "JHEP08(2019)110",
            "section_rank": 12
        },
        {
            "text": "Possible extensions of the method",
            "section_rank": 13
        },
        {
            "section": "Possible extensions of the method",
            "text": "The baseline architecture described in section 4 can be modified for more advanced purposes that go beyond the scope of this initial work, but are relevant for practical usage in collider experiments. Both extensions were implemented and produced results compatible with those presented above. The corresponding code is available in the repository.",
            "paragraph_rank": 27,
            "section_rank": 13
        },
        {
            "text": "Arbitrary number of input variables",
            "section_rank": 14
        },
        {
            "section": "Arbitrary number of input variables",
            "text": "In future application of the method to processes whose final states involve more than two particles, it would be desirable to have a more generic handling of the input variables. A common way to achieve this is to apply a Principal Component Analysis (PCA) to the input vector, a procedure that is often referred to as whitening in Deep Learning [47]. The purpose is to reduce the dimensionality of the input vector, while at the same time retaining most of JHEP08(2019)110 the information needed to extract useful features. PCA can be generalized to a non-linear transformation by using an unsupervised Deep Learning technique called autoencoder [48]. A deep network (encoder) transforms the input in the visible representation x vis \u2208 R N to a corresponding latent representation x lat \u2208 R M with lower dimensionality, i.e. M < N . Subsequently, the latent vector is fed into another network (decoder) that transforms back to the visible space. The complete chain of transformation is thus",
            "paragraph_rank": 28,
            "section_rank": 14,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b48",
                    "start": 346,
                    "text": "[47]",
                    "end": 350
                },
                {
                    "type": "bibr",
                    "ref_id": "b49",
                    "start": 647,
                    "text": "[48]",
                    "end": 651
                }
            ]
        },
        {
            "section": "Arbitrary number of input variables",
            "text": "For the purpose of this application, the generator and discriminator networks are trained using events transformed into the latent space representation (R M ) using the encoder network. After the GAN is trained, the generator is used to create events as described above. However, before filling the histograms, the generated events are transformed back to the physical representation (R N ) using the decoder network. A similar methodology called Variational Auto-encoder (VAE) was shown [13] to be able to generate other kinds of Standard Model processes.",
            "paragraph_rank": 29,
            "section_rank": 14,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b13",
                    "start": 488,
                    "text": "[13]",
                    "end": 492
                }
            ]
        },
        {
            "text": "Conditioning on external variable",
            "section_rank": 15
        },
        {
            "section": "Conditioning on external variable",
            "text": "It is a common problem in searches for physics beyond the SM and precision measurements that regions of the phase space with large invariant mass or transverse momentum (typically in the TeV regime) have very low cross-sections and are hence under-represented in the simulated samples, yielding a large statistical uncertainty that may limit the sensitivity. Two classical solutions are either to generate very large samples and discard uninteresting events, or to bias the event generation by over-sampling certain regions of the phase-space. The first, brute-force approach, usually requires an unusually large CPU and disk space usage, while the second method requires events to be weighted, which can yield large fluctuations and uncertainties introduced by the event selection. A third possibility is to create so-called sliced samples, i.e. the event generation is split into a number of subsamples in which the value of a certain variable that controls the event kinematics (such as the invariant mass or the transverse momentum of the dijet system) is limited to a certain range. The sub-samples are then added together to obtain an inclusive sample with comparable statistical uncertainty all over the range of certain variables of interest (e.g. the transverse momentum of the leading jet). In this regard, Generative-Adversarial Networks offer a similar possibility to bias the event generation without introducing event weights by the means of the so-called variable conditioning [49]. This is achieved by adding an auxiliary parameter to the input that controls the way noise is transformed into physical events. Examples of such conditioning parameter, as in the case of sliced MC samples, are the invariant mass or the transverse momentum of the dijet system, or the average number of pile-up interactions per bunch crossing \u00b5 . In the context of calorimeter simulations, the GAN can be conditioned on the energy of the incident particle to generate showers corresponding to a specific energy [10]. The number of conditioning parameters does not have to be limited to just one: for example, as in the case of the phenomenological supersymmetric standard model (pMSSM) [50], the generator network may be conditioned to create events for any given pair of (mg, m\u03c70) masses. This could be further extended to include more supersymmetric parameters.",
            "paragraph_rank": 30,
            "section_rank": 15,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b50",
                    "start": 1492,
                    "text": "[49]",
                    "end": 1496
                },
                {
                    "type": "bibr",
                    "ref_id": "b10",
                    "start": 2008,
                    "text": "[10]",
                    "end": 2012
                },
                {
                    "type": "bibr",
                    "ref_id": "b51",
                    "start": 2183,
                    "text": "[50]",
                    "end": 2187
                }
            ]
        },
        {
            "text": "JHEP08(2019)110 8 Conclusions and outlook",
            "section_rank": 16
        },
        {
            "section": "JHEP08(2019)110 8 Conclusions and outlook",
            "text": "The Generative-Adversarial Network presented in this paper provides an attractive solution to reduce the usage of CPU and possibly disk space to generate and simulate events at the LHC experiments. While still in its infancy, this method provides a unique opportunity to improve the quality of the MC used by the LHC collaborations as they will be able to use generators that are currently too time consuming to use. In the future, it should be possible to generalize this approach to more complicated processes such as top-quark pair or vector boson production in association with jets; the best MC predictions of these processes are also limited by high CPU requirements. Our results comparing simulated events show that our GAN can reproduce simulated events with high accuracy. This proof-of-concept shows the potential of these tools to provide an efficient solution to the large number of simulated events required by the ambitious physics programme of the LHC experiments. Future work will also focus on more advanced methods to further stabilize the training and avoid model collapse, while still being able to fit the relevant kinematic distributions in regions of the phase-space with low cross-sections.",
            "paragraph_rank": 31,
            "section_rank": 16
        },
        {
            "text": "JHEP08(2019)110",
            "section_rank": 17
        },
        {
            "section": "JHEP08(2019)110",
            "text": "a second:",
            "paragraph_rank": 32,
            "section_rank": 17
        },
        {
            "section": "JHEP08(2019)110",
            "text": "The optimal generator G is such that p fake = p data , hence the minimum of the loss function is achieved, i.e. D G (x) = 1/2, JS(p fake ||p data ) = 0 and L(G , D ) = \u22122 log 2.",
            "paragraph_rank": 33,
            "section_rank": 17
        },
        {
            "text": "B Application to high-momentum top-quark pairs",
            "section_rank": 18
        },
        {
            "section": "B Application to high-momentum top-quark pairs",
            "text": "To complement the investigations described above, we also trained our GAN on a sample of top-quark pairs decaying in the all-hadronic channel, i.e. tt \u2192 W bWb \u2192 bqq b qq . To ensure that the decay products of the top quarks are collimated due to Lorentz boost in a radius R \u2264 1.0 with respect to the jet axis, a cut on the scalar sum of the transverse momenta of the outgoing partons H T > 700 GeV was applied. Also, both jets at particle level are required to have a transverse momentum p T > 350 GeV and mass < 500 GeV. In this region of the phase space, the jet mass is expected to have a peak around the top mass, which is set to 172.5 GeV in the MC simulation. Such configuration is known as \"fully contained\" top quarks. However, in some cases the b-quarks are produced at an angle such that only the W boson is actually found within \u2206R <1.0 from the jet axis. In this case, a secondary peak appears around the W boson mass, set to 80.4 GeV in the MC simulation. The total jet mass distribution is then bi-modal. Apart from these differences, overall the events at particle level look very similar to QCD dijet ones, hence our GAN should be able to deal with this alternative physics process. It is worth stressing the fact that the information about the top quark and W boson masses are not fed into the GAN, but have to be inferred by the network during the training. As can be seen from figure 8, the agreement between the MadGraph5+Pythia8MC and the GAN output is in fact satisfactory. Open Access. This article is distributed under the terms of the Creative Commons Attribution License (CC-BY 4.0), which permits any use, distribution and reproduction in any medium, provided the original author(s) and source are credited.",
            "paragraph_rank": 34,
            "section_rank": 18
        },
        {
            "text": "JHEP08(2019)110",
            "section_rank": 19
        },
        {
            "text": "Figure 1 .",
            "section_rank": 20
        },
        {
            "section": "Figure 1 .",
            "text": "Figure 1. Network architecture: generator (top), discriminator (bottom). The GAN is composed by connecting the output of the generator to the input of the discriminator.",
            "paragraph_rank": 35,
            "section_rank": 20
        },
        {
            "text": "Figure 2 .",
            "section_rank": 21
        },
        {
            "section": "Figure 2 .",
            "text": "Figure 2. Evolution of the discriminator and generator loss at particle level as a function of the training epoch.",
            "paragraph_rank": 36,
            "section_rank": 21
        },
        {
            "text": "Figure 3 .",
            "section_rank": 22
        },
        {
            "section": "Figure 3 .",
            "text": "Figure 3. Evolution of the discriminator and generator loss at reco level as a function of the training epoch.",
            "paragraph_rank": 37,
            "section_rank": 22
        },
        {
            "text": "Figure 4 .",
            "section_rank": 23
        },
        {
            "section": "Figure 4 .",
            "text": "Figure 4. Comparison of kinematic observables with respect to particle-level (MadGraph5 + Pythia8) Monte Carlo simulation. The gray area represents the MC prediction, and the black line indicates the GAN output.",
            "paragraph_rank": 38,
            "section_rank": 23
        },
        {
            "text": "Figure 5 .",
            "section_rank": 24
        },
        {
            "section": "Figure 5 .",
            "text": "Figure 5. Comparison of kinematic observables with respect to reco-level (MadGraph5 + Pythia8 + Delphes3) Monte Carlo simulation. The gray area represents the MC prediction, and the black line indicates the GAN output.",
            "paragraph_rank": 39,
            "section_rank": 24
        },
        {
            "text": "Figure 6 .",
            "section_rank": 25
        },
        {
            "section": "Figure 6 .",
            "text": "Figure 6. Evolution of \u03c7 2 as a function of training epoch at particle-and reco-level.",
            "paragraph_rank": 40,
            "section_rank": 25
        },
        {
            "text": "Figure 7 .",
            "section_rank": 26
        },
        {
            "section": "Figure 7 .",
            "text": "Figure 7. Comparison between a (MadGraph5 + Pythia8) Monte Carlo simulation sample and the GAN extrapolation to high dijet invariant mass. The gray area represents the MC prediction, the black line indicates the GAN output, and the red line is the fitted four-parameters analytic function.",
            "paragraph_rank": 41,
            "section_rank": 26
        },
        {
            "text": "Figure 8 .",
            "section_rank": 27
        },
        {
            "section": "Figure 8 .",
            "text": "Figure 8. Comparison of kinematic observables for the all-hadronic tt production with respect to particle-level (MadGraph5+Pythia8) Monte Carlo simulation. The gray area represents the MC prediction, and the black line indicates the GAN output.",
            "paragraph_rank": 42,
            "section_rank": 27
        },
        {
            "section": "Figure 8 .",
            "text": "Particles with a mean lifetime \u03c4 > 300 ps.",
            "paragraph_rank": 43,
            "section_rank": 27
        },
        {
            "text": "Acknowledgments",
            "section_rank": 29
        },
        {
            "section": "Acknowledgments",
            "text": "We acknowledge the support of the Natural Sciences and Engineering Research Council of Canada (NSERC) and of the Science, Technology and Facility Council (STFC). This project has received funding from the European Union Horizon 2020 research and innovation programme under grant agreement No 765710. We received the donation of two P6000 GPU cards (one per group) in support of our work by the NVIDIA Corporation GPU grant programme. We would like to thank Benjamin Nachman for the useful discussions.",
            "paragraph_rank": 44,
            "section_rank": 29
        },
        {
            "text": "A Mathematical description of GANs",
            "section_rank": 31
        },
        {
            "section": "A Mathematical description of GANs",
            "text": "As discussed in [1], the procedure to optimized a GAN is equivalent to the minimization of the loss function L(G, D):",
            "paragraph_rank": 45,
            "section_rank": 31,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b0",
                    "start": 16,
                    "text": "[1]",
                    "end": 19
                }
            ]
        },
        {
            "section": "A Mathematical description of GANs",
            "text": "For a fixed G, the optimal discriminator D G is given by a monotonic function of:",
            "paragraph_rank": 46,
            "section_rank": 31
        },
        {
            "section": "A Mathematical description of GANs",
            "text": "Under this condition, the min-max game can be reformulated as the minimization of the Jensen-Shannon divergence JS, which can be expressed in terms of the Kullback-Leibler divergence KL, i.e. a measure of how one probability distribution is different from",
            "paragraph_rank": 47,
            "section_rank": 31
        }
    ]
}