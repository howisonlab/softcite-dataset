{
    "level": "paragraph",
    "abstract": [
        {
            "text": "Data analysis in high energy physics has to deal with data samples produced from different sources. One of the most widely used ways to unfold their contributions is the sPlot technique. It uses the results of a maximum likelihood fit to assign weights to events. Some weights produced by sPlot are by design negative. Negative weights make it difficult to apply machine learning methods. The loss function becomes unbounded. This leads to divergent neural network training. In this paper we propose a mathematically rigorous way to transform the weights obtained by sPlot into class probabilities conditioned on observables, thus enabling to apply any machine learning algorithm out-of-the-box.",
            "paragraph_rank": 0,
            "section_rank": 1
        }
    ],
    "body_text": [
        {
            "text": "Introduction",
            "section_rank": 2
        },
        {
            "section": "Introduction",
            "text": "Data obtained by high energy physics experiments is often a mixture of signal and background events. Application of conventional classification methods to such data is a challenging task as obtaining precise label information is usually impossible. Instead, one can use a probabilistic approach by employing so-called discriminative variables (usually, the invariant mass), whose distributions for signal and background events are known in advance, or can be estimated by a maximum-likelihood fit. The sPlot technique [1] allows us to recover the distribution of target variables for signal events, given that target variables are statistically independent of discriminative ones. sPlot achieves that by assigning weights (sWeights) to all events, e.g. histogram of the target variable is obtained as the sum of these weights within each bin:",
            "paragraph_rank": 1,
            "section_rank": 2,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b0",
                    "start": 518,
                    "text": "[1]",
                    "end": 521
                }
            ]
        },
        {
            "section": "Introduction",
            "text": "where x i are samples of the target variables, w i are weights assigned by sPlot, Q denotes an arbitrary set and S a signal event. Figure 1 provides an illustration of sPlot technique. Derivation and detailed analysis of the sPlot are given in [1]. A notable property of the sPlot technique is that these weights can be negative. Typically, This does not create problems for low-dimensional analysis with simple models. Nonetheless, one can encounter difficulties fitting large-capacity models, e.g. deep neural networks. The prior work [2] describes such difficulties for the problem of signal/background separation and introduces two approaches for dealing with them. In this work, we expand the previous results to the problem of classification between two signal sources, samples of each are contaminated by different sources of background events.  The rest of the paper is structured as follows. In the next section, we briefly outline prior results from [2]. Section 3 introduces methods for signal-signal classification. Experimental results are presented in section 4.",
            "paragraph_rank": 2,
            "section_rank": 2,
            "ref_spans": [
                {
                    "type": "figure",
                    "ref_id": "fig_1",
                    "start": 131,
                    "text": "Figure 1",
                    "end": 139
                },
                {
                    "type": "bibr",
                    "ref_id": "b0",
                    "start": 244,
                    "text": "[1]",
                    "end": 247
                },
                {
                    "type": "bibr",
                    "ref_id": "b1",
                    "start": 537,
                    "text": "[2]",
                    "end": 540
                },
                {
                    "type": "bibr",
                    "ref_id": "b1",
                    "start": 960,
                    "text": "[2]",
                    "end": 963
                }
            ]
        },
        {
            "text": "Background",
            "section_rank": 3
        },
        {
            "section": "Background",
            "text": "Consider the problem of training a classifier f (x) to separate background (denoted as B) from signal (denoted as S) events given unlabeled sample",
            "paragraph_rank": 3,
            "section_rank": 3
        },
        {
            "section": "Background",
            "text": ", where x denote target variables, m -discriminative variables. We assume that x and m are statistically independent:",
            "paragraph_rank": 4,
            "section_rank": 3
        },
        {
            "section": "Background",
            "text": "and p(m | S) and p(m | B) are known in advance. One can use a cross-entropy loss to train a classifier:",
            "paragraph_rank": 5,
            "section_rank": 3
        },
        {
            "section": "Background",
            "text": "where p(x, m), p(x) denote probability density functions and w(m) the weights assigned by sPlot. The last equality is due to the property of sPlot:",
            "paragraph_rank": 6,
            "section_rank": 3
        },
        {
            "section": "Background",
            "text": "Proof of this statement can be found in [2]. Loss (4) can be approximated with a finite sample as follows:",
            "paragraph_rank": 7,
            "section_rank": 3,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b1",
                    "start": 40,
                    "text": "[2]",
                    "end": 43
                }
            ]
        },
        {
            "section": "Background",
            "text": "This approach is quite popular in the high energy physics community [2]. One might notice, however, that some of the summands in (6) might not have a lower bound in case of negative w i . Therefore, if classifier f (x) can isolate a point x i with a negative corresponding weight w i from the rest of the sample, loss (6) can be made arbitrarily low, which leads to quick overfitting. This is often possible for complex models, e.g. for large-capacity neural networks.",
            "paragraph_rank": 8,
            "section_rank": 3,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b1",
                    "start": 68,
                    "text": "[2]",
                    "end": 71
                }
            ]
        },
        {
            "section": "Background",
            "text": "Two options are introduced in [2] for avoiding this problem. The first approach, instead of cross-entropy loss, suggests using mean square regression directly on weights, taking advantage of the property 5:",
            "paragraph_rank": 9,
            "section_rank": 3,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b1",
                    "start": 30,
                    "text": "[2]",
                    "end": 33
                },
                {
                    "ref_id": "formula_4",
                    "start": 204,
                    "text": "5",
                    "end": 205
                }
            ]
        },
        {
            "section": "Background",
            "text": "and restricting values of f (x) to [0, 1], e.g. by representing it as \u03c3(g(x)) where \u03c3(\u2022) is the sigmoid function, g(x) is an unbounded regression model. Another approach is to use the maximum likelihood principle and avoid sPlot altogether and use the known probabilities P (S | m) directly:",
            "paragraph_rank": 10,
            "section_rank": 3
        },
        {
            "section": "Background",
            "text": "A detailed analysis of these loss functions and formal proofs can be found in [2].",
            "paragraph_rank": 11,
            "section_rank": 3,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b1",
                    "start": 78,
                    "text": "[2]",
                    "end": 81
                }
            ]
        },
        {
            "text": "Our approaches",
            "section_rank": 4
        },
        {
            "section": "Our approaches",
            "text": "In this work we extend approaches discussed above to the case of classification between two classes (C 1 and C 2 ), both of them represented as mixtures of signal (denoted as S 1 and S 2 ) and background (denoted as B 1 and B 2 ) events:",
            "paragraph_rank": 12,
            "section_rank": 4
        },
        {
            "section": "Our approaches",
            "text": ", where x i are samples of target variables, m i samples of discriminative variables, and y i is binary class indicator.",
            "paragraph_rank": 13,
            "section_rank": 4
        },
        {
            "section": "Our approaches",
            "text": "Consider cross-entropy loss between S 1 and S 2 :",
            "paragraph_rank": 14,
            "section_rank": 4
        },
        {
            "section": "Our approaches",
            "text": "where",
            "paragraph_rank": 15,
            "section_rank": 4
        },
        {
            "section": "Our approaches",
            "text": "where:",
            "paragraph_rank": 16,
            "section_rank": 4
        },
        {
            "section": "Our approaches",
            "text": ". Therefore, loss (10) can be approximated by:",
            "paragraph_rank": 17,
            "section_rank": 4
        },
        {
            "section": "Our approaches",
            "text": "where c 1 and c 2 are estimates of P (S 1 | C 1 ) and P (S 2 | C 2 ) correspondingly, p 1,i and p 2,i are estimates of P (S 1 | x, C 1 ) and P (S 2 | x, C 2 ) obtained by e.g. exact likelihood approach from [2]. Alternatively, with the help of sPlot weights, loss (9) can be expressed through binary indicator y:",
            "paragraph_rank": 18,
            "section_rank": 4,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b1",
                    "start": 207,
                    "text": "[2]",
                    "end": 210
                }
            ]
        },
        {
            "section": "Our approaches",
            "text": "where w 1 (m) and w 2 (m) denote weights assigned by sPlot for classes C 1 and C 2 correspondingly.",
            "paragraph_rank": 19,
            "section_rank": 4
        },
        {
            "section": "Our approaches",
            "text": "Here we also encounter potentially negative weights and all the problems associated with them as discussed in section 2. Applying weights averaging as in [2], one can avoid negative weights:",
            "paragraph_rank": 20,
            "section_rank": 4,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b1",
                    "start": 154,
                    "text": "[2]",
                    "end": 157
                }
            ]
        },
        {
            "section": "Our approaches",
            "text": "where p 1 (x), p 2 (x) can be estimated by a regression trained on the corresponding weights with the onstrained MSE loss (7).",
            "paragraph_rank": 21,
            "section_rank": 4
        },
        {
            "text": "Experimental evaluation",
            "section_rank": 5
        },
        {
            "section": "Experimental evaluation",
            "text": "We evaluate the proposed methods on data collected from the LHCb Muon subsystem [3] (MuID). To our knowledge, this is the only open dataset [4] that has sWeights. The dataset contains different particle species obtained from different calibration decays [5] and is labeled. Each species has its own background that is subtracted with a separate application of the sPlot method. For simplicity, we use only pion and muon species and ignore the kinematic weights. We use 60 scalar features. We split the dataset into train and test parts containing 2 \u2022 10 6 and 6 \u2022 10 5 examples respectively. For each train dataset size, the classifier was fitted 10 times to estimate the standard deviation of the scores.  sWeights -using sWeights directly as weights for logloss; Constrained MSE -our regression (7) used to transform the sWeights, the result used as weights for classification; Ignoring weighttraining ignoring the sWeights.",
            "paragraph_rank": 22,
            "section_rank": 5,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b2",
                    "start": 80,
                    "text": "[3]",
                    "end": 83
                },
                {
                    "type": "bibr",
                    "ref_id": "b3",
                    "start": 140,
                    "text": "[4]",
                    "end": 143
                },
                {
                    "type": "bibr",
                    "ref_id": "b4",
                    "start": 254,
                    "text": "[5]",
                    "end": 257
                }
            ]
        },
        {
            "section": "Experimental evaluation",
            "text": "The results are on the figure 2. The models used are described in Appendix A. Direct application of sWeights statistically significantly loses to our approach for some training sizes, performing about the same for the rest. The gap decreases with the training set size increase.",
            "paragraph_rank": 23,
            "section_rank": 5
        },
        {
            "section": "Experimental evaluation",
            "text": "However, ignoring the sWeights during training also yields good results, so it seems that the impact of sWeights on the problem is limited. This suggests, that the distribution of pionic background has good separation from the distribution of muon signal and vice versa. In this case, a classifier that is optimal for separating the muon signal/background mixture from the pion signal/background mixture is also almost optimal for the separation of muon signal from the pion signal. This limits the utility of the dataset with the respect to evaluating the proposed methods.",
            "paragraph_rank": 24,
            "section_rank": 5
        },
        {
            "section": "Experimental evaluation",
            "text": "To address this problem, we do an additional test with synthetically constructed sWeights, that are tailored to impact muon/pion classification. There are many equally attractive ways to introduce such sWeights. We go with the most obvious one: mark a sample of particles that are muons with a high level of confidence as pion background. First, we train a CatBoost classifier to separate signal and background and a classifier to separate muons and pions as described in section 2. We select 30% of examples with muon label with the highest scores and marked them as pion background. Next, we use the signal probabilities obtained from the first step and assign definite signal and background labels by cutting on its output. We generate \"pseudomass\" for both muon and pion background, each from a different distribution, which we use to compute the new sWeights.",
            "paragraph_rank": 25,
            "section_rank": 5
        },
        {
            "section": "Experimental evaluation",
            "text": "As the result, we increase the impact of the sWeights on the classification problem to the point, where ignoring the sWeights leads to a significant drop in performance, while both our method and the baseline (using sWeights as example weights) are affected equally to permit for a fair comparison. The results are present in figure 3. Surprisingly, for CatBoost with train size equal to 2 \u2022 10 3 , using sWeights as example weights has the best performance. In all other cases, our methods perform better than using sWeights as example weights during classifier training.    Figure 3: Experimental evaluation of our loss functions on the MuID dataset with artificial sWeights. sWeights -using sWeights directly as weights for logloss; Constrained MSE -our regression (7) used to transform the sWeights, the result used as weights for classification; Likelihood -our likelihood (8) for the transformation; Ignoring weight -training ignoring the sWeights.",
            "paragraph_rank": 26,
            "section_rank": 5,
            "ref_spans": [
                {
                    "type": "figure",
                    "start": 326,
                    "text": "figure 3",
                    "end": 334
                },
                {
                    "type": "figure",
                    "start": 576,
                    "text": "Figure 3",
                    "end": 584
                }
            ]
        },
        {
            "text": "Conclusion",
            "section_rank": 6
        },
        {
            "section": "Conclusion",
            "text": "Training a machine learning algorithm on mixtures of signal and background events in the absence of exact labels is a challenging task. With discriminative variables available, the sPlot technique becomes a tempting solution. However, it might potentially lead to loss function with no lower bound, which in turn might result in catastrophic overfitting.",
            "paragraph_rank": 27,
            "section_rank": 6
        },
        {
            "section": "Conclusion",
            "text": "In this work, we consider the problem of separating two classes, each contaminated by background events. Building upon the prior work on signal-background separation, we introduce two loss functions that guarantee the stability of training by avoiding negative weights. Experimental evaluation shows improved classification quality compared to using sWeights directly as example weights, both for neural network and gradient boosting model",
            "paragraph_rank": 28,
            "section_rank": 6
        },
        {
            "text": "Figure 1 :",
            "section_rank": 7
        },
        {
            "section": "Figure 1 :",
            "text": "Figure 1: Demonstration of the sPlot technique. The known distributions of a discriminative variable m (left) is used to weight the distribution of a target variable x for mixture of signal and background events (right). Adapted from [2].",
            "paragraph_rank": 29,
            "section_rank": 7
        },
        {
            "text": "Figure 2 :",
            "section_rank": 8
        },
        {
            "section": "Figure 2 :",
            "text": "Figure 2: Experimental evaluation of CatBoost with the presented loss on the MuID dataset. sWeights -using sWeights directly as weights for logloss; Constrained MSE -our regression (7) used to transform the sWeights, the result used as weights for classification; Ignoring weighttraining ignoring the sWeights.",
            "paragraph_rank": 30,
            "section_rank": 8
        },
        {
            "text": "Acknowledgments",
            "section_rank": 10
        },
        {
            "section": "Acknowledgments",
            "text": "The research leading to these results has received funding from Russian Science Foundation under grant agreement n 17-72-20127.",
            "paragraph_rank": 31,
            "section_rank": 10
        },
        {
            "section": "Acknowledgments",
            "text": "We are grateful to the LHCb collaboration for opening their data; Artem Maevskiy for the weighted ROC AUC code.",
            "paragraph_rank": 32,
            "section_rank": 10
        },
        {
            "text": "Appendix A. Models parameters",
            "section_rank": 12
        },
        {
            "section": "Appendix A. Models parameters",
            "text": "\u2022 Fully-connected neural networks (NN): 4 layers, 512, 256, 128 neurons in the first three layers and either 1 or 2 neurons in the last one, leaky ReLu (0.05), optimized by adam [6] algorithm with learning rate 2 \u2022 10 \u22124 , \u03b2 1 = 0.9, \u03b2 2 = 0.999, trained for 32 epochs; regressors and classifiers use the same architecture; \u2022 CatBoost: 500 trees, leaf estimation method=\"Gradient\", version 0.10.2 with our losses added and check for negative weights removed: https://github.com/kazeevn/catboost/ tree/constrained_regression",
            "paragraph_rank": 33,
            "section_rank": 12,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b5",
                    "start": 178,
                    "text": "[6]",
                    "end": 181
                }
            ]
        }
    ]
}