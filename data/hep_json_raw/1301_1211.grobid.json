{
    "level": "paragraph",
    "abstract": [
        {
            "text": "High-energy physics data analysis relies heavily on the comparison between experimental and simulated data as stressed lately by the Higgs search at LHC and the recent identification of a Higgs-like new boson. The first link in the full simulation chain is the event generation both for background and for expected signals. Nowadays event generators are based on the automatic computation of matrix element or amplitude for each process of interest.",
            "paragraph_rank": 1,
            "section_rank": 1
        },
        {
            "text": "Moreover, recent analysis techniques based on the matrix element likelihood method assign probabilities for every event to belong to any of a given set of possible processes. This method originally used for the top mass measurement, although computing intensive, has shown its efficiency at LHC to extract the new boson signal from the background.",
            "paragraph_rank": 2,
            "section_rank": 1
        },
        {
            "text": "Perturbative and not perturbative? that is the \u2026 calculation.",
            "section_rank": 2
        },
        {
            "section": "Perturbative and not perturbative? that is the \u2026 calculation.",
            "text": "Let us first replace this computational activity in the general framework of particle physics and its Standard Model. There are basically two domains of calculation depending on the actual value of the",
            "paragraph_rank": 3,
            "section_rank": 2
        }
    ],
    "body_text": [
        {
            "text": "Serving both needs, the automatic calculation of matrix element is therefore more than ever of prime importance for particle physics. Initiated in the 80's, the techniques have matured for the lowest order calculations (tree-level), but become complex and CPU time consuming when higher order calculations involving loop diagrams are necessary like for QCD processes at LHC. New calculation techniques for next-to-leading order (NLO) have surfaced making possible the generation of processes with many final state particles (up to 6). If NLO calculations are in many cases under control, although not yet fully automatic, even higher precision calculations involving processes at 2loops or more remain a big challenge.",
            "paragraph_rank": 4,
            "section_rank": 3
        },
        {
            "text": "After a short introduction to particle physics and to the related theoretical framework, we will review some of the computing techniques that have been developed to make these calculations automatic. The main available packages and some of the most important applications for simulation and data analysis, in particular at LHC will also be summarized (see CCP2012 slides [1]).",
            "paragraph_rank": 5,
            "section_rank": 3,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b0",
                    "start": 371,
                    "text": "[1]",
                    "end": 374
                }
            ]
        },
        {
            "text": "Particle Physics goals and means",
            "section_rank": 4
        },
        {
            "section": "Particle Physics goals and means",
            "text": "Particle physics targets the study of the ultimate basic elements of matter and of the fundamental forces generated by or acting on them; the smaller the element is, the smaller the probe wavelength must be and, therefore, the larger the interaction energy. Actually, particle physics or more precisely its high-energy physics branch tends to reproduce on earth the range of energies that was prevalent when matter did start to form, at a very early time of the Universe history ( fig. 1), namely some 10 -10 s after the Big Bang. There is a profound duality between the today state of matter probed by high-energy colliders and the state of the Universe shortly after its creation, 13.7 billion years ago.",
            "paragraph_rank": 6,
            "section_rank": 4,
            "ref_spans": [
                {
                    "type": "figure",
                    "ref_id": "fig_0",
                    "start": 481,
                    "text": "fig. 1",
                    "end": 487
                }
            ]
        },
        {
            "section": "Particle Physics goals and means",
            "text": "The highest man-made particle colliding energy reaches, today, 8 TeV (Tera Electron Volt ~ thousand times the proton mass) on the way to the nominal 14 TeV expected at the large hadron collider (LHC [2]) at CERN [3], Geneva (Switzerland). This energy, however, remains many orders of magnitude smaller than the typical interaction energy occurring in the Universe when quarks, gluons and electrons were moving in a confinement free \"hot soup\". From this even earlier Universe energy we only have a glimpse, today, through the very high energy cosmic rays (more than 10 8 TeV) hitting the upper earth atmosphere. Producing huge particle showers they are detected by large area detectors on earth surface like at the 3000 km 2 Pierre Auger Observatory [4] or observed by satellite surveying the earth as proposed by the GEM-Euso Collaboration [5]. Located at CERN, the LHC (Fig. 2) is the latest built proton-proton collider. It is housed in a 27 km circular, 100 m deep underground tunnel in the Geneva region. The LHC benefits from a large accelerator infrastructure progressively installed at CERN since it was founded back in the 50's. The proton bunches produced from a hydrogen source are accelerated by the Linac and Booster. Then, the PS and SPS increase the energy up to 450 GeV before entering the LHC. Four experiments are located in the LHC ring: two general purpose ATLAS [6] and CMS [7] and two dedicated LHCb [8] for the meson B physics and ALICE [9] for hadronic physics when protons are replaced by heavy ions (e.g. lead-lead collision) in the LHC.",
            "paragraph_rank": 7,
            "section_rank": 4,
            "ref_spans": [
                {
                    "type": "bibr",
                    "start": 207,
                    "text": "CERN [3]",
                    "end": 215
                },
                {
                    "type": "bibr",
                    "start": 566,
                    "text": "10 8",
                    "end": 570
                },
                {
                    "type": "bibr",
                    "ref_id": "b1",
                    "start": 750,
                    "text": "[4]",
                    "end": 753
                },
                {
                    "type": "bibr",
                    "ref_id": "b2",
                    "start": 841,
                    "text": "[5]",
                    "end": 844
                },
                {
                    "type": "figure",
                    "start": 871,
                    "text": "(Fig. 2)",
                    "end": 879
                },
                {
                    "type": "bibr",
                    "ref_id": "b3",
                    "start": 1422,
                    "text": "[8]",
                    "end": 1425
                }
            ]
        },
        {
            "text": "Figure 2: LHC and experiments at CERN",
            "section_rank": 5
        },
        {
            "section": "Figure 2: LHC and experiments at CERN",
            "text": "The particle physics overall picture is now embedded in a very solid theory that, shyly enough, we keep calling a Model. The Standard Model (SM) is maybe one of the most developed and rugged theory of any physics fields left alone other research domains. This is probably due to the fact that dealing with fundamental processes, generally, does not lead to the complexity of n-body problems generally intractable, although supercomputers are opening new hopes, at least for small n.",
            "paragraph_rank": 8,
            "section_rank": 5
        },
        {
            "section": "Figure 2: LHC and experiments at CERN",
            "text": "But that does not mean that the theory is simple when it comes to numerical evaluations. Fortunately methods have been developed based on the underlying quantum field theory to make precise computations. Until now, the experimental data have always either confirmed the theoretical predictions or brought new insight reinforcing the general view.",
            "paragraph_rank": 9,
            "section_rank": 5
        },
        {
            "section": "Figure 2: LHC and experiments at CERN",
            "text": "This success story seems to continue with the recent discovery of a \"predicted\" boson at the CERN LHC, last July (ATLAS [10], CMS [11]). Nobody yet dares calling it \"Higgs\", the last missing ingredient of the SM at current scales. We will soon learn from the experiments whether or not this new boson is the actual SM Higgs proposed as a consequence of a mechanism giving mass to elementary particles [12] [13] [14] [15].",
            "paragraph_rank": 10,
            "section_rank": 5,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b4",
                    "start": 120,
                    "text": "[10]",
                    "end": 124
                },
                {
                    "type": "bibr",
                    "ref_id": "b5",
                    "start": 130,
                    "text": "[11]",
                    "end": 134
                },
                {
                    "type": "bibr",
                    "ref_id": "b6",
                    "start": 401,
                    "text": "[12]",
                    "end": 405
                },
                {
                    "type": "bibr",
                    "ref_id": "b9",
                    "start": 416,
                    "text": "[15]",
                    "end": 420
                }
            ]
        },
        {
            "section": "Figure 2: LHC and experiments at CERN",
            "text": "This discovery is materialized by the small bump popping out of the recombined \u03b3\u03b3 mass histograms ( fig.3) for both ATLAS [10] and CMS [11]. These simple plots summarize the extensive computational undertaking behind high-energy physics data analysis. For example, in the CMS experiment, the dots and the error bars represent the observed number of events in a given mass bin. The numbers come from a long chain of data analysis beginning with the trigger event selection, followed by the event reconstruction, the event filtering and analysis for the millions of events recorded by the experiments.",
            "paragraph_rank": 11,
            "section_rank": 5,
            "ref_spans": [
                {
                    "type": "figure",
                    "start": 100,
                    "text": "fig.3",
                    "end": 105
                },
                {
                    "type": "bibr",
                    "ref_id": "b4",
                    "start": 122,
                    "text": "[10]",
                    "end": 126
                },
                {
                    "type": "bibr",
                    "ref_id": "b5",
                    "start": 135,
                    "text": "[11]",
                    "end": 139
                }
            ]
        },
        {
            "section": "Figure 2: LHC and experiments at CERN",
            "text": "The yellow/green bands represent the best estimation of the so-called background events. They are based on a fit of the data points away from the bump. This fit is also compared to simulated events belonging to background processes. In red, the sum of background and simulated Higgs signal is shown. It is a clear demonstration of the interplay between experimental and simulated data.",
            "paragraph_rank": 12,
            "section_rank": 5
        },
        {
            "text": "Figure 3: Higgs-like boson signal from CMS and ATLAS (CERN)",
            "section_rank": 6
        },
        {
            "section": "Figure 3: Higgs-like boson signal from CMS and ATLAS (CERN)",
            "text": "Most of the data simulation and analysis is performed on a distributed data grid system called the WLCG (Worldwide LHC Computing Grid) [16]. This infrastructure based on tiers computing centres has been installed all around the world to absorb the heavy computing load of the high LHC statistics. The tier 0 is located at the data production centre (CERN) and absorbs 20% of the load, 11 tiers 1 connected through direct high bandwidth fibre links (10 Gb/s) are scattered across the main participating countries/regions and ~ 140 tiers 2 are providing the specific computing support necessary to research teams. The WLCG provides more than 250,000 processors, and close to 150 PB of disk storage from over 150 sites in 34 countries, producing a massive distributed computing infrastructure that support the needs of more than 8,000 physicists.",
            "paragraph_rank": 13,
            "section_rank": 6,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b10",
                    "start": 135,
                    "text": "[16]",
                    "end": 139
                }
            ]
        },
        {
            "section": "Figure 3: Higgs-like boson signal from CMS and ATLAS (CERN)",
            "text": "In sect. 2, we will describe the general theoretical framework. Sect. 3 is a short presentation of the perturbative calculus and of the recent higher precision calculation breakthroughs. Then, in the following sections, like Russian dolls, we will go deeper in the experimental simulation chain (sect. 4): with the description the event formation phases (sect. 5), of the various steps leading to the building of event generator (sect. 6), including the automatic calculation of matrix elements (sect. 7) and their multi-dimensional integration giving the cross-sections (sect. 8). Sect. 9 describes the more recent usage of the same matrix element expressions for a more complete event analysis. Finally, sect. 10 lists some of the computational techniques that are used and shared with other research fields. QCD 1 coupling constant \u03b1 s . This constant is actually \"running\" from small values close to zero in the \"asymptotic freedom\" regime, namely at high energy when probing the deep inside of hadrons, to large values (\u03b1 s ~ 1) in the \"confinement\" region where the partons (quarks and gluons) are kept inside the hadrons.",
            "paragraph_rank": 14,
            "section_rank": 6
        },
        {
            "section": "Figure 3: Higgs-like boson signal from CMS and ATLAS (CERN)",
            "text": "If \u03b1 s is small when, for example, 2 partons, almost free inside the protons, interact, the solutions of the Schr\u00f6dinger equation which represents the time evolution of the quantum state can be approximated by a series expansion in power of \u03b1 s . This is the so-called perturbative domain. Each term of the series corresponds to a level of precision, the higher the number of terms the more precise the calculation. We will see later also that each term of the series can be graphically represented by a set of \"Feynman\" diagrams [17], all having similar characteristics. This domain describes the dynamics of QCD interactions. This is the branch of computational particle physics that will be discussed in this paper.",
            "paragraph_rank": 15,
            "section_rank": 6,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b11",
                    "start": 530,
                    "text": "[17]",
                    "end": 534
                }
            ]
        },
        {
            "section": "Figure 3: Higgs-like boson signal from CMS and ATLAS (CERN)",
            "text": "Conversely, if \u03b1 s is large ~ 1, the hadron quantum system is probed at the confinement limit. Quarks and gluons are all strongly coupled and cannot escape the hadron boundary. Here the interest is more focused on the hadron as a whole. Global values like hadron masses are computed. The perturbative approximation does not apply though and the Schr\u00f6dinger equation must be solved exactly. However, this has proved to be analytically unfeasible despite many years of trials until K.G. Wilson [18] proposed to replace the continuum phase space by a discrete approximation, the size of the lattice introducing a cut-off stabilizing the solutions. It is only these recent years that this approach became successful with the tremendous increase in computational power of the super-computers and new breakthrough in the basic algorithms. This technique is known as the QCD calculus on lattice or lattice QCD. Please refer to the excellent status report by Karl Jansen in these proceedings.",
            "paragraph_rank": 16,
            "section_rank": 6,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b12",
                    "start": 492,
                    "text": "[18]",
                    "end": 496
                }
            ]
        },
        {
            "text": "From tree level to higher orders: the NLO Revolution",
            "section_rank": 7
        },
        {
            "section": "From tree level to higher orders: the NLO Revolution",
            "text": "Perturbative field theory allows level by level calculation of particle interactions. The technique was developed by R.P. Feynman [19] in the 50's. Each precision level can be matched to a set of similar diagrams. Let's take a simple example, in QED 2 ( fig. 4), at the lowest level or tree level in the coupling constant \u03b1, the simple + \u2212 \u2192 + \u2212 process involves only 2 diagrams: a s-channel graph when the electron and the positron annihilate to form a virtual photon, also called propagator, which then decay into an electron and a positron and a t-channel where a photon is exchanged between the electron and the positron. In QED, the coupling constant is small enough (~ 1/137, although also running) so that in most cases first order calculations provide good enough precisions matching experimental accuracies. The input and output particles are called \"legs\". But in some cases, experimental errors are much smaller than tree level computations like at the precision + \u2212 colliders. They can even be orders of magnitude smaller when, for example measuring the muon anomalous magnetic moment factor ( \u2212 2)/2 at 0.14 ppm precision [20]. The theoretical precision should be, at least, as good and computing the next terms in the expansion series becomes necessary both for electroweak and hadronic corrections.",
            "paragraph_rank": 17,
            "section_rank": 7,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b13",
                    "start": 130,
                    "text": "[19]",
                    "end": 134
                },
                {
                    "type": "figure",
                    "start": 254,
                    "text": "fig. 4)",
                    "end": 261
                },
                {
                    "type": "bibr",
                    "ref_id": "b14",
                    "start": 1135,
                    "text": "[20]",
                    "end": 1139
                }
            ]
        },
        {
            "section": "From tree level to higher orders: the NLO Revolution",
            "text": "The next level in precision is the so-called \"Next-to-Leading Order\" (NLO). It essentially implies adding 1-loop corrections. A photon can be exchanged between the initial state electron and positron, an + \u2212 loop can be inserted in the propagator or even (not shown), a second photon is exchanged in the t-channel (creating box diagrams). But the virtual exchanged particle can also be real and diagrams with an external photon must be included. If one sticks precisely to the same final state, the additional photon should not be observed namely, it should be either soft or almost collinear with the emitted electron. Loops and/or legs can be put on all elementary parts of the tree-level diagrams and the number of diagrams increases dramatically. Next-to-Next-Orders calculations follow the same trend, adding more and more diagrams and therefore becoming more and more computational intensive.",
            "paragraph_rank": 18,
            "section_rank": 7
        },
        {
            "section": "From tree level to higher orders: the NLO Revolution",
            "text": "At LHC, QCD plays a major role and things get more complicated. First the proton is not an elementary particle, but a complex system of 3 valence quarks in a gluon fog and a sea of quark-antiquark pairs. At LHC energy, the interest is on the hard scattering of proton constituents, namely gluon-gluon, quark-gluon or quark-quark. Therefore many different initial states must be taken into account. The selection of the initial partons within the proton is made through the use of the so-called parton distribution functions (PDF) which describe the parton content of the proton. These functions are mainly parameterizations based on data from the e-p experiments at HERA (DESY [21]) or in \u0305 experiments at D0/CDF (Tevatron [22], Fermilab) and now at LHC.",
            "paragraph_rank": 19,
            "section_rank": 7,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b15",
                    "start": 677,
                    "text": "[21]",
                    "end": 681
                },
                {
                    "type": "bibr",
                    "ref_id": "b16",
                    "start": 723,
                    "text": "[22]",
                    "end": 727
                }
            ]
        },
        {
            "section": "From tree level to higher orders: the NLO Revolution",
            "text": "Second, the final states may involve many particles, i.e. many legs, like shown in the experimental wish list discussed at the \"Les Houches\" meetings [23]. This is a huge step in complexity compared to the 2\u21924 channels studied at the electron-positron collider (LEP II [24]).",
            "paragraph_rank": 20,
            "section_rank": 7
        },
        {
            "text": "Figure 4: Loops and legs in QED",
            "section_rank": 8
        },
        {
            "section": "Figure 4: Loops and legs in QED",
            "text": "Finally, contrary to QED where tree-level calculations are often a good approximation, QCD estimations must go beyond LO which depend too much on arbitrary scales. The renormalization scale enters in the cross-section as well as in the value of . Renormalization is a necessary operation to cancel infinities arising in the calculation of the cross-section due to UV (Ultra-Violet) divergences. The factorization scale which also enters in the cross-section as well as in the parton distribution function is related to the soft and collinear divergences, namely in the IR (Infra-Red) region. These scales are arbitrary and induce large normalization errors (10-20% or even more). Higher order calculations reduce largely the sensitivity to these arbitrary scales and this trend is even more pronounced when the number of legs is large.",
            "paragraph_rank": 21,
            "section_rank": 8
        },
        {
            "section": "Figure 4: Loops and legs in QED",
            "text": "QCD NLO calculation of many legs processes may lead to more than 10 5-6 diagrams and for each, the amplitude or matrix element can be so large in the usual Feynman representation that computer memory sizes become an issue. Clearly the Feynman diagrammatic approach was not quite suited for this kind of calculation.",
            "paragraph_rank": 22,
            "section_rank": 8
        },
        {
            "section": "Figure 4: Loops and legs in QED",
            "text": "Although this conventional method is still in use and even being improved, new approaches originated from the string/twistor theory have recently been developed for the calculation of one-loop processes. The on-shell unitarity method [25] to the expense of rewriting the quantum field theory in terms of invariant gauge on-shell intermediate expression simply proposes to cut the loop diagrams in one or many places decomposing the initial one-loop calculation to a set of tree level amplitudes, easily handled by the conventional tree-level diagram calculators. Following these ideas many developments have been undertaken (see for a summary until 2008 [26]) and have led to automatic calculation of 2\u21924 or 2\u21925 NLO processes ( fig.5).",
            "paragraph_rank": 23,
            "section_rank": 8,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b18",
                    "start": 234,
                    "text": "[25]",
                    "end": 238
                },
                {
                    "type": "bibr",
                    "ref_id": "b19",
                    "start": 654,
                    "text": "[26]",
                    "end": 658
                },
                {
                    "type": "figure",
                    "ref_id": "fig_1",
                    "start": 728,
                    "text": "fig.5",
                    "end": 733
                }
            ]
        },
        {
            "section": "Figure 4: Loops and legs in QED",
            "text": "Even if these techniques for the most complex calculations have not reached full automation, they have made the computation possible with the current computers. Event generators at NLO based on SHERPA [27] and BlackHat [28], MadGraph [29] and others have been heavily used by the LHC experiments. ",
            "paragraph_rank": 24,
            "section_rank": 8,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b20",
                    "start": 201,
                    "text": "[27]",
                    "end": 205
                },
                {
                    "type": "bibr",
                    "ref_id": "b21",
                    "start": 219,
                    "text": "[28]",
                    "end": 223
                },
                {
                    "type": "bibr",
                    "ref_id": "b22",
                    "start": 234,
                    "text": "[29]",
                    "end": 238
                }
            ]
        },
        {
            "text": "Event Simulation and data analysis",
            "section_rank": 9
        },
        {
            "section": "Event Simulation and data analysis",
            "text": "It is impossible to calculate analytically the detector responses to an event produced in the quite complex LHC experiments. One therefore relies on Monte-Carlo simulations. Any process of interest can be actually generated and simulated in a virtual representation of the detectors. The simulation is supposed to mimic in detail what is detected when beams collide. Fig. 6 presents schematically the various steps of the simulation chain. On the top, the \"physics model\" describes the framework in which the calculation is performed. The basic model is the Standard Model (SM). But extension or beyond SM models are also implemented like for example the supersymmetric (SUSY) model. This model suggests that any known fermion (boson) has a partner boson (fermion) yet unobserved due to its high mass. Clearly beyond standard model (BSM) theories are made up to cure problems arising in the SM beyond current energy, in particular, the divergent contributions to the Higgs mass.",
            "paragraph_rank": 25,
            "section_rank": 9,
            "ref_spans": [
                {
                    "type": "figure",
                    "start": 367,
                    "text": "Fig. 6",
                    "end": 373
                }
            ]
        },
        {
            "section": "Event Simulation and data analysis",
            "text": "The actual process is, then, selected by defining the input and output particles as well as the order of calculation. The matrix element or the expression of the process probability is then automatically prepared based on the physics model data.",
            "paragraph_rank": 26,
            "section_rank": 9
        },
        {
            "section": "Event Simulation and data analysis",
            "text": "After integrating of the matrix element over the parameters phase space, the code for an event generator is constructed. It will generate random samples of energy-momentum four-vectors for all final state physical particles.",
            "paragraph_rank": 27,
            "section_rank": 9
        },
        {
            "section": "Event Simulation and data analysis",
            "text": "Each generated particle is then propagated into a model representing the experiment built by a detector simulation package. GEANT [31], initiated at CERN and developed by an international collaboration is the main package used by our community. Version 4 is an extremely detailed description of the detector components down to nuts and bolts and read-out cables. All physics particle-matter interactions have been implemented. Each particle, step by step is tracked in the material forming the structures as well as the detection sensitive elements. The charged particles may produce primary gas ionization, scintillation or Cherenkov light depending on the materials. Photons/gammas create electromagnetic showers producing hundred to thousand low energy particles which are all tracked down to final absorption. This information is collected by sensitive elements: wires, silicon strips/pixels or photo-detectors. All particles are followed until stopping in material or escaping the experimental setup. The huge amount of information produced by the detector simulation should in principle represent exactly what a real event in a real detector would produce. Obviously, confidence on the detector simulation accuracy has been obtained after thousands of validation measurements for each type of particles, materials and detectors.",
            "paragraph_rank": 28,
            "section_rank": 9,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b24",
                    "start": 130,
                    "text": "[31]",
                    "end": 134
                }
            ]
        },
        {
            "text": "Figure 6: Monte-Carlo simulation and the data analysis chain",
            "section_rank": 10
        },
        {
            "section": "Figure 6: Monte-Carlo simulation and the data analysis chain",
            "text": "At this level, one has now simulated i.e. theory \"generated\" event data that can be compared to the experimental data produced by Nature in the physical detector. Both simulation and real data are then reconstructed by the same reconstruction programme to get the initial set of energy-momentum fourvectors representing the actual simulated or observed event. Comparing the reconstructed simulated four-momenta to those produced by the event generator is a test of self-consistency of the whole system although departure between the simulated and the experimental data may give hints to new physics discovery.",
            "paragraph_rank": 29,
            "section_rank": 10
        },
        {
            "section": "Figure 6: Monte-Carlo simulation and the data analysis chain",
            "text": "The final analysis is generally done with the Root data analysis toolkit [32], developed at CERN. Events data are put in large ntuples (sort of expandable spread sheets) on which filters and fits can be applied. Estimators can then be computed and placed in histograms. This is how plots like those shown in fig.3 have been produced. Would the experimental data have been precisely fitted by the green-yellow background band, we would had concluded to a no Higgs discovery and higher limits on its production would had been set. But hopefully some excess was found, signalling the existence of something beyond the expected background, a new particle, maybe the expected Higgs.",
            "paragraph_rank": 30,
            "section_rank": 10,
            "ref_spans": [
                {
                    "type": "figure",
                    "start": 308,
                    "text": "fig.3",
                    "end": 313
                }
            ]
        },
        {
            "text": "Event sequential formation stages",
            "section_rank": 11
        },
        {
            "section": "Event sequential formation stages",
            "text": "Let's now focus on the physics of an event formation. In a high-energy scattering, the generation of a single event ( fig. 7) takes several steps. First, 2 partons (flavour and colour) are selected in each of the colliding protons. Before interacting, partons are allowed to emit gluons (initial state radiations). Then the actual hard-scattering interaction occurs and 2 or more particles are produced. These particles may then decay like in heavy quark/leptons or bosons (Z, W) producing several daughters. Finally, the last remaining quark and gluon undergo partonic showers before the final hadronization step occurs, namely the recombination of end partons in hadrons (baryons or mesons).",
            "paragraph_rank": 31,
            "section_rank": 11,
            "ref_spans": [
                {
                    "type": "figure",
                    "start": 118,
                    "text": "fig. 7",
                    "end": 124
                }
            ]
        },
        {
            "text": "Figure 7: Sequential steps in the formation of an event (The event presented here is a SUSY event with a hard process: u-quark gluon \u2192u-squark gluino, the u-squark being the superpartner of a u-quark and the gluino the gluon counterpart)",
            "section_rank": 12
        },
        {
            "section": "Figure 7: Sequential steps in the formation of an event (The event presented here is a SUSY event with a hard process: u-quark gluon \u2192u-squark gluino, the u-squark being the superpartner of a u-quark and the gluino the gluon counterpart)",
            "text": "In addition, several interactions between different partons can occur in the same proton collision leading to multiple parton interactions (MPI). Finally, in the collision of proton bunches more than one collision can occur and multiple vertices are created, this is called the underlying events (UE). The odds that several interactions producing a hard-scattering each leaving, in the detectors, high momentum transfer particles is quite small, but these extra collisions produce many additional tracks and energy depositions in the detector. These parasitic effects make the analysis more difficult as each track or energy deposition should be associated to the right vertex and, therefore, the whole reconstruction process takes more CPU resources and becomes more error prone.",
            "paragraph_rank": 32,
            "section_rank": 12
        },
        {
            "text": "Process calculation",
            "section_rank": 13
        },
        {
            "section": "Process calculation",
            "text": "Now that the physics of an event formation has been described, let us see how this is implemented in the global process calculation ( fig. 8). A flurry of packages has been developed to carry out the full event simulation.",
            "paragraph_rank": 33,
            "section_rank": 13,
            "ref_spans": [
                {
                    "type": "figure",
                    "ref_id": "fig_2",
                    "start": 134,
                    "text": "fig. 8)",
                    "end": 141
                }
            ]
        },
        {
            "section": "Process calculation",
            "text": "Based on the model Lagrangian or, more generally, on the group symmetries and fields definitions, symbolic manipulation languages like Reduce, Maple, Mathematica or Form [33] are used to encode in a model file the various couplings and the propagators expressions either on an analytic form or as a set of numerical libraries. The Form language has been developed to handle very large expressions buffered on disk files. Specific programs like LanHEP [34], FeynRules [35] or SARAH [36] (SUSY) have made this derivation automatic.",
            "paragraph_rank": 34,
            "section_rank": 13,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b25",
                    "start": 170,
                    "text": "[33]",
                    "end": 174
                },
                {
                    "type": "bibr",
                    "ref_id": "b26",
                    "start": 451,
                    "text": "[34]",
                    "end": 455
                },
                {
                    "type": "bibr",
                    "ref_id": "b27",
                    "start": 467,
                    "text": "[35]",
                    "end": 471
                },
                {
                    "type": "bibr",
                    "ref_id": "b28",
                    "start": 481,
                    "text": "[36]",
                    "end": 485
                }
            ]
        },
        {
            "section": "Process calculation",
            "text": "Then a specific process can be selected. The initial particles undergoing the hard collision are generated after initial state radiation and beamstrahlung (CIRCE [37]) for electrons or parton extraction from the parton distribution functions (PDF) (LHApdf [38], CTEQ [39], MRST [40], \u2026) for the protons has been applied. One of the automatic amplitude/matrix element generators can be put in action to produce the global expression of the process integrand. This expression must then be integrated over the full phase space, namely over all possible legs momenta/energy, only constrained by the energy momentum conservation and the initial conditions. If loops are involved, integration over the loop propagators must also be performed. The multidimensional integration provides the total cross-section and the probability function for the generation of the events.",
            "paragraph_rank": 35,
            "section_rank": 13,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b29",
                    "start": 162,
                    "text": "[37]",
                    "end": 166
                },
                {
                    "type": "bibr",
                    "ref_id": "b30",
                    "start": 256,
                    "text": "[38]",
                    "end": 260
                },
                {
                    "type": "bibr",
                    "ref_id": "b31",
                    "start": 267,
                    "text": "[39]",
                    "end": 271
                },
                {
                    "type": "bibr",
                    "ref_id": "b32",
                    "start": 278,
                    "text": "[40]",
                    "end": 282
                }
            ]
        },
        {
            "section": "Process calculation",
            "text": "The event generator can, now, be constructed. It will randomly generate events at the parton level. Parton shower and hadronization programs like Pythia [41] or Herwig [42] will take over to produce the final full-fledged event ready to enter the detector simulation stage.",
            "paragraph_rank": 36,
            "section_rank": 13,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b33",
                    "start": 153,
                    "text": "[41]",
                    "end": 157
                },
                {
                    "type": "bibr",
                    "ref_id": "b34",
                    "start": 168,
                    "text": "[42]",
                    "end": 172
                }
            ]
        },
        {
            "text": "Matrix element automatic calculations",
            "section_rank": 14
        },
        {
            "section": "Matrix element automatic calculations",
            "text": "The matrix element calculation it-self has been for many years performed manually by talented theorists. Back in the 80's, a group in Japan from the KEK laboratory (Grace) [43] and a Russian group from Moscow State University (CompHEP) [44]  [45] started to develop packages to do these lengthy and error prone calculations automatically, that was the beginning of the automatic process calculation endeavour.",
            "paragraph_rank": 37,
            "section_rank": 14,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b35",
                    "start": 172,
                    "text": "[43]",
                    "end": 176
                },
                {
                    "type": "bibr",
                    "ref_id": "b36",
                    "start": 236,
                    "text": "[44]",
                    "end": 240
                },
                {
                    "type": "bibr",
                    "ref_id": "b37",
                    "start": 242,
                    "text": "[45]",
                    "end": 246
                }
            ]
        },
        {
            "section": "Matrix element automatic calculations",
            "text": "The automation was initially seen as a toy project, at best to train students. But the experimental requests for new process calculations went exploding as well as the complexity of the process calculation and, finally, the automatic approach became the baseline. The first event generator with an automated computed matrix element was probably grc4f + \u2212 \u2192 4 fermions [46] which was used for the LEP2 analysis. For the first time, spin correlations and mass effects were included. Nowadays most calculations are made by computer tools in a combination of symbolic and numerical operations.",
            "paragraph_rank": 38,
            "section_rank": 14,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b38",
                    "start": 368,
                    "text": "[46]",
                    "end": 372
                }
            ]
        },
        {
            "section": "Matrix element automatic calculations",
            "text": "The various steps of the automatic calculation flowchart, at tree level can be summarized as follows:",
            "paragraph_rank": 39,
            "section_rank": 14
        },
        {
            "section": "Matrix element automatic calculations",
            "text": "1. Enumeration and description of the set of diagrams entering the calculation of a given process matrix element (e.g. standalone QGRAF [47] or included in the automatic packages e.g. [43], [44]) 2. Preparation of the matrix element expression for each diagram by various calls to the vertex and propagator subroutine library or by building a global analytical expression. 3. Multi-dimensional integration over the full phase space (including the PDF variables) to produce 1) the total cross-section 2) the probability function describing how likely an event with a given set of input and output four-vector will occur. 4. Preparation of the event generator code. According to the probability function, random set of the process four vectors will be generated.",
            "paragraph_rank": 40,
            "section_rank": 14,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b39",
                    "start": 136,
                    "text": "[47]",
                    "end": 140
                },
                {
                    "type": "bibr",
                    "ref_id": "b35",
                    "start": 184,
                    "text": "[43]",
                    "end": 188
                },
                {
                    "type": "bibr",
                    "ref_id": "b36",
                    "start": 190,
                    "text": "[44]",
                    "end": 194
                }
            ]
        },
        {
            "text": "Inclusion of the final state radiation calls as well as the parton showering and hadronization",
            "section_rank": 15
        },
        {
            "section": "Inclusion of the final state radiation calls as well as the parton showering and hadronization",
            "text": "steps based on Pythia [41] or Herwig [42].",
            "paragraph_rank": 41,
            "section_rank": 15,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b33",
                    "start": 22,
                    "text": "[41]",
                    "end": 26
                },
                {
                    "type": "bibr",
                    "ref_id": "b34",
                    "start": 37,
                    "text": "[42]",
                    "end": 41
                }
            ]
        },
        {
            "section": "Inclusion of the final state radiation calls as well as the parton showering and hadronization",
            "text": "For reference here follows some tree level matrix element generators: GRACE [43], CompHEP [44], CalcHEP [48], FeynCalc [49], MadGraph [29], AMEGIC++ [50] as well as multi-channel event generators HELAC-PHEGAS [51], ALPGEN [52], GR@PPA [53], O'Mega [54], Sherpa [27], Whizard [55], Pythia [41], Herwig [42].",
            "paragraph_rank": 42,
            "section_rank": 15,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b35",
                    "start": 76,
                    "text": "[43]",
                    "end": 80
                },
                {
                    "type": "bibr",
                    "ref_id": "b36",
                    "start": 90,
                    "text": "[44]",
                    "end": 94
                },
                {
                    "type": "bibr",
                    "ref_id": "b40",
                    "start": 104,
                    "text": "[48]",
                    "end": 108
                },
                {
                    "type": "bibr",
                    "ref_id": "b41",
                    "start": 119,
                    "text": "[49]",
                    "end": 123
                },
                {
                    "type": "bibr",
                    "ref_id": "b22",
                    "start": 134,
                    "text": "[29]",
                    "end": 138
                },
                {
                    "type": "bibr",
                    "ref_id": "b42",
                    "start": 149,
                    "text": "[50]",
                    "end": 153
                },
                {
                    "type": "bibr",
                    "ref_id": "b43",
                    "start": 209,
                    "text": "[51]",
                    "end": 213
                },
                {
                    "type": "bibr",
                    "ref_id": "b44",
                    "start": 222,
                    "text": "[52]",
                    "end": 226
                },
                {
                    "type": "bibr",
                    "ref_id": "b45",
                    "start": 235,
                    "text": "[53]",
                    "end": 239
                },
                {
                    "type": "bibr",
                    "ref_id": "b46",
                    "start": 248,
                    "text": "[54]",
                    "end": 252
                },
                {
                    "type": "bibr",
                    "ref_id": "b20",
                    "start": 261,
                    "text": "[27]",
                    "end": 265
                },
                {
                    "type": "bibr",
                    "ref_id": "b47",
                    "start": 275,
                    "text": "[55]",
                    "end": 279
                },
                {
                    "type": "bibr",
                    "ref_id": "b33",
                    "start": 288,
                    "text": "[41]",
                    "end": 292
                },
                {
                    "type": "bibr",
                    "ref_id": "b34",
                    "start": 301,
                    "text": "[42]",
                    "end": 305
                }
            ]
        },
        {
            "section": "Inclusion of the final state radiation calls as well as the parton showering and hadronization",
            "text": "In practice there are, yet, no complete and fully automatic matrix element generators at NLO, but many packages are being developed in this direction. Recent reports can be found in Ref. [56,57,58] and the main contenders are: MCFM [59], NLOJET++ [60], BlackHat [61], Rocket [62], CutTools [63], MadLoop [64], OpenLoops [65], GOLEM [66], POWHEG [67], aMC@NLO [68,69], Sher-pa+BlackHat [70], MadGOLEM [71], GoSam [72], HELAC-NLO [73].",
            "paragraph_rank": 43,
            "section_rank": 15,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b48",
                    "start": 187,
                    "text": "[56,",
                    "end": 191
                },
                {
                    "type": "bibr",
                    "ref_id": "b49",
                    "start": 191,
                    "text": "57,",
                    "end": 194
                },
                {
                    "type": "bibr",
                    "ref_id": "b50",
                    "start": 194,
                    "text": "58]",
                    "end": 197
                },
                {
                    "type": "bibr",
                    "ref_id": "b51",
                    "start": 232,
                    "text": "[59]",
                    "end": 236
                },
                {
                    "type": "bibr",
                    "ref_id": "b52",
                    "start": 247,
                    "text": "[60]",
                    "end": 251
                },
                {
                    "type": "bibr",
                    "ref_id": "b53",
                    "start": 262,
                    "text": "[61]",
                    "end": 266
                },
                {
                    "type": "bibr",
                    "ref_id": "b54",
                    "start": 275,
                    "text": "[62]",
                    "end": 279
                },
                {
                    "type": "bibr",
                    "ref_id": "b55",
                    "start": 290,
                    "text": "[63]",
                    "end": 294
                },
                {
                    "type": "bibr",
                    "ref_id": "b56",
                    "start": 304,
                    "text": "[64]",
                    "end": 308
                },
                {
                    "type": "bibr",
                    "ref_id": "b57",
                    "start": 320,
                    "text": "[65]",
                    "end": 324
                },
                {
                    "type": "bibr",
                    "ref_id": "b58",
                    "start": 332,
                    "text": "[66]",
                    "end": 336
                },
                {
                    "type": "bibr",
                    "ref_id": "b59",
                    "start": 345,
                    "text": "[67]",
                    "end": 349
                },
                {
                    "type": "bibr",
                    "ref_id": "b60",
                    "start": 359,
                    "text": "[68,",
                    "end": 363
                },
                {
                    "type": "bibr",
                    "ref_id": "b61",
                    "start": 363,
                    "text": "69]",
                    "end": 366
                },
                {
                    "type": "bibr",
                    "ref_id": "b62",
                    "start": 385,
                    "text": "[70]",
                    "end": 389
                },
                {
                    "type": "bibr",
                    "ref_id": "b63",
                    "start": 400,
                    "text": "[71]",
                    "end": 404
                },
                {
                    "type": "bibr",
                    "ref_id": "b64",
                    "start": 412,
                    "text": "[72]",
                    "end": 416
                },
                {
                    "type": "bibr",
                    "ref_id": "b65",
                    "start": 428,
                    "text": "[73]",
                    "end": 432
                }
            ]
        },
        {
            "text": "Matrix element multi-dimensional Monte-Carlo integration",
            "section_rank": 16
        },
        {
            "section": "Matrix element multi-dimensional Monte-Carlo integration",
            "text": "Unfortunately matrix elements tend to have singularities in some parts of the phase space making their integration difficult: for example, in the collinear region where two particles (e.g. an electron and a photon) get a small angular separation. This corresponds to the quantum physical situation where an electron alone cannot be distinguished from an electron closely accompanied by a soft photon. In that case one gets a singularity that can be tamed by introducing a cut on the photon-electron angle smaller than the actual experimental angular resolution.",
            "paragraph_rank": 44,
            "section_rank": 16
        },
        {
            "section": "Matrix element multi-dimensional Monte-Carlo integration",
            "text": "Anyway singularities occur in the expression of the integrand and the skill of the event generator developers are still needed to craft the most efficient variable transformations so that singularities are avoided or minimized. The mapping between integration and physical variables is coded into the \"kinematics subroutines\". They are selectively plugged into the integrand code according to its sensitivity to specific divergences.",
            "paragraph_rank": 45,
            "section_rank": 16
        },
        {
            "section": "Matrix element multi-dimensional Monte-Carlo integration",
            "text": "For the less acute variations, different techniques automatically adapting the sampling grid to the shape of the integrand have been implemented in the main multi-dimensional integration packages like Vegas [74,75], Bases/Spring [76], Dice [77], Foam [78], Vamp [79], Mint [80], ParInt [81]:",
            "paragraph_rank": 46,
            "section_rank": 16,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b66",
                    "start": 207,
                    "text": "[74,",
                    "end": 211
                },
                {
                    "type": "bibr",
                    "ref_id": "b67",
                    "start": 211,
                    "text": "75]",
                    "end": 214
                },
                {
                    "type": "bibr",
                    "ref_id": "b68",
                    "start": 229,
                    "text": "[76]",
                    "end": 233
                },
                {
                    "type": "bibr",
                    "ref_id": "b69",
                    "start": 240,
                    "text": "[77]",
                    "end": 244
                },
                {
                    "type": "bibr",
                    "ref_id": "b70",
                    "start": 251,
                    "text": "[78]",
                    "end": 255
                },
                {
                    "type": "bibr",
                    "ref_id": "b71",
                    "start": 262,
                    "text": "[79]",
                    "end": 266
                },
                {
                    "type": "bibr",
                    "ref_id": "b72",
                    "start": 273,
                    "text": "[80]",
                    "end": 277
                },
                {
                    "type": "bibr",
                    "ref_id": "b73",
                    "start": 286,
                    "text": "[81]",
                    "end": 290
                }
            ]
        },
        {
            "section": "Matrix element multi-dimensional Monte-Carlo integration",
            "text": "\u2022 Importance sampling: the grid intervals or bins get smaller and therefore the sampling is denser in regions of high integrand values as this phase space regions will contribute most to the total integral. \u2022 Stratified sampling: the bins get smaller in the region of fast variation of the integrand so that from one interval to the next the function varies slowly and smoothly. \u2022 Multi-channel sampling: several mappings, namely grid definitions, can be selected depending on the phase space region. The method is to always select the mapping that suits best the behavior of the integrand at the particular sampling point.",
            "paragraph_rank": 47,
            "section_rank": 16
        },
        {
            "section": "Matrix element multi-dimensional Monte-Carlo integration",
            "text": "For processes up to 2\u21924, at tree level the full automation is now usual business. For more particles in the final state (e.g. 2\u21926), the integration stage may take a lot of CPU time and the calculation may become intractable if the kinematics mappings are not adequate.",
            "paragraph_rank": 48,
            "section_rank": 16
        },
        {
            "section": "Matrix element multi-dimensional Monte-Carlo integration",
            "text": "For NLO calculations the number of integration dimensions increases due to the additional loop four-momenta and the integration difficulties become even more stringent. In addition, the size of the integrant can be enormous and compiler and computer memory limitations become an issue. Fortunately after the NLO revolution, some computations have become feasible, but automation has not yet been fully implemented.",
            "paragraph_rank": 49,
            "section_rank": 16
        },
        {
            "section": "Matrix element multi-dimensional Monte-Carlo integration",
            "text": "When the integrand becomes too large, it can be cut in smaller parts (e.g. each sub-diagram) distributed over many nodes. This is one direction for parallelization. A second one to accelerate the computation is to share the whole phase space again over many nodes. So that, in the end, each node will compute a part of the integrand for a part of the phase space. However, the integration algorithm requires the full value of one or several integrand calculations over the whole phase space, before executing the next iteration. This is the serial part of the integration algorithm and load balancing between the nodes becomes an issue that has to be addressed [82,83] .",
            "paragraph_rank": 50,
            "section_rank": 16,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b74",
                    "start": 661,
                    "text": "[82,",
                    "end": 665
                },
                {
                    "type": "bibr",
                    "ref_id": "b75",
                    "start": 665,
                    "text": "83]",
                    "end": 668
                }
            ]
        },
        {
            "section": "Matrix element multi-dimensional Monte-Carlo integration",
            "text": "Recently, it has been shown that the computation of loop integrals or tree level processes on GPUs have led to dramatic acceleration [84,85]. Supercomputers will certainly benefit these calculations.",
            "paragraph_rank": 51,
            "section_rank": 16,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b76",
                    "start": 133,
                    "text": "[84,",
                    "end": 137
                },
                {
                    "type": "bibr",
                    "ref_id": "b77",
                    "start": 137,
                    "text": "85]",
                    "end": 140
                }
            ]
        },
        {
            "text": "Matrix element method for data analysis.",
            "section_rank": 17
        },
        {
            "section": "Matrix element method for data analysis.",
            "text": "As we have seen, the process matrix element is at the heart of the event generator and its calculation can be made automatic in most cases, at least, at tree level. But a new application of the matrix element is looming up for the data analysis it-self. The usual way of extracting new insights from the experiments is by comparing the simulated and experimental event distributions. This is the conventional approach. A more elaborate, but similar technique called the template analysis has been developed along these lines and involving likelihood maximization. But a new approach is nowadays preferred. It involves the use of the matrix element expression in the event selection algorithm it-self. This is the matrix-element method [86] or also called the matrix element likelihood algorithm (MELA) in CMS. 9.1. Template method Let us assume that a process parameter is to be estimated from experimental data. Let's take for example, the mass of the top quark. The t-quark has a very short life-time and decays, even before it gets hadronized, in various channels including in lepton + jet. From these decay products the top quark mass can be reconstructed. The template method requires the simulation of several similar distributions assuming different top masses. Then, a likelihood expression is built and its maximization provides the best fit between the experimental and simulated distributions. This method is straight forward, but only a small part of the full event information is used, namely the reconstructed top mass. The rest of the event data like the distributions of remaining particles and their correlations does not enter this optimization.",
            "paragraph_rank": 52,
            "section_rank": 17,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b78",
                    "start": 735,
                    "text": "[86]",
                    "end": 739
                }
            ]
        },
        {
            "text": "Matrix-element method",
            "section_rank": 18
        },
        {
            "section": "Matrix-element method",
            "text": "From the same selected sample, a probability is built for each event taking into account all final state particle kinematics to estimate to which category it is most likely part of. The following probability is therefore estimated event per event:",
            "paragraph_rank": 53,
            "section_rank": 18
        },
        {
            "section": "Matrix-element method",
            "text": "Where:",
            "paragraph_rank": 54,
            "section_rank": 18
        },
        {
            "section": "Matrix-element method",
            "text": "represents the measured kinematic variables for all final state particles is the parameter to be estimated, here the top mass",
            "paragraph_rank": 55,
            "section_rank": 18
        },
        {
            "section": "Matrix-element method",
            "text": "is the square of the matrix element associated to the signal process under study, here the production of a pair of top quarks.",
            "paragraph_rank": 56,
            "section_rank": 18
        },
        {
            "section": "Matrix-element method",
            "text": "( , ) is the transfer function mapping the measured four-vectors and the matrix element theoretical parameters.",
            "paragraph_rank": 57,
            "section_rank": 18
        },
        {
            "section": "Matrix-element method",
            "text": "( ) is the parton distribution function described above giving the probability that the initial colliding partons with 1 and 2 four-momenta are produced by the interacting protons.",
            "paragraph_rank": 58,
            "section_rank": 18
        },
        {
            "section": "Matrix-element method",
            "text": "This expression is integrated over the phase space and normalized to the observed cross-section .",
            "paragraph_rank": 59,
            "section_rank": 18
        },
        {
            "text": "Similar probabilities",
            "section_rank": 19
        },
        {
            "section": "Similar probabilities",
            "text": "1\u2026 are formed assuming that the same event belongs to background processes, namely processes having a similar final state but not through the decay of top quarks.",
            "paragraph_rank": 60,
            "section_rank": 19
        },
        {
            "section": "Similar probabilities",
            "text": "If the , 1\u2026 are the fractional parts of the total sample (signal or backgrounds), a global probability for one event with the kinematics to form a mass whether from signal or from background can then be computed:",
            "paragraph_rank": 61,
            "section_rank": 19
        },
        {
            "section": "Similar probabilities",
            "text": "Finally, a sample level likelihood is computed as the product of the global probability of all events and the following inverse log is minimized to get the most probable :",
            "paragraph_rank": 62,
            "section_rank": 19
        },
        {
            "section": "Similar probabilities",
            "text": "In this method implemented in the MadWeight [87] package, the whole event information is incorporated in the parameter estimation. In addition the separation between the experimental data and the theory input is well identified. But this is a very CPU time consuming procedure as for each event several multi-dimensional integrations are necessary. To help solving this problem, the use of GPU has been implemented successfully [88]. The template ( fig. 9) and the matrix element method ( fig.10) have been used with great success for establishing the value of the top mass at the Tevatron by the D0 [89] and CDF [89] experiments. A similar technique has also been used at LHC to estimate the Higgs mass. A global expression ME-LA is computed from the likelihood, namely MELA is 0 (1) if the probability of the event to be a background (signal) is maximal. Each event is presented on a scatter plot with the estimated Higgs mass on the x axis and the MELA value on y-axis. Setting the threshold on MELA above 0.5 selects events more likely to be a signal than a background which greatly enhances the signal over noise ratio of the Higgs mass measurement ( fig. 11). Matrix element methods incorporating NLO corrections have also been studied [90], but have not yet been implemented for experimental data analyses.  ",
            "paragraph_rank": 63,
            "section_rank": 19,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b79",
                    "start": 44,
                    "text": "[87]",
                    "end": 48
                },
                {
                    "type": "bibr",
                    "ref_id": "b80",
                    "start": 428,
                    "text": "[88]",
                    "end": 432
                },
                {
                    "type": "figure",
                    "ref_id": "fig_3",
                    "start": 449,
                    "text": "fig. 9",
                    "end": 455
                },
                {
                    "type": "figure",
                    "ref_id": "fig_0",
                    "start": 489,
                    "text": "fig.10",
                    "end": 495
                },
                {
                    "type": "bibr",
                    "ref_id": "b81",
                    "start": 600,
                    "text": "[89]",
                    "end": 604
                },
                {
                    "type": "bibr",
                    "ref_id": "b81",
                    "start": 613,
                    "text": "[89]",
                    "end": 617
                },
                {
                    "type": "figure",
                    "ref_id": "fig_0",
                    "start": 1156,
                    "text": "fig. 11",
                    "end": 1163
                },
                {
                    "type": "bibr",
                    "ref_id": "b82",
                    "start": 1242,
                    "text": "[90]",
                    "end": 1246
                }
            ]
        },
        {
            "text": "Cross-disciplinary fertilization and conclusions",
            "section_rank": 20
        },
        {
            "section": "Cross-disciplinary fertilization and conclusions",
            "text": "Computational particle physics covers two main branches, one dealing with the non-perturbative calculations, lattice QCD, which is virtually intractable without the use of large parallel super computers. Recent developments show that thanks to a new algorithm and a drastic increase in computing power, the hadronic mass spectrum and other global parameters are now well reproduced. The other branch is related to the perturbative region probing the inner part of the nucleon as well as its high energy dynamics. These computations, for the highest available energy, have also become feasible thanks to new NLO calculation methods and, again, to the availability of a large Grid computing infrastructure. The automation is now on track for the most complex calculations. Experimental design, physics analysis and theoretical interpretation at current colliders deeply rely on the development of these \"new\" tools providing matrix elements and event generators.",
            "paragraph_rank": 64,
            "section_rank": 20
        },
        {
            "section": "Cross-disciplinary fertilization and conclusions",
            "text": "In fact, perturbative calculations both benefit from and motivate the development of techniques and methods common to other domains in computational science including:",
            "paragraph_rank": 65,
            "section_rank": 20
        },
        {
            "section": "Cross-disciplinary fertilization and conclusions",
            "text": "\u2022 Symbolic calculation: the traduction of symmetries and fields to Feynman rules and couplings has been automated using algebraic languages like Reduce, Maple or Mathematica. Often created to support these calculations, they are now multi-purpose languages and are heavily used in many other research fields. However, the handling of very large expression of high complexity has triggered the implementation of expression disk caching and parallel techniques like in the language Form. \u2022 Parallelism:",
            "paragraph_rank": 66,
            "section_rank": 20
        },
        {
            "section": "Cross-disciplinary fertilization and conclusions",
            "text": "o Numerical multi-dimensional integration: one of the main issues in perturbative computation is the integration over many dimensions of very large singular expressions. Some of the integration packages have been made parallel and able to run on GPU or supercomputers. o Process calculation factorial growth: the number of models, of processes and sub-processes is sky rocketing prompting for the use of massively parallel supercomputers or large clusters. \u2022 High precision floating point computation: often double precision is not sufficient. Quadruple or octuple precision becomes necessary to achieve convergence for the most singular integrands leading to order of magnitude increase in computing time [92]. Specific libraries and hardware developments based on co-processors or GPUs [93] are in progress.",
            "paragraph_rank": 67,
            "section_rank": 20,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b84",
                    "start": 706,
                    "text": "[92]",
                    "end": 710
                },
                {
                    "type": "bibr",
                    "ref_id": "b85",
                    "start": 788,
                    "text": "[93]",
                    "end": 792
                }
            ]
        },
        {
            "text": "\u2022 Large computing infrastructures:",
            "section_rank": 21
        },
        {
            "section": "\u2022 Large computing infrastructures:",
            "text": "o The internationally distributed Data Grid architecture, mostly created to cover the collider experiments needs, has fulfilled the expectations. It gave birth to the \"Cloud\" technology, a new commercial endeavor. o Although most of the current data analysis and simulations programs are serial, both the manycore/GPU long term industrial trends and the need for higher precision theoretical predictions are incentives to implement multi-threading and parallel execution techniques (e.g. for analysis [94] and simulation [95]). Certainly not an easy task in object oriented environments, but it will open access to the huge power of supercomputers. \u2022 General data analysis toolkits (e.g. Root) implementing advanced statistical methods developed for particle physics has found applications in other environment like biology, finance and medicine.",
            "paragraph_rank": 68,
            "section_rank": 21,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b86",
                    "start": 501,
                    "text": "[94]",
                    "end": 505
                },
                {
                    "type": "bibr",
                    "ref_id": "b87",
                    "start": 521,
                    "text": "[95]",
                    "end": 525
                }
            ]
        },
        {
            "section": "\u2022 Large computing infrastructures:",
            "text": "Finally, the initial dream of automatic perturbative physics calculations has triggered the development of a flurry of packages and specific tools. Some are at the heart of today high-energy colliders and astroparticle experiment data analysis. But these packages have usually very little in common. Despite great attempts to standardize some aspects like the output files format (StdHep [96], HepMC [97]) or the interface with NLO calculations (BLHA [98]), they still have no common input syntax, no general parameter definition format, a poor modularity, no universal submission scheme, no common visualization and analysis tools, and so on. It is time to address these issues to facilitate their necessary detailed comparisons and their embedding in the large experimental simulation packages.",
            "paragraph_rank": 69,
            "section_rank": 21,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b88",
                    "start": 388,
                    "text": "[96]",
                    "end": 392
                },
                {
                    "type": "bibr",
                    "ref_id": "b89",
                    "start": 400,
                    "text": "[97]",
                    "end": 404
                },
                {
                    "type": "bibr",
                    "ref_id": "b90",
                    "start": 451,
                    "text": "[98]",
                    "end": 455
                }
            ]
        },
        {
            "text": "Figure 1 :",
            "section_rank": 22
        },
        {
            "section": "Figure 1 :",
            "text": "Figure 1: History of the Universe",
            "paragraph_rank": 70,
            "section_rank": 22
        },
        {
            "text": "Figure 5 :",
            "section_rank": 23
        },
        {
            "section": "Figure 5 :",
            "text": "Figure 5: NLO Revolution[30] ",
            "paragraph_rank": 71,
            "section_rank": 23,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b23",
                    "start": 24,
                    "text": "[30]",
                    "end": 28
                }
            ]
        },
        {
            "text": "Figure 8 :",
            "section_rank": 24
        },
        {
            "section": "Figure 8 :",
            "text": "Figure 8: From symmetries to Event Generation",
            "paragraph_rank": 72,
            "section_rank": 24
        },
        {
            "text": "Figure 9 :",
            "section_rank": 25
        },
        {
            "section": "Figure 9 :",
            "text": "Figure 9: The template distributions for reconstructed top mass (left) and mw (right) for CDF[89] ",
            "paragraph_rank": 73,
            "section_rank": 25,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b81",
                    "start": 93,
                    "text": "[89]",
                    "end": 97
                }
            ]
        },
        {
            "text": "Figure 10 :Figure 11 :",
            "section_rank": 26
        },
        {
            "section": "Figure 10 :Figure 11 :",
            "text": "Figure 10: Measured likelihood for the top mass at D0 and CDF[89,91] ",
            "paragraph_rank": 74,
            "section_rank": 26,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b81",
                    "start": 61,
                    "text": "[89,",
                    "end": 65
                },
                {
                    "type": "bibr",
                    "ref_id": "b83",
                    "start": 65,
                    "text": "91]",
                    "end": 68
                }
            ]
        },
        {
            "section": "Figure 10 :Figure 11 :",
            "text": "QCD: Quantum ChromoDynamics, model of the strong interaction particularly probed at protons colliders like the LHC or at lepton-proton collider like HERA (DESY). 2 QED: Quantum ElectroDynamics, quantum theory for the electromagnetic interactions.",
            "paragraph_rank": 75,
            "section_rank": 26
        }
    ]
}