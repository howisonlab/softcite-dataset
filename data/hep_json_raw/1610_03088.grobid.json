{
    "level": "paragraph",
    "abstract": [
        {
            "text": "Study of the production of pairs of top quarks in association with a Higgs boson is one of the primary goals of the Large Hadron Collider over the next decade, as measurements of this process may help us to understand whether the uniquely large mass of the top quark plays a special role in electroweak symmetry breaking. Higgs bosons decay predominantly to bb, yielding signatures for the signal that are similar to tt + jets with heavy flavor. Though particularly challenging to study due to the similar kinematics between signal and background events, such final states (ttbb) are an important channel for studying the top quark Yukawa coupling. This paper presents a systematic study of machine learning (ML) methods for detecting tth in the h \u2192 bb decay channel. Among the eight ML methods tested, we show that two models, extreme gradient boosted trees and neural network models, outperform alternative methods. We further study the effectiveness of ML algorithms by investigating the impact of feature set and data size, as well as the structure of the models. While extended feature set and larger training sets expectedly lead to improvement of performance, shallow models deliver comparable or better performance than their deeper counterparts. Our study suggests that ensembles of trees and neurons, not necessarily deep, work effectively for the problem of tth detection.",
            "paragraph_rank": 1,
            "section_rank": 1
        }
    ],
    "body_text": [
        {
            "text": "I. INTRODUCTION",
            "section_rank": 2
        },
        {
            "section": "I. INTRODUCTION",
            "text": "One of the key avenues of study for Higgs boson physics at the LHC is observing the interaction of the Higgs boson with top quarks; the top quark is the most massive particle ever observed in nature, with a mass over 180 times that of the proton [1][2][3]. This large mass of the top quark, and how it is derived from the Higgs sector, is an important area of exploration for the LHC. Unfortunately, roughly only one out of every 100 Higgs bosons at the LHC is produced in association with a pair of top quarks (tth, denoted in this paper as signal) [4], and the dominant top quark + anti-top quark (tt, denoted as background) production is over 3 orders of magnitude more common [5]. The similar kinematics between signal and background events motivate the use of multivariate techniques to separate out these processes.",
            "paragraph_rank": 2,
            "section_rank": 2,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b0",
                    "start": 246,
                    "text": "[1]",
                    "end": 249
                },
                {
                    "type": "bibr",
                    "ref_id": "b1",
                    "start": 249,
                    "text": "[2]",
                    "end": 252
                },
                {
                    "type": "bibr",
                    "ref_id": "b2",
                    "start": 252,
                    "text": "[3]",
                    "end": 255
                },
                {
                    "type": "bibr",
                    "ref_id": "b3",
                    "start": 550,
                    "text": "[4]",
                    "end": 553
                },
                {
                    "type": "bibr",
                    "ref_id": "b4",
                    "start": 680,
                    "text": "[5]",
                    "end": 683
                }
            ]
        },
        {
            "section": "I. INTRODUCTION",
            "text": "The tth process has not been definitively observed yet at the LHC, though multivariate techniques have already been used in multiple searches. Such techniques include artificial neural networks (ANN) with simple network structures [6][7][8] and Boosted Decision Trees [8][9][10], and build on significant recent interest in particle physics in studying and adapting machine learning (ML) techniques to improve object reconstruction and identification [11][12][13][14][15][16][17], triggering on interesting physics [18,19], and to classify events as in tth analyses and other searches [20][21][22][23][24][25]. It should be noted that some tth analyses also make use of \"matrix element methods,\" which attempt to estimate leading order probabilities for events to come from signal or background processes by integrating over their production probabilities in a large multi-dimensional space [6,26,27], and are complimentary to the approaches studied here.",
            "paragraph_rank": 3,
            "section_rank": 2,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b5",
                    "start": 231,
                    "text": "[6]",
                    "end": 234
                },
                {
                    "type": "bibr",
                    "ref_id": "b6",
                    "start": 234,
                    "text": "[7]",
                    "end": 237
                },
                {
                    "type": "bibr",
                    "ref_id": "b7",
                    "start": 237,
                    "text": "[8]",
                    "end": 240
                },
                {
                    "type": "bibr",
                    "ref_id": "b7",
                    "start": 268,
                    "text": "[8]",
                    "end": 271
                },
                {
                    "type": "bibr",
                    "ref_id": "b8",
                    "start": 271,
                    "text": "[9]",
                    "end": 274
                },
                {
                    "type": "bibr",
                    "ref_id": "b9",
                    "start": 274,
                    "text": "[10]",
                    "end": 278
                },
                {
                    "type": "bibr",
                    "ref_id": "b10",
                    "start": 451,
                    "text": "[11]",
                    "end": 455
                },
                {
                    "type": "bibr",
                    "ref_id": "b11",
                    "start": 455,
                    "text": "[12]",
                    "end": 459
                },
                {
                    "type": "bibr",
                    "ref_id": "b12",
                    "start": 459,
                    "text": "[13]",
                    "end": 463
                },
                {
                    "type": "bibr",
                    "ref_id": "b13",
                    "start": 463,
                    "text": "[14]",
                    "end": 467
                },
                {
                    "type": "bibr",
                    "start": 467,
                    "text": "[15]",
                    "end": 471
                },
                {
                    "type": "bibr",
                    "ref_id": "b14",
                    "start": 471,
                    "text": "[16]",
                    "end": 475
                },
                {
                    "type": "bibr",
                    "ref_id": "b15",
                    "start": 475,
                    "text": "[17]",
                    "end": 479
                },
                {
                    "type": "bibr",
                    "ref_id": "b16",
                    "start": 515,
                    "text": "[18,",
                    "end": 519
                },
                {
                    "type": "bibr",
                    "ref_id": "b17",
                    "start": 519,
                    "text": "19]",
                    "end": 522
                },
                {
                    "type": "bibr",
                    "ref_id": "b18",
                    "start": 585,
                    "text": "[20]",
                    "end": 589
                },
                {
                    "type": "bibr",
                    "ref_id": "b19",
                    "start": 589,
                    "text": "[21]",
                    "end": 593
                },
                {
                    "type": "bibr",
                    "ref_id": "b20",
                    "start": 593,
                    "text": "[22]",
                    "end": 597
                },
                {
                    "type": "bibr",
                    "ref_id": "b21",
                    "start": 597,
                    "text": "[23]",
                    "end": 601
                },
                {
                    "type": "bibr",
                    "ref_id": "b22",
                    "start": 601,
                    "text": "[24]",
                    "end": 605
                },
                {
                    "type": "bibr",
                    "ref_id": "b23",
                    "start": 605,
                    "text": "[25]",
                    "end": 609
                },
                {
                    "type": "bibr",
                    "ref_id": "b5",
                    "start": 891,
                    "text": "[6,",
                    "end": 894
                },
                {
                    "type": "bibr",
                    "ref_id": "b24",
                    "start": 894,
                    "text": "26,",
                    "end": 897
                },
                {
                    "type": "bibr",
                    "ref_id": "b25",
                    "start": 897,
                    "text": "27]",
                    "end": 900
                }
            ]
        },
        {
            "section": "I. INTRODUCTION",
            "text": "We consider only Higgs bosons that decay via the dominant decay mode (h \u2192 bb). We make event selections that aim to find tt pairs that decay semi-leptonically, though all tt decays are included in the simulation. The pp \u2192 tth W + bW \u2212 bbb \u2192 \u03bdqq bbbb production process and decay chain is a particularly interesting final state for studying the potential use of more advanced ML techniques in particle physics. The signal has eight final state objects, two intermediate W boson mass resonances, two top quark mass resonances, and a dijet resonance at the Higgs boson mass that is of particular importance in separating signal and background. The neutrino is not directly detected, but instead inferred from applying conservation of momentum to the objects in the detector. In addition, most of the jets are expected to come from the hadronization and decay of b quarks. An example Feynman diagram for this process is shown on the left in Figure 1. It is notoriously difficult to predict the rates for the dominant backgrounds in the analysis (tt production produced in association with extra jets, particularly but not only of heavy flavor, as shown in the right in Figure 1), making the use of advanced ML techniques potentially useful if improved signal-background separation can be obtained. Particle physicists typically use the word \"variable\" to identify salient information about an object in the detector or an event; computer scientists and ML experts often use the word \"feature\" to reference the same item. The two words are used interchangeably here. This paper presents the first systematic studies of ML methods trained to separate the tth process from the dominant tt background. This is a compelling use case for ML, in part because both signal and background events have rich final states with many reconstructible features. The performance of multiple ML techniques are compared, as are results using different sets of features. A number of models of gradient boosted trees are studied, as are many neural network models, including for the first time in this physics channel several models of deep networks. The wide range of models show the importance of picking optimal network structure and training parameters. The importance of the availability of large samples in order to achieve the best performance is shown.",
            "paragraph_rank": 4,
            "section_rank": 2,
            "ref_spans": [
                {
                    "type": "figure",
                    "ref_id": "fig_0",
                    "start": 937,
                    "text": "Figure 1",
                    "end": 945
                },
                {
                    "type": "figure",
                    "ref_id": "fig_0",
                    "start": 1165,
                    "text": "Figure 1",
                    "end": 1173
                }
            ]
        },
        {
            "text": "II. DATA SETS FOR MACHINE LEARNING",
            "section_rank": 3
        },
        {
            "section": "II. DATA SETS FOR MACHINE LEARNING",
            "text": "Large numbers of simulated Monte Carlo (MC) events are required for any high-energy physics analysis, in particular for those that try to separate out very small signals from large backgrounds. The MC simulations for the signal and background samples used in this paper assume pp collision events at a center-of-mass energy of \u221a s = 13 TeV. Data sets for the samples listed below can be found at https://atlaswww.hep.anl.gov/asc/papers/1512.",
            "paragraph_rank": 5,
            "section_rank": 3
        },
        {
            "text": "08928/.",
            "section_rank": 4
        },
        {
            "text": "A. Signal events",
            "section_rank": 5
        },
        {
            "section": "A. Signal events",
            "text": "Signal events are simulated using Standard Model predictions at next-to-leading order (NLO) in QCD using Madgraph5 [28]. The NLO part of the calculations is performed using aMC@NLO [29], while the parton shower is modeled with the Herwig6 [30] generator.",
            "paragraph_rank": 6,
            "section_rank": 5,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b26",
                    "start": 115,
                    "text": "[28]",
                    "end": 119
                },
                {
                    "type": "bibr",
                    "ref_id": "b27",
                    "start": 181,
                    "text": "[29]",
                    "end": 185
                },
                {
                    "type": "bibr",
                    "ref_id": "b28",
                    "start": 239,
                    "text": "[30]",
                    "end": 243
                }
            ]
        },
        {
            "section": "A. Signal events",
            "text": "All decay channels for the Higgs boson and the top quarks are enabled. NN23NLO [31] is used for the parton-density function (PDF), with the value of the strong coupling constant set to \u03b1 s = 0.122. The total number of signal events is 12.5 million, corresponding to an integrated luminosity of 26, 975 fb \u22121 .",
            "paragraph_rank": 7,
            "section_rank": 5,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b29",
                    "start": 79,
                    "text": "[31]",
                    "end": 83
                }
            ]
        },
        {
            "text": "B. Background events",
            "section_rank": 6
        },
        {
            "section": "B. Background events",
            "text": "In order to speed up the rate at which background events are simulated in the region of interest, only the tt+1, 2, \u2265 3 parton processes are produced; the single lepton tt background requires at least two additional jets to enter the signal region of six or more jets, such that the zero-parton tt sample is expected to have negligible contribution in the signal region. To test this hypothesis, a smaller sample of tt + 0, 1, 2 \u2265 3 partons is produced and compared to the nominal sample, and good agreement in a variety of kinematic distributions is observed after the nominal signal selection. The background process is computed at leading order with",
            "paragraph_rank": 8,
            "section_rank": 6
        },
        {
            "section": "B. Background events",
            "text": "Madgraph5 [28] using tree-level matrix elements. High-order QCD effects are included using the parton-shower approach as implemented in Pythia6 [32], with NN23LO1 used for the PDF. The \"maxjetflavor\" option in Madgraph5 is set to 5 to allow for production of tt events in association with jets of b flavor. The MLM matching scheme [33] is used to remove overlap between events, with a minimum kt jet measure of 20. The total number of background events is 16.2 million, which corresponds to an integrated luminosity of 51.8 fb \u22121 .",
            "paragraph_rank": 9,
            "section_rank": 6,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b26",
                    "start": 10,
                    "text": "[28]",
                    "end": 14
                },
                {
                    "type": "bibr",
                    "ref_id": "b30",
                    "start": 144,
                    "text": "[32]",
                    "end": 148
                },
                {
                    "type": "bibr",
                    "ref_id": "b31",
                    "start": 331,
                    "text": "[33]",
                    "end": 335
                }
            ]
        },
        {
            "text": "C. Simulation",
            "section_rank": 7
        },
        {
            "section": "C. Simulation",
            "text": "All truth-level events are included in the public Monte Carlo HepSim repository [34], and further processed with the Delphes 3.3 fast simulation [35]. This simulation uses an ATLAS-like detector geometry as included in Delphes (though results should be generic, regardless of the LHC detector), with minor modifications to reduce the event-record size and to give photons the lowest priority in object overlap removal.",
            "paragraph_rank": 10,
            "section_rank": 7,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b32",
                    "start": 80,
                    "text": "[34]",
                    "end": 84
                },
                {
                    "type": "bibr",
                    "ref_id": "b33",
                    "start": 145,
                    "text": "[35]",
                    "end": 149
                }
            ]
        },
        {
            "section": "C. Simulation",
            "text": "Jets in the Delphes fast simulation are reconstructed with the anti-k T algorithm [36] with a distance parameter of 0.4 using the FastJet package [37].  events passing this selection are 1.3 million and 0.2 million, respectively.",
            "paragraph_rank": 11,
            "section_rank": 7,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b34",
                    "start": 82,
                    "text": "[36]",
                    "end": 86
                },
                {
                    "type": "bibr",
                    "ref_id": "b35",
                    "start": 146,
                    "text": "[37]",
                    "end": 150
                }
            ]
        },
        {
            "text": "III. SETUP FOR MACHINE LEARNING",
            "section_rank": 8
        },
        {
            "section": "III. SETUP FOR MACHINE LEARNING",
            "text": "To search for a process such as tth, the main ML task is to formulate a learning model that can effectively distinguish the signal from the background. In addition, we are also interested in understanding the full shape of the output of the ML discriminant. This is particularly important for searches for rare processes such as tth, in which the background is not a priori well understood. By including the full shape of the output of the multivariate discriminate, physicists can control some of the uncertain parameters of the background model. In other words, it is important not only to define signal-enriched regions, but also  [39]. The three variants of neural networks are described below. These neural models have similar network structure but vary based on the optimization algorithm employed. In contrast to the basic features, the extended features use information about the objects in a collision inspired in some ways by physics, typically by combining multiple pieces of information. With the exception of matrix element-based analyses, ML techniques in searches for tth production have so far typically used very few or zero basic features, and only a subset of extended variables. Such limitations are typically due to the complexity in working with more complicated algorithms and also the very large size of simulated samples required for an adequate training. One of the more useful quantities is the invariant mass of two or The advantage to using this figure of merit, as opposed to other, more complicated versions for discovery, is that relative comparisons between methods are insensitive to a global scaling of the luminosity for a study. In addition, F-score, defined as the harmonic mean of the precision rate and recall rate, is also studied. Table I compares    Figure 8.    Table II shows a comparison of these optimized cuts and this figure of merit. It also shows the relative increase in the amount of data that would be needed by each algorithm to achieve the same (statistical-only) sensitivity as the best performing algorithm. Since statistical sensitivity goes as the square root of the amount of data, small increases in S \u221a B can still indicate large improvements in performance, and be quite important for experiments that can take years, if not decades, to accumulate data sets of sufficient size for discovery. For example, the best neural network structure needs 3% more data to achieve the same performance as XGBoost, and using only the extended features requires at least nearly 18% more data. The use of only basic features requires 51% more data. In addition, the worse performance of algorithms such as the decision tree and random forest can be seen clearly. The optimal network (using as many inputs as possible) from NeuroBayes requires 27% more data to achieve the same sensitivity as the leading XGBoost algorithm.",
            "paragraph_rank": 12,
            "section_rank": 8,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b37",
                    "start": 634,
                    "text": "[39]",
                    "end": 638
                },
                {
                    "type": "table",
                    "ref_id": "tab_1",
                    "start": 1773,
                    "text": "Table I compares",
                    "end": 1789
                },
                {
                    "type": "figure",
                    "start": 1793,
                    "text": "Figure 8",
                    "end": 1801
                },
                {
                    "type": "table",
                    "ref_id": "tab_1",
                    "start": 1806,
                    "text": "Table II",
                    "end": 1814
                }
            ]
        },
        {
            "text": "A. Comparing learning algorithms",
            "section_rank": 9
        },
        {
            "section": "A. Comparing learning algorithms",
            "text": "To understand the full shape of the output from these models, Figure 9    as the figure of merit. The value of AxB for structure refers to A layers with B nodes per layer. For each classifier, a cut is placed on the network to maximize the value of the figure of merit. The value of S \u221a B is shown for 100 fb \u22121 . The last column is the relative amount of increase in data needed to match the (statistical-only) sensitivity of the best algorithm, shown in the top row. AF refers to the use of all features, EF refers to the use of extended features only, and BF refers to the use of basic features only. Only the best performing algorithms for each category are shown.  The same trend is observed with the XGBoost classifier: The highest performance of 80.2% AUC is obtained with all features, while the basic feature only delivers 73.6% AUC and the extended feature set delivers 79.6% AUC, all using the full training set. The results support the effectiveness of using or including the extended feature set.",
            "paragraph_rank": 13,
            "section_rank": 9,
            "ref_spans": [
                {
                    "type": "figure",
                    "start": 62,
                    "text": "Figure 9",
                    "end": 70
                }
            ]
        },
        {
            "section": "A. Comparing learning algorithms",
            "text": "One interesting observation is that when only basic features are used, NeuroBGD consistently delivers a better AUC than XGBoost. However, when the extended features are added, XGBoost delivers comparable or higher AUC. The comparison is shown in Table IV.",
            "paragraph_rank": 14,
            "section_rank": 9,
            "ref_spans": [
                {
                    "type": "table",
                    "ref_id": "tab_1",
                    "start": 246,
                    "text": "Table IV",
                    "end": 254
                }
            ]
        },
        {
            "section": "A. Comparing learning algorithms",
            "text": "The better performance of NeuroBGD when using only basic features can be related to its capability to extract correlations and information from these variables for classification.  variables such as the kinematics of b-tagged jets.",
            "paragraph_rank": 15,
            "section_rank": 9
        },
        {
            "text": "C. On data size and training time",
            "section_rank": 10
        },
        {
            "section": "C. On data size and training time",
            "text": "It is important to understand the effect of the training set size for our problem, not only because data size often impacts learning, but also because of the difficult and lengthy process of simulating background events for this analysis. Figure 12 shows how the performance changes as the size of the training set increases. The experiments were performed using the NeuroBGD classifier. Figure 11 (left plot) shows the ROC curves for the 2x15 network.",
            "paragraph_rank": 16,
            "section_rank": 10,
            "ref_spans": [
                {
                    "type": "figure",
                    "ref_id": "fig_0",
                    "start": 239,
                    "text": "Figure 12",
                    "end": 248
                },
                {
                    "type": "figure",
                    "ref_id": "fig_0",
                    "start": 388,
                    "text": "Figure 11",
                    "end": 397
                }
            ]
        },
        {
            "section": "C. On data size and training time",
            "text": "Not surprisingly, the AUC improves as the training set size increases. For example, for the NeuroBGD with 2 layers and 15 nodes per layer, when the training set size increases   make from 1K to 338K, the AUC for detecting tth increases from 65.5% to 80.0% (as is illustrated in Figure 9 and Figure 12). The trend of AUC improvement plateaus as the training size continues toward the maximum value of 338K. Meanwhile, the time needed for training still grows at a significant rate as the training set size increases, from 17 seconds to more than 3 hours. Similar trends are observed for all tested structures of the neural classifier.",
            "paragraph_rank": 17,
            "section_rank": 10,
            "ref_spans": [
                {
                    "type": "figure",
                    "start": 278,
                    "text": "Figure 9",
                    "end": 286
                },
                {
                    "type": "figure",
                    "ref_id": "fig_0",
                    "start": 291,
                    "text": "Figure 12",
                    "end": 300
                }
            ]
        },
        {
            "section": "C. On data size and training time",
            "text": "For the XGBoost classifier, when the training set size increases from 1K to 338K, the AUC for detecting tth increases from 70.8% to 80.2%, with training time growing from 11 seconds to 324 seconds. XGBoost is significantly more efficient than NeuroBGD when the data size grows larger. When column subsampling is not used for XGBoost, the training time is about three times longer (about 15 minutes using 338K training events) but the AUC is not affected.",
            "paragraph_rank": 18,
            "section_rank": 10
        },
        {
            "text": "D. On deep networks",
            "section_rank": 11
        },
        {
            "section": "D. On deep networks",
            "text": "Recently in the ML field, there has been a strong interest in deep learning. Deep neural networks, particularly deep convolutional neural networks (CNN) and variants have obtained  For XGboost, it is also found the the best AUC is obtained with tree depth of 1. When deeper trees with depths of 2 to 6 are tested with the same parameter setup and number of iterations, lower AUCs between 74.9% and 79.3% are obtained and overfitting is observed.",
            "paragraph_rank": 19,
            "section_rank": 11
        },
        {
            "section": "D. On deep networks",
            "text": "The observation that deep model is not helpful for the learning problem seems surprising but is not entirely unexpected. Among the applications in deep learning, the most successful domains have been with unstructured data, which refer to inputs with spatial or temporal correlations such as images and speech. For those problems, explicit feature extraction from the raw images is often not required, especially when models such as convolutional neural networks are employed. Such deep networks also demand a huge training set (often in millions) and a long training time. For domains and problems where much effort has put into carefully crafting features (referred to as feature engineering) and thus that have structured data, alternative learning methods can deliver comparable performance with reduced computational complexity in training, as has been found among Kaggle ML competitions [39].",
            "paragraph_rank": 20,
            "section_rank": 11,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b37",
                    "start": 893,
                    "text": "[39]",
                    "end": 897
                }
            ]
        },
        {
            "section": "D. On deep networks",
            "text": "Our empirical results suggest that advanced ensembles of trees and neurons, not necessarily deep, work effectively for the problem, or at least for the given features. With the reason still to be further explored, one possible explanation of such phenomenon is that the discriminating information for detecting tth reside in the individual features, especially in the extended features that are inspired by physics and already contain the combination of pieces of information. This explanation is supported by the fact that when only basic features are used, NeuroBGD works better than XGBoost across the board on all training data sizes. However, they become comparable once extended features are introduced. Meanwhile, there also exists a possibility that the advantage of a deeper model is yet to be revealed with larger training sets to offset overfitting.",
            "paragraph_rank": 21,
            "section_rank": 11
        },
        {
            "text": "E. Future work",
            "section_rank": 12
        },
        {
            "section": "E. Future work",
            "text": "The detection of tth remains an important but difficult learning problem, though multiple avenues exist to improve the separation of signal and background in future studies. One potential improvement, as-of-yet studied with deep networks, is the use of expert classification tools, where initial classifiers are used to make jet assignments to objects from the collision and thus to remove combinatorial issues. The information from these initial classifiers would then be used in a second ML algorithm. Such techniques were used in the recent ATLAS tth search [10], and offered significant improvement over single classifier techniques.",
            "paragraph_rank": 22,
            "section_rank": 12,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b9",
                    "start": 561,
                    "text": "[10]",
                    "end": 565
                }
            ]
        },
        {
            "text": "VI. CONCLUSION",
            "section_rank": 13
        },
        {
            "section": "VI. CONCLUSION",
            "text": "The use of ML algorithms has already been shown to be important for observing rare particle physics processes such as tth, though the use of more sophisticated and advanced techniques offer important gains. A clear example of the use of such tools is shown in Figure 14, which indicates the power of a neural network to separate out signal and background.",
            "paragraph_rank": 23,
            "section_rank": 13,
            "ref_spans": [
                {
                    "type": "figure",
                    "start": 260,
                    "text": "Figure",
                    "end": 266
                }
            ]
        },
        {
            "section": "VI. CONCLUSION",
            "text": "Before any ML requirements, the small signal is extremely difficult to see on top of a very large background; after placing a cut on a neural network discriminant, the signal becomes clearly visible by eye.",
            "paragraph_rank": 24,
            "section_rank": 13
        },
        {
            "section": "VI. CONCLUSION",
            "text": "In the data sets studied here, both NeuroBGD and NeuroSGD outperform NeuroBayes along with the four other methods tested, both in terms of AUC, as well as in terms of \"equivalent data needed for discovery.\" Furthermore, it is shown that XGBoost has similar performance to NeuroBGD and NeuroSGD, but takes less training time compared to the NNs. The availability of large samples for training is found, as expected, to be important for network training. The benefit of deeper models, however, is not proved for our problem in either NeuroBGD or XGBoost, two of the top performing models.",
            "paragraph_rank": 25,
            "section_rank": 13
        },
        {
            "section": "VI. CONCLUSION",
            "text": "The use of both extended and basic features in combination is found to give marginally better performance than either single feature set. When only basic features are used, the neural networks are found to perform better than XGBoost, likely because they are able to reproduce some of the extended features; once the extended features are included, this performance difference vanishes, indicating that the standard set of physics-inspired features captures most of the salient information that separates signal from background. lab at NIU for providing the hardware resources for this research.",
            "paragraph_rank": 26,
            "section_rank": 13
        },
        {
            "text": "FIG. 1 .",
            "section_rank": 14
        },
        {
            "section": "FIG. 1 .",
            "text": "FIG. 1. Example Feynman diagrams for the tth process of interest (left) and for the dominant tt background produced in association with b quarks (right).",
            "paragraph_rank": 27,
            "section_rank": 14
        },
        {
            "text": "Calorimeter towers are used as inputs for jet reconstruction. Jets are required to have E T > 20 GeV and |\u03b7| < 3, where E T = E sin \u03b8 and \u03b8 is the polar angle. For jet clustering, stable particles are selected if their mean lifetimes are larger than 3 \u00d7 10 \u221211 seconds. Neutrinos are excluded from consideration in jet clustering. The b\u2212tagging is parameterized by the Delphes 3.3 using the expected b\u2212tagging efficiency and fake rates as given by the ATLAS collaboration [38]. In the nominal Delphes simulation, the b-tagging efficiency is parameterized as 0.80 tanh(0.003p T )(30/(1 + 0.086p T ), where p T is the component of the momentum in the transverse plane. Misidentification rates for light quarks and c\u2212 quarks are approximately 0.002 and 0.2, respectively, with a weak dependence on the transverse energy. For this analysis, the b-tagging is modified to allow for multiple working points with similar functional forms but tradeoffs in terms of efficiency versus rejection of mistagged jets. Such \"continuous b-tagging\" can be a powerful tool in a multivariate discriminant. Figure 2 shows the efficiencies and mistag rates for the five working points. D. Event selection Objects (jets, electrons and muons) are required to have p T > 20 GeV and |\u03b7| < 2.5. The selection for both signal and background follows a loose lepton+jets selection with the requirement of extra jets and b-tags: exactly one electron or muon, at least six jets, and at least 3 b-tags using the loosest working point. The total number of signal and background",
            "paragraph_rank": 28,
            "section_rank": 15
        },
        {
            "text": "10 FIG. 2 .",
            "section_rank": 16
        },
        {
            "section": "10 FIG. 2 .",
            "text": "FIG. 2. Left: Parameterized b-tag (red) and c-tag efficiency (blue) as a function of jet p T for the five working points considered in this analysis. Right: The mistag rate for these same five working points.",
            "paragraph_rank": 29,
            "section_rank": 16
        },
        {
            "text": "\u2022FIG. 3 .FIG. 4 .",
            "section_rank": 17
        },
        {
            "section": "\u2022FIG. 3 .FIG. 4 .",
            "text": "FIG. 3. Example signal and background distributions for three basic variables associated with btagged jets. Left: transverse momentum of the second hardest b-tagged jet. Middle: pseudorapidity of the second hardest b-tagged jet. Right: transverse momentum of the third hardest b-tagged jet.",
            "paragraph_rank": 30,
            "section_rank": 17
        },
        {
            "text": "FIG. 5 .FIG. 6 .FIG. 7 .",
            "section_rank": 18
        },
        {
            "section": "FIG. 5 .FIG. 6 .FIG. 7 .",
            "text": "FIG. 5. Example signal and background distributions for three basic variables associated with jet and b-tag multiplicities. Left: total number of identified jets per event. Middle: total number of b-tags per event, as calculated using the loosest b-tag working point. Right: total number of b-tags per event, as calculated using a tighter working point.",
            "paragraph_rank": 31,
            "section_rank": 18
        },
        {
            "text": "the effectiveness of the eight machine learning algorithms described in Section III. The results are based on a set that contains 423,173 total events, with equal contributions from signal and background. Among them, 10% of the data set is set aside to provide an independent validation set for model training. Another 10% is used as the testing set, and the remaining 338,541 events are used for training. Each feature is normalized over the entire balanced data set using the z-score so that the data mean is 0 and the standard deviation is 1. An equal number of samples are randomly selected from the signal and background data pool when designating events for testing or validation. The training set is randomly shuffled. Experiments are conducted using the full set of 597 features, with the exception of NeuroBayes, which uses the top 200 ranked features due to the software's limit on input feature dimensionality (see section V B for feature ranking). For NeuroSGD and NeuroBGD, the network has 2 layers and 15 nodes per layer. In addition to the Tanh activation function, a Rectified Linear (ReLU) activation function is also tested for the hidden layers. ReLU takes less training time than the Tanh activation function and delivers comparable AUC. For XGBoost, the result inTable Iis obtained by a model of depth 1 and number of training iterations 1000, with column subsampling set to 25 percent.",
            "paragraph_rank": 32,
            "section_rank": 19
        },
        {
            "text": "For",
            "section_rank": 20
        },
        {
            "section": "For",
            "text": "each ML technique under study, an optimized requirement is placed on the value of the output discriminant by choosing the cut such that it gives the largest value of S \u221a B .",
            "paragraph_rank": 33,
            "section_rank": 20
        },
        {
            "text": "depicts signal and background distributions from NeuroBGD and XGBoost. The output distributions from the NeuroBayes model are shown in Figure 10. The structure seen is related to the b-tagging, with clear \"bumps\" arising from regions with different numbers of b-tags. The more complex algorithms shown in Figure 9 do not have these features.",
            "paragraph_rank": 34,
            "section_rank": 21
        },
        {
            "text": "FIG. 10 .",
            "section_rank": 22
        },
        {
            "section": "FIG. 10 .",
            "text": "FIG. 10. NeuroBayes output distributions for the NeuroBayes MVA trained using the top 20 (left) and top 200 (right) discriminating variables",
            "paragraph_rank": 35,
            "section_rank": 22
        },
        {
            "text": "Figure 11 (",
            "section_rank": 23
        },
        {
            "section": "Figure 11 (",
            "text": "right plot) shows the ROC curves of a NeuroBGD classifier when partial or all features are used for learning.",
            "paragraph_rank": 36,
            "section_rank": 23
        },
        {
            "text": "Once extended features are involved, the effect of NeuroBGD for feature extraction becomes less critical since the extended features are already the combination of basic features, based on physical principles. To understand the effectiveness of features used as the inputs for the learning algorithms, we also calculate the AUC of each individual feature. The 597 features are then ranked using the individual AUC. As expected, b-tagging related variables are among the top ranked features, particularly the number of b-tagged jets passing a given threshold, as well as other",
            "paragraph_rank": 37,
            "section_rank": 24
        },
        {
            "text": "FIG. 12 .",
            "section_rank": 25
        },
        {
            "section": "FIG. 12 .",
            "text": "FIG.12. Summary of the AUC (left) and the total training time required (right) for all of the NeuroBGD networks trained on the full set of input variables. The vertical AUC axis in the left plot starts at 0.5 since this is the value that can be obtained when there is no separation between signal and background. The training times reported in the right plot vary for repeated experiments.",
            "paragraph_rank": 38,
            "section_rank": 25
        },
        {
            "text": "many successes[47][48].To understand the impact of network depth for this problem, NeuroBGD of varying structures are tested. Results are shown inFigure 13and inTable III. In this analysis, improvement in detection performance is not observed in deeper networks with more layers.Networks of different number of layers deliver comparable AUCs, with the highest AUC obtained by a network of 2 layers and 15 nodes per layer (80.0%). In contrast, the networks of 8 or 11 layers and 300 nodes per layer deliver an AUC of 79.5%.",
            "paragraph_rank": 39,
            "section_rank": 26,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b45",
                    "start": 14,
                    "text": "[47]",
                    "end": 18
                },
                {
                    "type": "bibr",
                    "ref_id": "b46",
                    "start": 18,
                    "text": "[48]",
                    "end": 22
                }
            ]
        },
        {
            "text": "FIG. 13 .",
            "section_rank": 27
        },
        {
            "section": "FIG. 13 .",
            "text": "FIG. 13. ROC curves from NeuroBGD networks with different numbers of layers and nodes perlayer. The variation in performance is small so the curves overlap.",
            "paragraph_rank": 40,
            "section_rank": 27
        },
        {
            "text": "FIG. 14 .",
            "section_rank": 28
        },
        {
            "section": "FIG. 14 .",
            "text": "FIG. 14. Stacked signal and background plots of a candidate reconstructed Higgs mass variable, normalized to their respective cross-sections. Left: Signal and background after the loose event selection described in section II D is applied, but before any ML requirement. The signal contribution is difficult to see due to the large background. Right: The same simulated data after placing a cut on the NeuroBGD output (using the 2 \u00d7 15 structure shown on the right in Fig 9) to maximize S/ \u221a B. After cutting the signal becomes clearly visible. The background also peaks near the Higgs mass of 125 GeV due to making a requirement that events look signal-like.",
            "paragraph_rank": 41,
            "section_rank": 28
        },
        {
            "text": "How does the size of the data set impact learning, particularly for the neural net models?4. Do deeper models, such as neural networks of more layers or deeper trees provide any",
            "paragraph_rank": 42,
            "section_rank": 29
        },
        {
            "text": "Table I",
            "section_rank": 30
        },
        {
            "section": "Table I",
            "text": "shows that XGBoost and the three neural network models are more effective forAlgorithm KNN Naive Bayes Decision Tree RF NeuroBayes NeuroSGD NeuroBGD XGBoost",
            "paragraph_rank": 43,
            "section_rank": 30
        },
        {
            "section": "Table I",
            "text": "the models. XGBoost and NeuroBGD are the top two performing models and they have nearly identical performance. Excluding XGBoost and the three neural learning algorithms, the random forest performs the best. A comparison among the ML methods using ROC curves is shown in",
            "paragraph_rank": 44,
            "section_rank": 30
        },
        {
            "text": "Table III",
            "section_rank": 31
        },
        {
            "section": "Table III",
            "text": "compares the results of using basic features, extended features and all features.For this comparative experiment, the NeuroBGD classifier is used with the data sets as described in Section V A. Using the set of extended features is shown to be more effective",
            "paragraph_rank": 46,
            "section_rank": 31
        },
        {
            "text": "# of Hidden Layers x Nodes per Layer Basic Features Extended Features All Features",
            "paragraph_rank": 47,
            "section_rank": 32
        },
        {
            "text": "FIG. 11. ROC curves for a variety of different NeuroBGD configurations. Left: results from a 2 \u00d7 15 network trained using all features with different training sample sizes. Right: results from a 11 \u00d7 300 network trained on the full training sample using different inputs.",
            "paragraph_rank": 48,
            "section_rank": 33
        }
    ]
}