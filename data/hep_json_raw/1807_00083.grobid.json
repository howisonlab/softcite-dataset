{
    "level": "paragraph",
    "abstract": [
        {
            "text": "We show how an event topology classification based on deep learning could be used to improve the purity of data samples selected in real time at the Large Hadron Collider. We consider different data representations, on which different kinds of multi-class classifiers are trained. Both raw data and high-level features are utilized. In the considered examples, a filter based on the classifier's score can be trained to retain \u223c 99% of the interesting events and reduce the false-positive rate by more than one order of magnitude. By operating such a filter as part of the online event selection infrastructure of the LHC experiments, one could benefit from a more flexible and inclusive selection strategy while reducing the amount of downstream resources wasted in processing false positives. The saved resources could translate into a reduction of the detector operation cost or into an effective increase of storage and processing capabilities, which could be reinvested to extend the physics reach of the LHC experiments.",
            "paragraph_rank": 1,
            "section_rank": 1
        }
    ],
    "body_text": [
        {
            "text": "Introduction",
            "section_rank": 2
        },
        {
            "section": "Introduction",
            "text": "The CERN Large Hadron Collider (LHC) collides protons every 25 ns. Each collision can result in any of hundreds of physics processes. The total data volume exceeds by far what the experiments could record. This is why the incoming data flow is typically filtered through a set of rule-based algorithms, designed to retain only events with particular signatures (e.g., the presence of a high-energy particle of some kind). Such a system, commonly referred to as trigger, consists of hundreds of algorithms, each designed to accept events with a specific topology. The ATLAS [1] and CMS [2] trigger systems are based on this idea. In their current implementation, given the throughput capability and the typical event size, these two experiments can write on disk \u223c 1000 events/sec. A few processes, e.g., QCD multijet production, constitute the vast majority of the produced events. One is typically interested to select a fraction of these events for further studies. On the other hand, the main interest of the LHC experiments is related to selecting and studying the many rare processes which occur at the LHC. In a typical data flow, these events are overwhelmed by the large amount of QCD multijet events. The trigger system is put in place to make sure that the majority of these rare events are part of the stored \u223c 1000 events/sec.",
            "paragraph_rank": 2,
            "section_rank": 2,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b0",
                    "start": 573,
                    "text": "[1]",
                    "end": 576
                },
                {
                    "type": "bibr",
                    "ref_id": "b1",
                    "start": 585,
                    "text": "[2]",
                    "end": 588
                }
            ]
        },
        {
            "section": "Introduction",
            "text": "Trigger algorithms are typically designed to maximize the efficiency (i.e., the true-positive rate), resulting in a non-negligible false-positive rate and, consequently, in a substantial waste of resources Figure 1: Relative composition of the isolated-lepton sample after the acceptance requirement (left) and the trigger selection (right), as described in the text.",
            "paragraph_rank": 3,
            "section_rank": 2,
            "ref_spans": [
                {
                    "type": "figure",
                    "start": 206,
                    "text": "Figure 1",
                    "end": 214
                }
            ]
        },
        {
            "section": "Introduction",
            "text": "at trigger level (i.e., data throughput that could have been used for other purposes) and downstream (i.e., storage disk, processing power, etc.).",
            "paragraph_rank": 4,
            "section_rank": 2
        },
        {
            "section": "Introduction",
            "text": "The most commonly used selection rules are inclusive, i.e., more than one topology is selected by the same requirement. The so-called isolated lepton triggers are a typical example of this kind of algorithms. These triggers select events with a high-momentum electron or muon and no surrounding energetic particle, a typical signature of an interesting rare process, e.g., the production of a W boson decaying to a neutrino and an electron or muon. With such a requirement, one can simultaneously collect W bosons produced in the primary interaction (W events) or from the cascade decay of other particles, e.g., top quarks (mainly in tt events where a top quark-antiquark pair is produced). A sample selected this way is dominated by W events but it retains a substantial (> 10%) contamination from QCD multijet. The tt contribution is smaller than 1%. Events from tt production are sometimes triggered by a set of dedicated lepton+jets algorithms, capable of using looser requirements on the lepton at the cost of introducing requirements on jets. 2 Due to this additional complexity, the use of these triggers in a data analysis comes with additional complications. For instance, the applied jet requirements produce distortions on offline distributions of jet-related quantities. To avoid having this effect, any typical data analysis applies a tighter offline selection. This means that many of the selected events close to the online-selection threshold are discarded. This is not necessarily the most cost-effective way to retain an unbiased dataset for offline analysis.",
            "paragraph_rank": 5,
            "section_rank": 2,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b1",
                    "start": 1050,
                    "text": "2",
                    "end": 1051
                }
            ]
        },
        {
            "section": "Introduction",
            "text": "In this paper, we investigate the possibility of using machine learning to classify events based on their topologies, serving as an additional clean-up algorithm at the trigger level. Doing so, one could customize the trigger-selection strategy on individual processes (depending on the physics goals) while keeping the selection loose and simple. As a benchmark case, we consider a stream of data selected by requiring the presence of one electron or muon with transverse momentum p T > 23 GeV 3 and a loose requirement on the isolation. Details on the applied selection can be found in Sec. 2.",
            "paragraph_rank": 6,
            "section_rank": 2,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b2",
                    "start": 495,
                    "text": "3",
                    "end": 496
                }
            ]
        },
        {
            "section": "Introduction",
            "text": "The considered benchmark sample is dominated by direct W production, with a sizable contamination from QCD multijet events and a small contribution of tt events. Other interesting processes (e.g., W W , W Z, and ZZ production) are usually selected with more exclusive and dedicated trigger algorithms (e.g., di-muon or di-electron triggers), or share the same kinematic properties of the two main interesting processes (W and tt). For the sake of simplicity, we ignore these sub-leading processes in our study, without compromising the validity of our conclusions. Fig. 1 shows the composition of a sample with one electron or muon within the defined acceptance (p T > 22 GeV and pseudorapidity |\u03b7| = | \u2212 log[tan(\u03b8/2)]| < 2.6, where \u03b8 is the polar angle), before and after applying the trigger requirements (p T > 23 GeV and loose isolation).",
            "paragraph_rank": 7,
            "section_rank": 2,
            "ref_spans": [
                {
                    "type": "figure",
                    "start": 565,
                    "text": "Fig. 1",
                    "end": 571
                }
            ]
        },
        {
            "section": "Introduction",
            "text": "Such a loose set of requirements would translate into an event acceptance rate of \u223c 690 Hz for a luminosity of 2 \u00d7 10 34 cm \u22122 s \u22121 , well beyond the currently allocated budget for these triggers (typically \u223c 200 Hz). We suggest that, using the score of our topology classifier, one could tune the amount of each process to be stored for further analysis, within the boundaries of the allocated resources. For instance, one might be interested to retain all the tt events and some fraction of W events, while rejecting the QCD multijet events. We envision two main applications: for a given total rate, one could loosen the baseline trigger requirements, increasing the acceptance efficiency at no cost. Or, for a given acceptance efficiency (true positive rate), one could save resources by reducing the overall rate, rejecting the contribution of unwanted topologies (see Appendix A).",
            "paragraph_rank": 8,
            "section_rank": 2
        },
        {
            "section": "Introduction",
            "text": "We consider several topology classifiers based on deep learning model architectures: fully-connected deep neural networks (DNNs), convolutional neural networks (CNNs) [3], and recurrent neural networks such as Long-Short-Term-Memory networks (LSTMs) [4] and gated recurrent units (GRUs) [5]. We consider four different representations of the collision events: (i) a set of physics-motivated highlevel features, (ii) the raw image of the detector hits, (iii) a sequence of particles, characterized by a limited set of basic features (energy, direction, etc.), and (iv) an abstract representation of this list of particles as an image.",
            "paragraph_rank": 9,
            "section_rank": 2,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b2",
                    "start": 167,
                    "text": "[3]",
                    "end": 170
                },
                {
                    "type": "bibr",
                    "ref_id": "b3",
                    "start": 250,
                    "text": "[4]",
                    "end": 253
                },
                {
                    "type": "bibr",
                    "ref_id": "b4",
                    "start": 287,
                    "text": "[5]",
                    "end": 290
                }
            ]
        },
        {
            "section": "Introduction",
            "text": "The paper is structured as follows. In Sec. 2 we describe the four data representations. In Sec. 3 we describe the corresponding classification models. Results are discussed in Sec. 4. In Sec. 5 we investigate the generalization properties of the four classifiers to scenarios of other topologies. We study the robustness of our classifiers against Monte-Carlo simulation inaccuracy with pseudo-data in Sec. 6. In Sec. 7 we briefly discuss applications of machine learning algorithms to similar problems. Conclusions are given in Sec. 8. Appendix A describes a different scenario, in which the classifier is used to save resources by reducing the trigger acceptance rate, as opposed to using it to sustain a loose trigger selection that could otherwise require too many resources.",
            "paragraph_rank": 10,
            "section_rank": 2
        },
        {
            "text": "Dataset",
            "section_rank": 3
        },
        {
            "section": "Dataset",
            "text": "Synthetic data corresponding to W , tt and QCD multijet production topologies are generated with 10 5 events per process (3 \u2022 10 5 events in total) using the PYTHIA8 event generation library [6]. The setup of the proton-beam simulation is loosely inspired by the LHC running configuration in 2015-2016: two proton beams, each with 6.5 TeV, generate on average 20 proton-proton collisions per crossing following a Poisson distribution. Generated samples are processed with the DELPHES library [7], which applies a parametric model of a detector response. Detector performances is tuned to the CMS upgrade design foreseen for the High-Luminosity LHC [8], as implemented in the corresponding default card provided with DELPHES. We run the DELPHES particle-flow (PF) algorithm, which combines the information from all the CMS detector components to derive a list of reconstructed particles, the so-called PF candidates. For each particle, the algorithm returns the measured energy and flight direction. Each particle is associated to one of three classes: charged particles, photons, and neutral hadrons. Jets are clustered from the reconstructed PF candidates, using the FASTJET [9] implementation of the anti-k T jet algorithm [10], with jet-size parameter R = 0.4. The jet's b-tagging efficiency is parametrized as a function of jet's p T and \u03b7 in the default DELPHES CMS upgrade design card. The parametrized b-tagging efficiency is shown to provide a reasonable agreement with CMS [7].",
            "paragraph_rank": 11,
            "section_rank": 3,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b5",
                    "start": 191,
                    "text": "[6]",
                    "end": 194
                },
                {
                    "type": "bibr",
                    "ref_id": "b6",
                    "start": 492,
                    "text": "[7]",
                    "end": 495
                },
                {
                    "type": "bibr",
                    "ref_id": "b7",
                    "start": 648,
                    "text": "[8]",
                    "end": 651
                },
                {
                    "type": "bibr",
                    "ref_id": "b8",
                    "start": 1176,
                    "text": "[9]",
                    "end": 1179
                },
                {
                    "type": "bibr",
                    "ref_id": "b9",
                    "start": 1225,
                    "text": "[10]",
                    "end": 1229
                },
                {
                    "type": "bibr",
                    "ref_id": "b6",
                    "start": 1482,
                    "text": "[7]",
                    "end": 1485
                }
            ]
        },
        {
            "section": "Dataset",
            "text": "The basic event representation consists of a list of reconstructed PF candidates. For each candidate q, the following information is given: (i) The particle four-momentum in Cartesian coordinates (E, p x , p y , p z ); (ii) The particle three-momentum, computed from (i), in cylindrical coordinates: the transverse momentum p T , the pseudorapidity \u03b7, and the azimuthal angle \u03c6; (iii) The Cartesian coordinates (x vtx , y vtx , z vtx ) of the particle point of origin. For all neutral particles, (0, 0, 0) is used in the absence of pointing information; (iv) The electric charge; (v) The particle isolation with respect to charged particles (ChPFIso), photons (GammaPFIso), or neutral hadrons (NeuPFIso). For each particle class, the isolation is quantified as",
            "paragraph_rank": 12,
            "section_rank": 3
        },
        {
            "section": "Dataset",
            "text": "where the sum extends over all the particles of the appropriate class with angular distance \u2206R = (\u2206\u03b7) 2 + (\u2206\u03c6) 2 < 0.3 from the particle q.",
            "paragraph_rank": 13,
            "section_rank": 3
        },
        {
            "section": "Dataset",
            "text": "The particle identity is categorized via a one-hot-encoded representation (isChP ar, isN euHad, isGamma), corresponding to a charged particle, a neutral hadron, or a photon. In addition, two boolean flags are stored (isEle and isM u) to identify if a given particle is an electron or a muon. In total, each particle is then described by 19 features.",
            "paragraph_rank": 14,
            "section_rank": 3
        },
        {
            "section": "Dataset",
            "text": "The trigger selection is emulated by requiring all the events to include one isolated electron or muon with transverse momentum p T > 23 GeV and particle-based isolation ChISO + GammaISO + NeuISO < 0.45. This baseline selection, which follows the typical requirements of an inclusive single-lepton trigger algorithm, accepts \u2248 100 QCD multijet events and \u2248 176 W events for every tt event. Despite its large W and tt efficiency, this trigger selection comes with a large cost in terms of QCD multijet events written on disk and processed offline. The cost is even larger if the main physics target is tt events and the W contribution is seen as an additional source of background (e.g., in a high-statistics scenario, with all measurements of W properties limited in precision by systematic uncertainties).",
            "paragraph_rank": 15,
            "section_rank": 3
        },
        {
            "section": "Dataset",
            "text": "All particles are ranked in decreasing order of p T . For each event, the isolated lepton is the first entry of the list of particles. To avoid double counting of this isolated lepton as a charged particle, each charged particle q is required to have \u2206R(q, ) > 10 \u22124 . In addition to the isolated lepton, we consider the first 450 charged particles, the first 150 photons, and the first 200 neutral hadrons. This corresponds to a total of 801 particles per event, each characterized by the 19 features described above.",
            "paragraph_rank": 16,
            "section_rank": 3
        },
        {
            "section": "Dataset",
            "text": "The choice of the numbers of particles is made such that, on average, only 5% charged particles, 5% neutral hadrons and 1% photons are ignored. Thanks to p T ordering by particle category, what we remove carries small information. In early stages of this work we experimented with tighter cuts on particle multiplicity without observing substantial difference. We verified that the particles we ignore have typical p T below 1 GeV. If fewer particles are found in the event, zero padding is used to guarantee a fixed length of the particle list across different events. The events are then stored as NumPy arrays in a set of compressed HDF5 files. The dataset is planned to be released on the CERN OpenData portal, accessible at opendata.cern.ch.",
            "paragraph_rank": 17,
            "section_rank": 3
        },
        {
            "section": "Dataset",
            "text": "In addition to this raw-event representation, we provide a list of physics-motivated high-level features, computed from the full event (the HLF dataset):",
            "paragraph_rank": 18,
            "section_rank": 3
        },
        {
            "section": "Dataset",
            "text": "\u2022 The scalar sum, S T , of the p T of all the jets, leptons, and photons in the event with p T > 30 GeV and |\u03b7| < 2.6.",
            "paragraph_rank": 19,
            "section_rank": 3
        },
        {
            "section": "Dataset",
            "text": "\u2022 The missing transverse energy E miss T , defined as the absolute value of the missing transverse momentum, computed summing over the full list of reconstructed PF candidates:",
            "paragraph_rank": 20,
            "section_rank": 3
        },
        {
            "section": "Dataset",
            "text": "\u2022 The squared transverse mass, M 2 T , of the isolated lepton and the E miss T system, defined as:",
            "paragraph_rank": 21,
            "section_rank": 3
        },
        {
            "section": "Dataset",
            "text": "with p T the transverse momentum of the lepton and \u2206\u03c6 the azimuthal separation between the lepton and p miss T vector.",
            "paragraph_rank": 22,
            "section_rank": 3
        },
        {
            "section": "Dataset",
            "text": "\u2022 The azimuthal angle of the p miss T vector, \u03c6 miss . \u2022 The number of jets entering the S T sum.",
            "paragraph_rank": 23,
            "section_rank": 3
        },
        {
            "section": "Dataset",
            "text": "\u2022 The number of these jets identified as originating from a b quark.",
            "paragraph_rank": 24,
            "section_rank": 3
        },
        {
            "section": "Dataset",
            "text": "\u2022 The isolated-lepton momentum, expressed in polar coordinates (p T , \u03b7, \u03c6)",
            "paragraph_rank": 25,
            "section_rank": 3
        },
        {
            "section": "Dataset",
            "text": "\u2022 The three isolation quantities (ChPFIso, NeuPFIso, GammaPFIso) for the isolated lepton.",
            "paragraph_rank": 26,
            "section_rank": 3
        },
        {
            "section": "Dataset",
            "text": "\u2022 The lepton charge.",
            "paragraph_rank": 27,
            "section_rank": 3
        },
        {
            "section": "Dataset",
            "text": "\u2022 The isEle flag for the isolated lepton.",
            "paragraph_rank": 28,
            "section_rank": 3
        },
        {
            "section": "Dataset",
            "text": "The list of 801 particles is used to generate two visual representations of the events: raw representation and abstract representation. In the raw representation, the (\u03b7, \u03c6) plane corresponding to the detector acceptance is divided into a barrel region (|\u03b7| < 1.5), two end-cap regions (1.5 \u2264 \u03b7 < 3.0 and \u22123.0 < \u03b7 \u2264 \u22121.5), and two forward regions (3.0 \u2264 \u03b7 < 5.0 and \u22125.0 < \u03b7 \u2264 \u22123.0). The barrel and endcap regions of the electromagnetic calorimeter, as well as the endcap of the hadronic calorimeter (HCAL), are binned in cells of size 0.0187 \u00d7 0.0187. The barrel region of the HCAL is binned with cells of size 0.087 \u00d7 0.087. The forward regions are binned with cells of size 0.175 in \u03b7, while the dimension in \u03c6 varies from 0.175 to 0.35. Each cell is filled with the scalar sum of the p T of the particles pointing to that cell. The three classes of particles (charged particles, photons, and neutral hadrons) are considered separately, resulting in three channels. An example is shown in Fig. 2 for a tt event. This representation corresponds to the raw image recorded by the detector.  Recently, it was proposed to represent LHC collision events as abstract images where reconstructed physics objects (jets, in that case) are represented as geometric shapes whose size reflects the energy of the particle [11]. We generalize this abstract representation approach by applying it to the full list of particles. Each particle is represented as a unique geometric shape, centered at the particle's (\u03b7, \u03c6) coordinates and with size proportional to its log p T . The geometric shapes are chosen as follow: (i) pentagons for the selected isolated electron or muon; (ii) triangles for photons; (iii) squares for charged particles; (iv) hexagons for neutral hadrons. The images are digitized as arrays of size 5 \u00d7 150 \u00d7 94, where each of the first four channels contains a separated particle class, and the last channel contains the E miss T , represented as a circle. As an example, the abstract representation for the event in Fig. 2 is shown in Fig. 3.",
            "paragraph_rank": 29,
            "section_rank": 3,
            "ref_spans": [
                {
                    "type": "figure",
                    "ref_id": "fig_1",
                    "start": 992,
                    "text": "Fig. 2",
                    "end": 998
                },
                {
                    "type": "bibr",
                    "ref_id": "b10",
                    "start": 1310,
                    "text": "[11]",
                    "end": 1314
                },
                {
                    "type": "figure",
                    "ref_id": "fig_1",
                    "start": 2025,
                    "text": "Fig. 2",
                    "end": 2031
                },
                {
                    "type": "figure",
                    "ref_id": "fig_2",
                    "start": 2044,
                    "text": "Fig. 3",
                    "end": 2050
                }
            ]
        },
        {
            "section": "Dataset",
            "text": "This abstract representation allows mitigating the sparsity problem of the raw images. On the other hand, there is no guarantee that the physics information is fully retained in this translation. As a result, there could be a reduction of discrimination power. This is one of the points we aim to investigate in this study.",
            "paragraph_rank": 30,
            "section_rank": 3
        },
        {
            "text": "Model description",
            "section_rank": 4
        },
        {
            "section": "Model description",
            "text": "In this section, we describe five types of multi-class classifiers, trained on the four data representations described in the previous section. We start by considering a state-of-the art HEP application, based on the high-level features listed in Sec. 2. We then consider a convolutional neural network taking as input the raw images. This model offers the baseline point of comparison for the classifier using the abstract images. In order to have a fair comparison between the two approaches, the same kind of network architecture is used for the two sets of images. Next, we consider recurrent neural networks based on LSTMs and GRUs, trained directly on the lists of 801 particles. Finally, we consider a classifier taking both the high-level features and the list of 801 particles as inputs, using a combination of recurrent neural networks and fully connected neural networks.",
            "paragraph_rank": 31,
            "section_rank": 4
        },
        {
            "section": "Model description",
            "text": "The CNNs are implemented in PyTorch [12]. The recurrent neural networks and feed-forward neural networks are implemented in Keras [?] and trained using Theano [13] as a back-end. The Adam optimizer [14] is used to adapt the learning rate. The training is capped at 50 epochs, and can be stopped early if there is no improvement in terms of validation loss after 8 epochs. Categorical cross entropy is used as the loss function. All trainings are performed on a cluster of GeForce GTX 1080 GPUs. In an early stage of this work, experiments on the recurrent models were performed on the CSCS Piz Daint super computer, using the mpi-learn library [15] for multiple-GPU training.",
            "paragraph_rank": 32,
            "section_rank": 4,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b11",
                    "start": 36,
                    "text": "[12]",
                    "end": 40
                },
                {
                    "type": "bibr",
                    "ref_id": "b12",
                    "start": 159,
                    "text": "[13]",
                    "end": 163
                },
                {
                    "type": "bibr",
                    "ref_id": "b13",
                    "start": 198,
                    "text": "[14]",
                    "end": 202
                },
                {
                    "type": "bibr",
                    "ref_id": "b14",
                    "start": 644,
                    "text": "[15]",
                    "end": 648
                }
            ]
        },
        {
            "text": "High-level-feature classifier",
            "section_rank": 5
        },
        {
            "section": "High-level-feature classifier",
            "text": "A fully connected feed-forward DNN based on a set of high-level features (HLF classifier) is the closest approach to the currently used rule-based trigger algorithms. We train a model of this kind taking as input the 14 features contained in the HLF dataset (see Sec. 2). The 14 features are normalized to take values between 0 and 1.",
            "paragraph_rank": 33,
            "section_rank": 5
        },
        {
            "section": "High-level-feature classifier",
            "text": "The final network configuration is the result of an optimization process performed using the scikit-learn optimizer [16], which performs an exhaustive cross-validated grid-search over a set of hyperparameters related to the network architecture and the training setup. The number of layers, the number of nodes in each layer, and the choice of optimizer have been considered in the scan. For a given number of layers, discrimination performances were found to be constant over the considered range of number of nodes per layer. We believe that this is a direct consequence of the simple problem at hand: even a relatively small networks achieve good classification performances. We then took the smallest network as the best compromise between performance and architecture minimality.",
            "paragraph_rank": 34,
            "section_rank": 5,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b15",
                    "start": 116,
                    "text": "[16]",
                    "end": 120
                }
            ]
        },
        {
            "section": "High-level-feature classifier",
            "text": "The chosen architecture consists of three hidden layers with 50, 20, and 10 nodes, activated by rectified linear units (ReLU) [17]. The output layer consists of 3 nodes, activated by a softmax activation function.",
            "paragraph_rank": 35,
            "section_rank": 5,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b16",
                    "start": 126,
                    "text": "[17]",
                    "end": 130
                }
            ]
        },
        {
            "text": "Raw-image classifier",
            "section_rank": 6
        },
        {
            "section": "Raw-image classifier",
            "text": "To classify events represented as raw calorimeter images (raw-image classifier), we use DenseNet-121, a model based on the Densely Connected Convolutional Network [18]. The DenseNet-121 architecture includes 4 dense blocks, each of which contains 6, 12, 24, 16 dense layers, respectively. Each dense layer contains two 2D convolutional layers preceded by batch normalization layers. A dropout rate of 0.5 is applied after each dense layer. Between two subsequent dense blocks is a transition layer consisting of a batch normalization layer, a 2D convolutional layer, and an average pooling layer.",
            "paragraph_rank": 36,
            "section_rank": 6,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b17",
                    "start": 163,
                    "text": "[18]",
                    "end": 167
                }
            ]
        },
        {
            "text": "Abstract-image classifier",
            "section_rank": 7
        },
        {
            "section": "Abstract-image classifier",
            "text": "We use the same DenseNet-121 architecture above to classify the abstract image representation. We refer to this model as abstract-image classifier.",
            "paragraph_rank": 37,
            "section_rank": 7
        },
        {
            "text": "Particle-sequence classifier",
            "section_rank": 8
        },
        {
            "section": "Particle-sequence classifier",
            "text": "A particle-sequence classifier is trained using a recurrent network, taking as input the 801 candidates. To feed these particles into a recurrent network, particles are ordered according to their increasing or decreasing distance from the isolated lepton. Different physics-inspired metrics are considered to quantify the distance (\u2206R, \u2206\u03c6, \u2206\u03b7, k T [10], or anti-k T [19]). The best results are obtained using the \u2206R decreasing distance ordering.",
            "paragraph_rank": 38,
            "section_rank": 8,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b9",
                    "start": 348,
                    "text": "[10]",
                    "end": 352
                },
                {
                    "type": "bibr",
                    "ref_id": "b18",
                    "start": 366,
                    "text": "[19]",
                    "end": 370
                }
            ]
        },
        {
            "section": "Particle-sequence classifier",
            "text": "We use gated recurrent units (GRU) to aggregate the input sequence of particle flow candidate features into a fixed size encoding. The fixed encoding is fed into a fully connected layer with 3 softmax activated nodes. Input data is standardized so that each feature has zero mean and unit standard deviation. The zero-padded entries in the particle sequence are skipped with the Masking layer. The best internal width of the recurrent layers was found to be 50, determined by k-fold cross validation on a training set of 210,000 events. We also considered using long short-term memory networks (LSTM) to replace the GRU, but we found that the GRU architecture outperformed the LSTM architecture for the same number of internal cells. ",
            "paragraph_rank": 39,
            "section_rank": 8
        },
        {
            "text": "Inclusive classifier",
            "section_rank": 9
        },
        {
            "section": "Inclusive classifier",
            "text": "In order to inject some domain knowledge in the GRU classifier, we consider a modification of its architecture in which the 14 features of the HLF dataset are concatenated to the output of the GRU layer after some dropout (see Fig. 4). As for the other classifiers, the final output layer consists of 3 nodes, activated by a softmax activation function. We refer to this model as inclusive classifier.",
            "paragraph_rank": 40,
            "section_rank": 9,
            "ref_spans": [
                {
                    "type": "figure",
                    "ref_id": "fig_3",
                    "start": 227,
                    "text": "Fig. 4",
                    "end": 233
                }
            ]
        },
        {
            "text": "Results",
            "section_rank": 10
        },
        {
            "section": "Results",
            "text": "Each of the models presented in the previous section returns the probability of each event to be associated to a given topology: y QCD , y W , and y tt . By applying a threshold requirement on y W or y tt , one can define a W or a tt classifier, respectively. By changing the threshold value, one can build the corresponding receiver operating characteristic (ROC) curve. Fig. 5 shows the comparison of the ROC curves for five classifiers: the DenseNets based on raw images and abstract images, the GRU using the list of particles, the DNN using the HLFs, and the inclusive classifier using both the HLFs and the list of particles. Results for both a tt and W selectors are shown.  Acceptable results are obtained already with the raw-image classifier. On the other hand, the use of abstract images allows us to reach better performances. A further improvement is observed for those models not using an image-based representation of the event. The fact that the HLF selectors perform so well doesn't come as a surprise, given a considerable amount of physics knowledge implicitly provided by the choice of the relevant features. On the other hand, the fact that the particle-sequence classifier reaches better performances compared to the HLF selector is remarkable, as is the further improvement observed by merging the two approaches in the inclusive classifier. In some sense, the GRU layer is gaining a good part of the physics intuition that motivated the choice of the HLF quantities, but not entirely. Fig. 6 shows the Pearson correlation coefficients between the GRU scores (y tt and y W ) and the HLF quantities. As one would expect, y tt exhibits a stronger correlation with those features that quantify jet activity (n jets in Fig. 6), as well as with the b-jet multiplicity (n b-jets ). On the contrary, W events shows an anti-correlation with respect to jet quantities, since the production of associated jets in W events is much more penalized than for tt events. As expected, both scores are anti-correlated to the isolation quantities, which takes larger values for non-isolated leptons.  The trigger baseline selection we use in this study, looser than what is used nowadays in CMS, gives an overall trigger rate (i.e., summing electron and muon events) of \u223c 690 Hz, more than a factor two larger than what is currently allocated. Using the 99% working points of the two classifiers, one would reduce the overall rate to \u223c 270 Hz (counting the overlap between the two triggers). This would be comparable to what is currently allocated for these triggers, but with a looser selection, i.e., with a less severe bias on the offline analysis. In addition, the trigger efficiency (the TPR) is so high that the bias imposed on offline quantities is quite minimal. This is illustrated in Fig. 7, where the dependence of the TPR on the most relevant HLF quantities is shown. In our experience, any rule-based algorithm with the same target trigger rate would result in larger inefficiencies at small values of at least some of these quantities, e.g., the lepton p T . One should also consider that the principle of a topology classifier could be generalized to other physics cases, as well as to other uses (e.g., labels for fast reprocessing or access to specific subsets of the triggered samples). Figure 8 shows the TPR and FPR of the inclusive tt selector when applying the 99% TPR workingpoint threshold, as a function of the number of vertices in the event, which quantifies the amount of pileup. The TPR is fairly insensitive to PU until P U \u223c 35, (the average PU recorded by the LHC in 2018), where the TPR drops to 97%. At the same time, the FPR increases mildly, resulting in a rate increase from \u223c 34 Hz (at the average PU value \u223c 20) to \u223c 48 Hz at P U \u223c 35. In other words, the algorithm trained on 2016 conditions would have been sustainable until 2018 with \u223c 15% rate increase (with respect to the average value) or it would have required a threshold adjustment along the way, a pretty standard operation when designing a trigger menu at the beginning of the year. We believe that, in view of these facts, the proposed algorithm would be as robust as many state-of-the-art algorithms operated at the LHC experiments.",
            "paragraph_rank": 41,
            "section_rank": 10,
            "ref_spans": [
                {
                    "type": "figure",
                    "ref_id": "fig_5",
                    "start": 372,
                    "text": "Fig. 5",
                    "end": 378
                },
                {
                    "type": "figure",
                    "ref_id": "fig_7",
                    "start": 1509,
                    "text": "Fig. 6",
                    "end": 1515
                },
                {
                    "type": "figure",
                    "ref_id": "fig_7",
                    "start": 1738,
                    "text": "Fig. 6",
                    "end": 1744
                },
                {
                    "type": "figure",
                    "start": 2798,
                    "text": "Fig. 7",
                    "end": 2804
                },
                {
                    "type": "figure",
                    "start": 3309,
                    "text": "Figure 8",
                    "end": 3317
                }
            ]
        },
        {
            "text": "ST E miss",
            "section_rank": 11
        },
        {
            "text": "Impact on other topologies",
            "section_rank": 12
        },
        {
            "section": "Impact on other topologies",
            "text": "While reducing the resource consumption of standard physics analyses is the main motivation behind this study, it is important to evaluate the impact of the proposed classifiers on other kind of topologies. For this purpose, we consider a handful of beyond-the-standard-model (BSM) scenarios, and we compute the TPR as a function of the most relevant kinematic quantities, similar to what was done in Fig. 7 for the standard topologies.",
            "paragraph_rank": 42,
            "section_rank": 12,
            "ref_spans": [
                {
                    "type": "figure",
                    "start": 401,
                    "text": "Fig. 7",
                    "end": 407
                }
            ]
        },
        {
            "section": "Impact on other topologies",
            "text": "We consider the following BSM processes: where H 0 is the 125 GeV Higgs boson, which we force to decay to a bottom quark-antiquark pair. This model, introduced in Ref. [20], generates a 2b2W topology similar to that given by tt events.",
            "paragraph_rank": 43,
            "section_rank": 12,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b19",
                    "start": 168,
                    "text": "[20]",
                    "end": 172
                }
            ]
        },
        {
            "section": "Impact on other topologies",
            "text": "\u2022 High-mass A \u2192 H + W : a high-mass variation of the previous model, in which the A and H + masses are set to 1025 GeV and 625 GeV, respectively.",
            "paragraph_rank": 44,
            "section_rank": 12
        },
        {
            "section": "Impact on other topologies",
            "text": "\u2022 A \u2192 4 : a light neutral scalar particle A with mass 20 GeV, decaying to two neutral scalars of 5 GeV each, both decaying to muon pairs, for a total of four muons in the final state. \u2022 W resonance with mass 300 GeV, decaying inclusively with W -like couplings.   Figure 10: Distributions of the validation sample and pseudo-data. The pseudo-data is created by adding a Gaussian noise of mean zero and standard deviation of 10% to the validation sample's particle momenta. The high-level features are then recomputed with the new list of particles. \u2022 Z resonance with mass 600 GeV, decaying to a pair of electrons or muons.",
            "paragraph_rank": 45,
            "section_rank": 12,
            "ref_spans": [
                {
                    "type": "figure",
                    "start": 264,
                    "text": "Figure 10",
                    "end": 273
                }
            ]
        },
        {
            "section": "Impact on other topologies",
            "text": "These events are filtered with the baseline selection described in Sec. 2.",
            "paragraph_rank": 46,
            "section_rank": 12
        },
        {
            "section": "Impact on other topologies",
            "text": "For each of these models, we consider the inclusive classifier and apply the 99%-TPR thresholds on y tt and y W . We then consider the fraction of events passing at least one of the two selectors. Results are shown in Fig. 9 for the most relevant kinematic quantities. While the individual selectors might show local inefficiencies, the combination of the two trigger paths is perfectly capable of retaining any event with features different from that of a QCD multijet event. In this respect, the logical OR of our two exclusive topology classifiers is robust enough to also select a large spectrum of BSM topologies. On the other hand, one cannot guarantee that QCD-like topologies (e.g., a dark photon produced in jet showers and decaying to lepton pairs) would not be rejected, a limitation which also affects traditional inclusive trigger strategies.",
            "paragraph_rank": 47,
            "section_rank": 12,
            "ref_spans": [
                {
                    "type": "figure",
                    "ref_id": "fig_10",
                    "start": 218,
                    "text": "Fig. 9",
                    "end": 224
                }
            ]
        },
        {
            "text": "Robustness study",
            "section_rank": 13
        },
        {
            "section": "Robustness study",
            "text": "As the classifier is trained on Monte-Carlo simulation samples, one needs to consider the discrepancy between Monte-Carlo and real data when deploying the classifier in the trigger. We investigate the robustness of our topology classifiers against this discrepancy by creating a pseudo-data sample, which attempts to emulate real data by adding a Gaussian noise to the particles' momenta in the simulation samples. The Gaussian noise has mean of zero and standard deviation of 10% of the variable's values being applied. Fig. 10 shows some comparisons between the Monte-Carlo samples and the pseudo-data with this Gaussian noise added.",
            "paragraph_rank": 48,
            "section_rank": 13,
            "ref_spans": [
                {
                    "type": "figure",
                    "start": 521,
                    "text": "Fig. 10",
                    "end": 528
                }
            ]
        },
        {
            "section": "Robustness study",
            "text": "We evaluate the performance of our fully-trained inclusive classifier on the new pseudo-data. Tab. 2 shows a slight reduction of signal efficiency: at the same background contamination rate of 5.2%, the signal efficiency reduces by only 1.4%. This demonstrates that our classifiers can be robust against some augmentation that mimics the discrepancy between data and Monte-Carlo simulation. A comprehensive study on full simulation and data in proper control regions would be needed when deploying this classifier into production.",
            "paragraph_rank": 49,
            "section_rank": 13
        },
        {
            "text": "Related works",
            "section_rank": 14
        },
        {
            "section": "Related works",
            "text": "Machine learning is traditionally used in high-energy physics as part of data analysis, and was an important ingredient to the discovery of the Higgs boson, as discussed in [21]. Several classification algorithms have been studied in the context of LHC physics application, notably for jet tagging [22,23,24,25,26,27,28,29] and event topology identification [20,30,11] using feed-forward neural networks, convolutional neural networks or physics-inspired architectures. Lists of particles have been used to define jet and event classifiers starting from a list of reconstructed particle momenta [31,32,33]. These studies typically consider data analysis as the main use case, focusing on small FPR selections. This is the main difference with respect to this study, which focuses on the optimization of real-time data-taking procedure.",
            "paragraph_rank": 50,
            "section_rank": 14,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b20",
                    "start": 173,
                    "text": "[21]",
                    "end": 177
                },
                {
                    "type": "bibr",
                    "ref_id": "b21",
                    "start": 298,
                    "text": "[22,",
                    "end": 302
                },
                {
                    "type": "bibr",
                    "ref_id": "b22",
                    "start": 302,
                    "text": "23,",
                    "end": 305
                },
                {
                    "type": "bibr",
                    "ref_id": "b23",
                    "start": 305,
                    "text": "24,",
                    "end": 308
                },
                {
                    "type": "bibr",
                    "ref_id": "b24",
                    "start": 308,
                    "text": "25,",
                    "end": 311
                },
                {
                    "type": "bibr",
                    "ref_id": "b25",
                    "start": 311,
                    "text": "26,",
                    "end": 314
                },
                {
                    "type": "bibr",
                    "ref_id": "b26",
                    "start": 314,
                    "text": "27,",
                    "end": 317
                },
                {
                    "type": "bibr",
                    "ref_id": "b27",
                    "start": 317,
                    "text": "28,",
                    "end": 320
                },
                {
                    "type": "bibr",
                    "ref_id": "b28",
                    "start": 320,
                    "text": "29]",
                    "end": 323
                },
                {
                    "type": "bibr",
                    "ref_id": "b19",
                    "start": 358,
                    "text": "[20,",
                    "end": 362
                },
                {
                    "type": "bibr",
                    "ref_id": "b29",
                    "start": 362,
                    "text": "30,",
                    "end": 365
                },
                {
                    "type": "bibr",
                    "ref_id": "b10",
                    "start": 365,
                    "text": "11]",
                    "end": 368
                },
                {
                    "type": "bibr",
                    "ref_id": "b30",
                    "start": 595,
                    "text": "[31,",
                    "end": 599
                },
                {
                    "type": "bibr",
                    "ref_id": "b31",
                    "start": 599,
                    "text": "32,",
                    "end": 602
                },
                {
                    "type": "bibr",
                    "ref_id": "b32",
                    "start": 602,
                    "text": "33]",
                    "end": 605
                }
            ]
        },
        {
            "section": "Related works",
            "text": "In parallel, machine learning techniques have also been used in online event selection. For example, the LHCb experiment used a decision-tree based approach for the high-level trigger in the first LHC run [34] and re-optimized it with MatrixNet algorithm for Run II [35]; ATLAS uses BDT in its multi-step tau trigger for Run II [36]; a BDT was also deployed on FPGA cards of the hardware-level trigger of the CMS experiment [37]. These triggers are mainly based on high-level features related to specific parts of a collision event. We propose instead to define an algorithm that is based on a raw-event representation and considers the full event collision at once. To our knowledge, this is the first demonstration of how a recurrent neural network could perform a successful inference on a full event and improve topology identification based on object-specific features.",
            "paragraph_rank": 51,
            "section_rank": 14,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b33",
                    "start": 205,
                    "text": "[34]",
                    "end": 209
                },
                {
                    "type": "bibr",
                    "ref_id": "b34",
                    "start": 266,
                    "text": "[35]",
                    "end": 270
                },
                {
                    "type": "bibr",
                    "ref_id": "b35",
                    "start": 328,
                    "text": "[36]",
                    "end": 332
                },
                {
                    "type": "bibr",
                    "ref_id": "b36",
                    "start": 424,
                    "text": "[37]",
                    "end": 428
                }
            ]
        },
        {
            "section": "Related works",
            "text": "In addition, traditional triggers based on machine learning run in tagging mode, i.e., are used to identify certain types of particles. Instead, we propose to use our topology classifier in veto mode: the trigger algorithm running downstream would be a classic trigger with loose selection, which would normally be unsustainable due to high throughput. The topology classifier would subsequently remove a majority of background events, sustaining the trigger rate and saving downstream computing resources.",
            "paragraph_rank": 52,
            "section_rank": 14
        },
        {
            "section": "Related works",
            "text": "Note. After submitting this paper for review, the study presented in Ref. [38] showed how a topology classification based on full event information can boost tagging efficiency or purity of a single-object trigger, or both, in the context of an offline analysis.",
            "paragraph_rank": 53,
            "section_rank": 14,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b37",
                    "start": 74,
                    "text": "[38]",
                    "end": 78
                }
            ]
        },
        {
            "text": "Conclusions",
            "section_rank": 15
        },
        {
            "section": "Conclusions",
            "text": "We show how deep neural networks can be used to train topology classifiers for LHC collision events, which could be used as a cleanup filter to select or reject specific event topologies in a trigger system. We consider several network architectures, applied to different representations of the same collision datasets.",
            "paragraph_rank": 54,
            "section_rank": 15
        },
        {
            "section": "Conclusions",
            "text": "The best results are obtained by combining a set of physics-motivated high-level features with the output of a GRU unit applied to a list of particle-level features. For the most difficult case, i.e., selecting rare tt events, we show how a trigger based on this concept would retain 99% of the tt events while reducing the FPR by more than \u223c 10 times.",
            "paragraph_rank": 55,
            "section_rank": 15
        },
        {
            "section": "Conclusions",
            "text": "The information given as input to the GRU, the abstract-image CNN and the raw-image CNN is the same, but coded differently. The difference in performance is then a combination of two effects: the encoding of this information in the input event representation and the way the network architecture exploits it. The DNN case is different. The DNN uses in principle less information. On the other hand, the list of HLFs given as input to the DNN is based on domain knowledge that the other networks have to learn by themselves. This is why the DNN model is very competitive despite using less information and why the inclusive classifier (GRU+DNN) improves on the GRU-based particle sequence classifier. Nevertheless, it is remarkable that the score of the particle sequence classifier learns interesting correlation patterns with the HLF features, showing that (to some extent) the GRU is learning some of this domain knowledge.",
            "paragraph_rank": 56,
            "section_rank": 15
        },
        {
            "section": "Conclusions",
            "text": "We show that such a trigger would have a minimal impact on the main kinematic features of the event topologies under consideration. The effect of operating this topology classifier as a final filter of a given single-lepton trigger would result in small decrease of trigger efficiency by few percentage (depending on the TPR of the chosen working point). On the other hand, such a filter would allow for a looser selection, efficiently including non-isolated leptons with low p T without downstream consequences in terms of computational power and storage. In addition, the logic OR of the tt and W selections would also catch a broad class of new-physics topologies, on which the classifiers were not trained.",
            "paragraph_rank": 57,
            "section_rank": 15
        },
        {
            "section": "Conclusions",
            "text": "The advantages of running these types of algorithms comes at the cost of computational resources to train the models. In our case, a single training of the inclusive classifier took 4 hours on a cluster consisting of 6 GeForce GTX 1080 GPUs. Building a cluster of a few tens of GPUs of this kind, to be used as a training facility, is well within the budget of big-experiment computing projects. For this reason, dedicated studies are ongoing to integrate train-on-demand services in the computing infrastructures of LHC experiments [15]  [39]. In view of the challenging trigger environment foreseen for the High-Luminosity LHC, it would be important to test this trigger strategy as a way to preserve a good experimental reach with a substantial reduction of computational resources. In this respect, we look forward to the LHC Run III as an opportunity to experiment with this technique using full simulation and study its impacts on real-time event selection.",
            "paragraph_rank": 58,
            "section_rank": 15,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b14",
                    "start": 533,
                    "text": "[15]",
                    "end": 537
                },
                {
                    "type": "bibr",
                    "ref_id": "b38",
                    "start": 539,
                    "text": "[39]",
                    "end": 543
                }
            ]
        },
        {
            "text": "g e d T r a c k s Ne u t r a l Ha d r o n s",
            "paragraph_rank": 59,
            "section_rank": 16
        },
        {
            "text": "Figure 2 :",
            "section_rank": 17
        },
        {
            "section": "Figure 2 :",
            "text": "Figure 2: An example of a tt event as the input of the raw-image classifier. Vertical and horizontal axess are the \u03c6 and \u03b7 coordinates, respectively, of the sub-detectors.",
            "paragraph_rank": 60,
            "section_rank": 17
        },
        {
            "text": "Figure 3 :",
            "section_rank": 18
        },
        {
            "section": "Figure 3 :",
            "text": "Figure 3: Example of a tt event, represented as a 5-channel abstract images of photons (top-left), charged hadrons (top-center), neutral hadrons (top-right), the isolated lepton (bottom-kleft), and the event E miss T (botton-right).",
            "paragraph_rank": 61,
            "section_rank": 18
        },
        {
            "text": "Figure 4 :",
            "section_rank": 19
        },
        {
            "section": "Figure 4 :",
            "text": "Figure 4: Network architecture of the inclusive classifier.",
            "paragraph_rank": 62,
            "section_rank": 19
        },
        {
            "text": "classifier (AUC): 0.9973 Particle-sequence classifier (AUC): 0.9952 HLF classifier (AUC): 0.9861 Abstract-image classifier (AUC): 0.9681 Raw-image classifier (AUC): 0classifier (AUC): 0.9957 Particle-sequence classifier (AUC): 0.9920 HLF classifier (AUC): 0.9799 Abstract-image classifier (AUC): 0.9287 Raw-image classifier (AUC): 0.8036",
            "paragraph_rank": 63,
            "section_rank": 20
        },
        {
            "text": "Figure 5 :",
            "section_rank": 21
        },
        {
            "section": "Figure 5 :",
            "text": "Figure 5: ROC curves for the tt (left) and W (right) selectors described in the paper.",
            "paragraph_rank": 64,
            "section_rank": 21
        },
        {
            "text": "Figure 6 :",
            "section_rank": 22
        },
        {
            "section": "Figure 6 :",
            "text": "Figure 6: Pearson correlation coefficients between the y tt (left) and y W (right) scores of the Particlesequence classifier and the 14 quantities of the HLF dataset.The performance of each of the five classifiers is summarized in Tab. 1 in terms of false-positive rate (FPR) and trigger rate (TR) as a function of the true-positive rate (TPR). The best QCD rejection is obtained by the inclusive classifier, which can retain 99% of the tt or W events with a false-positive rate of \u223c 5.2%.",
            "paragraph_rank": 65,
            "section_rank": 22
        },
        {
            "text": "Figure 7 :Figure 8 :",
            "section_rank": 23
        },
        {
            "section": "Figure 7 :Figure 8 :",
            "text": "Figure 7: Selection efficiency using 99% TPR working point as functions of lepton p T , M 2T , and E miss T for the tt selector on tt events (top) and the W selector on W events (bottom).",
            "paragraph_rank": 66,
            "section_rank": 23
        },
        {
            "text": "Figure 9 :",
            "section_rank": 24
        },
        {
            "section": "Figure 9 :",
            "text": "Figure 9: Selection efficiencies of different BSM models using 99% TPR working point as functions of lepton p T , M 2 T , and E miss T . From top to bottom, A \u2192 H + W \u2212 , High-mass A \u2192 H + W \u2212 , A \u2192 4 , W , and Z .",
            "paragraph_rank": 67,
            "section_rank": 24
        },
        {
            "text": "Table 1 :",
            "section_rank": 25
        },
        {
            "section": "Table 1 :",
            "text": "False positive rate (FPR) and trigger rate (TR) at different values of the true positive rate (TPR), for a tt (top) and W selector. Rate values are estimated scaling the TPR and processdependent FPR values by the acceptance and efficiency, assuming a leading-order (LO) production cross section and luminosity of 2\u00d710 34 cm \u22122 s \u22121 . TR values should be taken only as suggestions of the actual rates, since the accuracy is limited by the use of LO cross sections and a parametric detector simulation. Hz 462.3 \u00b1 0.5 Hz 301.9 \u00b1 0.6 Hz 268.2 \u00b1 0.5 Hz 259.7 \u00b1 0.4 Hz TR @95% TPR 454.5 \u00b1 0.6 Hz 365.1 \u00b1 0.8 Hz 259.2 \u00b1 0.5 Hz 242.6 \u00b1 0.4 Hz 238.0 \u00b1 0.4 Hz TR @90% TPR 408.2 \u00b1 0.8 Hz 301.8 \u00b1 0.8 Hz 235.0 \u00b1 0.5 Hz 225.4 \u00b1 0.5 Hz 223.3 \u00b1 0.5 Hz",
            "paragraph_rank": 68,
            "section_rank": 25
        },
        {
            "text": "Table 2 :",
            "section_rank": 26
        },
        {
            "section": "Table 2 :",
            "text": "Signal efficiency (TPR) at different values of the false positive rate (FPR) for the inclusive classifier selecting tt evaluated on the validation sample and the pseudo-data.",
            "paragraph_rank": 69,
            "section_rank": 26
        },
        {
            "section": "Table 2 :",
            "text": "A jet is a spray of hadrons, typically originating from the hadronization of gluons and quarks produced in the proton collisions.3 In this paper, we set units in such a way that c = = 1.",
            "paragraph_rank": 70,
            "section_rank": 26,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b2",
                    "start": 129,
                    "text": "3",
                    "end": 130
                }
            ]
        },
        {
            "text": "Acknowledgments",
            "section_rank": 28
        },
        {
            "section": "Acknowledgments",
            "text": "This work is supported by grants from the Swiss National Supercomputing Center (CSCS) under project ID d59, the United States Department of Energy, Office of High Energy Physics Research under Caltech Contract No. de-sc0011925, and the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation program (grant agreement n o 772369). T.N. would like to thank Duc Le for valuable discussions during the earlier stage of this project. We thank CERN OpenLab for supporting D.W. during his internship at CERN. We are grateful to Caltech and the Kavli Foundation for their support of undergraduate student research in cross-cutting areas of machine learning and domain sciences. Part of this work was conducted at \"iBanks\", the AI GPU cluster at Caltech. We acknowledge NVIDIA, SuperMicro and the Kavli Foundation for their support of \"iBanks\".",
            "paragraph_rank": 71,
            "section_rank": 28
        },
        {
            "text": "Appendix A An alternative use case",
            "section_rank": 30
        },
        {
            "section": "Appendix A An alternative use case",
            "text": "In this paper, we showed how one could use a topology classifier to keep the overall trigger rate under control while operating triggers with otherwise unsustainable loose selections. In this appendix we discuss how topology classifiers could be used to save resources for a pre-defined baseline trigger selection by rejecting events associated to unwanted topologies. In this case, the main goal is not to reduce the impact of the online selection. Instead, we focus on reducing resource consumption downstream for a given trigger selection.",
            "paragraph_rank": 72,
            "section_rank": 30
        },
        {
            "section": "Appendix A An alternative use case",
            "text": "To this purpose, we consider a copy of the dataset described in Sec. 2, obtained tightening the p T threshold from 23 to 25 GeV and the isolation requirement from ISO < 0.45 to ISO < 0.20. Doing so, the sample composition changes as follow: 7.5% QCD; 92% W ; 0.5% tt. With such selections, the trigger acceptance rate would decrease from 690 Hz to 390 Hz, closer to what is currently allocated for these triggers in the CMS experiment.",
            "paragraph_rank": 73,
            "section_rank": 30
        },
        {
            "section": "Appendix A An alternative use case",
            "text": "Following the procedure described in Sec. 3 and 4, we train the same topology classifiers on this dataset. The corresponding ROC curves are presented in Fig. 11 for a tt and a W selector.  We then define a set of trigger filters applying a lower threshold to the normalized score of the classifier, choosing the threshold value that corresponds to a certain TPR value. The result is presented in Table 3, in terms of the FPR and the trigger rate.",
            "paragraph_rank": 74,
            "section_rank": 30,
            "ref_spans": [
                {
                    "type": "figure",
                    "start": 153,
                    "text": "Fig. 11",
                    "end": 160
                },
                {
                    "type": "table",
                    "start": 396,
                    "text": "Table 3",
                    "end": 403
                }
            ]
        },
        {
            "section": "Appendix A An alternative use case",
            "text": "The trigger baseline selection we use in this study, close to what is used nowadays in CMS for muons, gives an overall trigger rate (i.e., summing electron and muon events) of \u223c 390 Hz (i.e., 190 Hz per lepton flavor). If one was willing to take (as an example) half the W events and all the tt events, this number could be reduced to \u223c 200 Hz using the inclusive selectors presented in this study (taking into account the partial overlap between the two triggers). A more classic approach would consist in prescaling the isolated lepton triggers, i.e. randomly accepting half of the events. The effect on W events would be the same, but one would lose half of the tt events while still writing 15 times more QCD than tt events. In this respect, the strategy we propose would allow a more flexible and cost-effective strategy. ",
            "paragraph_rank": 75,
            "section_rank": 30
        }
    ]
}