{
    "level": "paragraph",
    "abstract": [
        {
            "text": "The strong coupling constant is one of the fundamental parameters of the standard model of particle physics. In this review I will briefly summarise the theoretical framework, within which the strong coupling constant is defined and how it is connected to measurable observables. Then I will give an historical overview of its experimental determinations and discuss the current status and world average value. Among the many different techniques used to determine this coupling constant in the context of quantum chromodynamics, I will focus in particular on a number of measurements carried out at the Large Electron Positron Collider (LEP) and the Large Hadron Collider (LHC) at CERN.",
            "paragraph_rank": 1,
            "section_rank": 1
        }
    ],
    "body_text": [
        {
            "text": "Introduction",
            "section_rank": 2
        },
        {
            "section": "Introduction",
            "text": "The strong coupling constant, \u03b1 s , is the only free parameter of the lagrangian of quantum chromodynamics (QCD), the theory of strong interactions, if we consider the quark masses as fixed. As such, this coupling constant, or equivalently g s = \u221a 4\u03c0\u03b1 s , is one of the three fundamental coupling constants of the standard model (SM) of particle physics. It is related to the SU(3) C colour part of the overall SU(3) C \u00d7 SU(2) L \u00d7 U(1) Y gauge symmetry of the SM. The other two constants g and g indicate the coupling strengths relevant for weak isospin and weak hypercharge, and can be rewritten in terms of the Weinberg mixing angle tan \u03b8 W = g /g and the fine-structure constant \u03b1 = e 2 /(4\u03c0), where the electric charge is given by e = g sin \u03b8 W . Note that natural units are used throughout.",
            "paragraph_rank": 2,
            "section_rank": 2
        },
        {
            "section": "Introduction",
            "text": "While typically denoted as constants, actually all these coupling strengths vary as a function of the energy scale or momentum transfer Q of the particular process looked at, as will be discussed later. In contrast to \u03b1(Q 2 ), which increases with increasing Q, the strong coupling \u03b1 s (Q 2 ) decreases for increasing scale, leading to the famous property of QCD known as asymptotic freedom. It is interesting to compare the values of these two coupling strengths at some fixed scale, such as the mass of the Z boson, Q \u2248 M z \u2248 91 GeV. We find that \u03b1(M 2 z ) \u2248 1/128, whereas \u03b1 s (M 2 z ) \u2248 0.12; that is, the strong coupling is still about 15 times larger than the fine-structure constant at energy scales much larger than those relevant for quark confinement into hadrons (Q \u223c 1 GeV). Thus, strong interactions are indeed strong compared to electroweak interactions, even at large energy scales such as those probed by CERN's past and present colliders, in particular the Large Electron Positron Collider (LEP) or the Large Hadron Collider (LHC).",
            "paragraph_rank": 3,
            "section_rank": 2
        },
        {
            "section": "Introduction",
            "text": "The different energy dependence of the coupling strengths triggers the immediate question if and at which exact energy scale these coupling constants become of equal strength, implying the onset of a possible grand unification.",
            "paragraph_rank": 4,
            "section_rank": 2
        },
        {
            "section": "Introduction",
            "text": "Obviously, the answer to this question also depends on the precision at which \u03b1 and \u03b1 s have been determined by experiment, and it is instructive to realize that today \u03b1(Q 2 = 0) \u2248 1/137 is known at an accuracy of 32 parts per billion [1], whereas the relative uncertainty of the current world average (WA) value [2] of \u03b1 s (M 2 z ) = 0.1185 \u00b1 0.0006 amounts to half a percent; quite an astonishing difference.",
            "paragraph_rank": 5,
            "section_rank": 2,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b0",
                    "start": 313,
                    "text": "[2]",
                    "end": 316
                }
            ]
        },
        {
            "section": "Introduction",
            "text": "Besides the wish to improve the accuracy of the aforementioned very high energy extrapolation, it is of general importance to know \u03b1 s at the best possible precision, since it enters the calculation of each and every process that involves strong interactions and thus ultimately limits the precision at which such processes can be predicted theoretically. As a most recent and prominent example, it is worth mentioning that the uncertainty on \u03b1 s gives a non-negligible contribution to the overall theoretical uncertainty on Higgs boson production at the LHC [3]. Correspondingly, this limits the studies looking for effects beyond the SM that could manifest themselves through deviations of the measured Higgs production cross sections from their theoretical predictions. In the following I will indicate the experimental and theoretical difficulties that limit the precision at which we know this fundamental parameter, but also highlight the dramatic improvements, which have been achieved during the last three decades.",
            "paragraph_rank": 6,
            "section_rank": 2,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b1",
                    "start": 559,
                    "text": "[3]",
                    "end": 562
                }
            ]
        },
        {
            "text": "Theoretical framework",
            "section_rank": 3
        },
        {
            "section": "Theoretical framework",
            "text": "The basic elements of QCD, including a discussion of the scale dependence of \u03b1 s (Q 2 ) and the related structure of theoretical predictions obtained in perturbation theory, are summarized elsewhere in this series of reviews [4]. Further extensive descriptions of the theoretical framework can be found in, e.g., Refs. [2,5,6]. Here I will only highlight a few important aspects of perturbative QCD (pQCD), that are relevant for the remainder of this review.",
            "paragraph_rank": 7,
            "section_rank": 3,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b2",
                    "start": 225,
                    "text": "[4]",
                    "end": 228
                },
                {
                    "type": "bibr",
                    "ref_id": "b0",
                    "start": 319,
                    "text": "[2,",
                    "end": 322
                },
                {
                    "type": "bibr",
                    "ref_id": "b3",
                    "start": 322,
                    "text": "5,",
                    "end": 324
                },
                {
                    "type": "bibr",
                    "ref_id": "b4",
                    "start": 324,
                    "text": "6]",
                    "end": 326
                }
            ]
        },
        {
            "section": "Theoretical framework",
            "text": "When calculating amplitudes corresponding to Feynman graphs that involve loop diagrams, ultraviolet divergences are encountered. The procedure of renormalization absorbs these divergences into a redefinition of the bare parameters and fields that appear in the lagrangian. In particular, this leads to the renormalised or so-called running coupling constant \u03b1 s (\u00b5 2 ), a function of the (unphysical) renormalization scale \u00b5. If \u00b5 is chosen close to the scale of the momentum transfer Q in a given process, then \u03b1 s (\u00b5 2 \u2248 Q 2 ) is indicative of the effective strength of the strong interaction in that process [2]. This explains why in the literature we often find a discussion of the running coupling constant as function of the physical scale Q, while the renormalized coupling actually is a function of the unphysical scale \u00b5. This will also become clearer from the following discussion of the structure of perturbative predictions.",
            "paragraph_rank": 8,
            "section_rank": 3,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b0",
                    "start": 611,
                    "text": "[2]",
                    "end": 614
                }
            ]
        },
        {
            "section": "Theoretical framework",
            "text": "While the value of \u03b1 s (\u00b5 2 ) at a fixed scale \u00b5 can not be predicted and has to be determined from experiment instead, its \u00b5 dependence is given by the renormalization group equation,",
            "paragraph_rank": 9,
            "section_rank": 3
        },
        {
            "section": "Theoretical framework",
            "text": "The first two coefficients in the perturbative expansion of the so-called \u03b2function of QCD are b 0 = (33 \u2212 2n f )/(12\u03c0) and",
            "paragraph_rank": 10,
            "section_rank": 3
        },
        {
            "section": "Theoretical framework",
            "text": "where n f is the number of \"light\" quark flavours (m q \u00b5). Most importantly, for n f < 17 we have b 0 > 0, which leads to a decreasing coupling strength for increasing scale (asymptotic freedom), as originally predicted by Politzer [7], Gross and Wilczek [8]. Considering only the first term of the expansion on the right hand side of eq. 1, a solution can be written as",
            "paragraph_rank": 11,
            "section_rank": 3,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b5",
                    "start": 232,
                    "text": "[7]",
                    "end": 235
                },
                {
                    "type": "bibr",
                    "ref_id": "b6",
                    "start": 255,
                    "text": "[8]",
                    "end": 258
                }
            ]
        },
        {
            "section": "Theoretical framework",
            "text": "\u22121 , with \u039b \u2248 200 MeV defined as the scale where \u03b1 s (\u00b5 2 ) formally diverges. Whereas at small scales of order GeV or lower the coupling constant increases dramatically, making any perturbative approach to the solution of low-energy strong interactions and the property of confinement meaningless, it is the property of asymptotic freedom that leads to an expansion parameter \u03b1 s well below unity and thus allows perturbative methods to be applied for the calculation of scattering processes.",
            "paragraph_rank": 12,
            "section_rank": 3
        },
        {
            "section": "Theoretical framework",
            "text": "To second order, including the resummation of leading logarithms of type ln(\u00b5 2 /Q 2 ), a solution of eq. 1 allows to relate \u03b1 s at one scale \u00b5 2 to that at another scale Q 2 ,",
            "paragraph_rank": 13,
            "section_rank": 3
        },
        {
            "section": "Theoretical framework",
            "text": "In the following we will use the resulting expansion",
            "paragraph_rank": 14,
            "section_rank": 3
        },
        {
            "section": "Theoretical framework",
            "text": "This shows that a change of scale only manifests itself as a non-leading effect in \u03b1 s ; in other words, a meaningful determination of the running coupling constant necessarily has to involve a next-to-leading order (NLO) prediction. In order to highlight this even further, let's look at the perturbative structure, up to NLO, of some generic cross section that is proportional to \u03b1 s at leading order (e.g., a three-jet cross section in e + e \u2212 annihilations at",
            "paragraph_rank": 15,
            "section_rank": 3
        },
        {
            "section": "Theoretical framework",
            "text": "The coefficients A and B have to be calculated for the specific process at hand. Now let us first assume that only A is known for a particular process \"1\", i.e., only the leading order (LO) expansion is available, \u03c3 LO 1 = \u03b1 s (\u00b5 2 )A 1 . However, at the same LO we could equally well write this prediction as \u03c3 LO 1 = \u03b1 s (\u00b5 2 )A 1 = \u03b1 s (Q 2 )A 1 , because using the above expansion of the coupling constant, eq. 3, we see that the scale dependence would only appear as an NLO correction, namely",
            "paragraph_rank": 16,
            "section_rank": 3
        },
        {
            "section": "Theoretical framework",
            "text": "Thus, strictly sticking to the LO expression, it is clear that an experimental measurement of \u03c3 1 and its comparison to \u03c3 LO 1 only allows to determine some \"effective\" LO coupling constant \u03b1 eff,1 s , where it is unclear to which scale this really corresponds to. Furthermore, repeating the same procedure at LO for some other process \"2\", at some different physical energy or momentum scale, would result in a measurement \u03b1 eff,2 s , and most likely these two measurements of the effective coupling constant would give differing results.",
            "paragraph_rank": 17,
            "section_rank": 3
        },
        {
            "section": "Theoretical framework",
            "text": "Looking again at the expression",
            "paragraph_rank": 18,
            "section_rank": 3
        },
        {
            "section": "Theoretical framework",
            "text": "we also see that \u03c3 LO 1 strongly depends on the unphysical scale \u00b5, since the logarithm with the explicit \u00b5 dependence already appears at NLO. Correspondingly, a determination of \u03b1 s (Q 2 ) using this prediction would result in a large uncertainty when varying the unphysical parameter \u00b5 in the fits to the measured cross section. This procedure of \u00b5-variations, typically over a range of 0.5 < \u00b5/Q < 2, has become a standard approach to estimating the possible impact of unknown higher-order contributions. In fact, the \u00b5-dependence always appears at one order higher than the fully known perturbative expansion. More concretely, let's now assume that also the NLO coefficient B has been calculated. Then, by plugging the expansion 3 into eq. 4 we find",
            "paragraph_rank": 19,
            "section_rank": 3
        },
        {
            "section": "Theoretical framework",
            "text": "We see that there is no \u00b5-dependence up to NLO; at this order it is thus equivalent to set \u00b5 = Q and to write \u03c3(\u03b1 s (\u00b5 2 ), \u00b5 2 Q 2 ) = \u03c3(\u03b1 s (Q 2 )); i.e., we can replace the dependence of the running coupling constant on the unphysical scale \u00b5 with a dependence on the physical scale Q of the process at hand. Furthermore, we see that the explicit \u00b5-dependence of the cross section prediction only appears at next-to-NLO (NNLO), i.e. suppressed by two powers of \u03b1 s relative to the LO term. This leads to a smaller uncertainty of the extracted \u03b1 s (Q 2 ) value when varying \u00b5 in the fit procedure. Finally, the NLO expression in eq. 5 leads to the first non-trivial dependence of the cross section on \u03b1 s (Q 2 ) at the particular scale Q. Therefore, determinations of \u03b1 s (Q 2 ) from two different processes, at possibly different values of Q, using the NLO predictions for the cross sections and the running of \u03b1 s in order to relate the measured values to each other, should result, within uncertainties, in consistent measurements. Similarly, the value of \u03b1 s (Q 2 ) determined at NLO for some specific process can be used for predicting, at the same order, the cross section for another process at a different physical scale.",
            "paragraph_rank": 20,
            "section_rank": 3
        },
        {
            "section": "Theoretical framework",
            "text": "The extension of this discussion to NNLO and beyond is straightforward, and easily shows that theoretical uncertainties estimated from \u00b5-variations should decrease even further. This is nicely illustrated in Fig. 1, where the dependence of the extracted value of \u03b1 s (M 2 z ) is shown, when using the LO, NLO and NNLO pQCD expressions for fitting the measured hadronic width of the Z boson, normalised to its leptonic width [9]. Ultimately, for an observable known at all orders in pQCD the \u00b5-dependence would vanish completely, as it should be for a physical observable that cannot depend on unphysical parameters. In fact, the \u00b5-dependence of the NLO term in expression 4 could have simply been derived by imposing this requirement for a physical observable and using the renormalization group equation.",
            "paragraph_rank": 21,
            "section_rank": 3,
            "ref_spans": [
                {
                    "type": "figure",
                    "ref_id": "fig_0",
                    "start": 208,
                    "text": "Fig. 1",
                    "end": 214
                },
                {
                    "type": "bibr",
                    "ref_id": "b8",
                    "start": 424,
                    "text": "[9]",
                    "end": 427
                }
            ]
        },
        {
            "section": "Theoretical framework",
            "text": "At this stage it should have become clear, but still is worth highlighting, that the running coupling constant \u03b1 s (Q 2 ) is not a physical quantity, but simply a parameter defined in the context of a particular theoretical framework, namely pQCD up to some order in \u03b1 s . It can be determined experimentally in this context and used for making predictions for observables calculated within the same framework.",
            "paragraph_rank": 22,
            "section_rank": 3
        },
        {
            "section": "Theoretical framework",
            "text": "I would like to conclude these theoretical considerations by highlighting a further consequence of the particular scale behaviour of \u03b1 s : An uncertainty \u03b4 on a measurement of \u03b1 s (Q 2 ), at a scale Q, translates to an uncertainty",
            "paragraph_rank": 23,
            "section_rank": 3
        },
        {
            "section": "Theoretical framework",
            "text": "; that is, \u03b4 < \u03b4 for Q < M z . This enhances the impact of precise low-Q measurements, such as from \u03c4 decays (c.f. below), in combinations performed at the M z scale.",
            "paragraph_rank": 24,
            "section_rank": 3
        },
        {
            "text": "Observables",
            "section_rank": 4
        },
        {
            "section": "Observables",
            "text": "The strong coupling constant has been measured in a large variety of physics processes, using many different observables. As depicted in Fig. 2, sensitivity to the coupling of quarks to gluons is obtained by studying, e.g., deepinelastic lepton-nucleon scattering, e + e \u2212 annihilations, hadron collisions or resonance decays. Since we are not able to directly measure partons (quarks or gluons), but only hadrons and their decay products, a central issue is establishing a correspondence between observables obtained at the partonic and the hadronic level. The only theoretically sound correspondence is achieved by means of infrared and collinear safe (ICS) quantities (see e.g. Ref. [2]), which allow to obtain finite predictions at any order of perturbative QCD. ICS observables are insensitive to the addition of a soft parton, or to the splitting of one parton into two collinear ones. This guarantees that singularities, which appear in the infrared or collinear limits of diagrams involving real and/or virtual radiation, cancel order by order in perturbation theory.",
            "paragraph_rank": 25,
            "section_rank": 4,
            "ref_spans": [
                {
                    "type": "figure",
                    "ref_id": "fig_1",
                    "start": 137,
                    "text": "Fig. 2",
                    "end": 143
                },
                {
                    "type": "bibr",
                    "ref_id": "b0",
                    "start": 686,
                    "text": "[2]",
                    "end": 689
                }
            ]
        },
        {
            "section": "Observables",
            "text": "Generally speaking, ICS observables can be divided into different classes, depending on how detailed the hadronic final state is being scrutinized. As for example discussed in Ref. [2], the simplest case of ICS observables are total cross sections. More generally, when measuring fully inclusive observables, the final state is not analyzed at all regarding its (topological, kinematical) structure or its composition. Basically the relevant information consists in the rate of a process ending up in a partonic or hadronic final state. In e + e \u2212 annihilation, widely used examples are the ratios of partial widths or branching ratios for the electroweak decay of particles into hadrons or leptons, such as Z or \u03c4 decays. The strong suppression of non-perturbative effects, O(\u039b 4 /Q 4 ), is one of the attractive features of such observables. However, at the same time the sensitivity to radiative QCD corrections is small, since here the perturbative expansion is of the type 1 + n c n \u03b1 n s , corresponding to, e.g., a 4% correction, 1+\u03b1 s (M 2 z )/\u03c0 \u2248 1+0.04, for the scaled hadronic Z width. In the case of \u03c4 decays not only the hadronic branching ratio is of interest, but also moments of the spectral functions of hadronic tau decays, which sample different parts of the decay spectrum and thus provide additional information.",
            "paragraph_rank": 26,
            "section_rank": 4,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b0",
                    "start": 181,
                    "text": "[2]",
                    "end": 184
                }
            ]
        },
        {
            "section": "Observables",
            "text": "Other examples of fully inclusive observables, that are used for \u03b1 s determinations, are the total top-pair production cross section in proton-proton collisions, the ratio of the hadronic to leptonic branching ratios of \u03a5 decays, which is proportional to \u03b1 3 s at LO (cf. the right-most diagram in Fig. 2), or structure functions (and related sum rules) in deep-inelastic scattering. Structure functions are sensitive to \u03b1 s through the Dokshitzer-Gribov-Lipatov-Altarelli-Parisi [10,11,12,13] ",
            "paragraph_rank": 27,
            "section_rank": 4,
            "ref_spans": [
                {
                    "type": "figure",
                    "ref_id": "fig_1",
                    "start": 298,
                    "text": "Fig. 2",
                    "end": 304
                },
                {
                    "type": "bibr",
                    "ref_id": "b9",
                    "start": 480,
                    "text": "[10,",
                    "end": 484
                },
                {
                    "type": "bibr",
                    "ref_id": "b10",
                    "start": 484,
                    "text": "11,",
                    "end": 487
                },
                {
                    "type": "bibr",
                    "ref_id": "b11",
                    "start": 487,
                    "text": "12,",
                    "end": 490
                },
                {
                    "type": "bibr",
                    "ref_id": "b12",
                    "start": 490,
                    "text": "13]",
                    "end": 493
                }
            ]
        },
        {
            "section": "Observables",
            "text": ", which depicts in a simplified manner the scaling violation of the F 2 structure function; here x is the Bjorken scaling variable, P qg is a so-called splitting function and g(x, Q 2 ) is the parton distribution function (PDF) of the gluon. Such equations are used in global PDF fits in order to relate measurements at different Q scales to each other and to fit the PDFs at a chosen initial scale. We see that in such approaches the fit results for \u03b1 s and g(x, Q 2 ) are strongly correlated. Similar considerations apply to the measurements of scaling violations of fragmentation functions.",
            "paragraph_rank": 28,
            "section_rank": 4
        },
        {
            "section": "Observables",
            "text": "Compared to inclusive observables, the particular structure, topology or composition of the hadronic final state can give enhanced sensitivity to \u03b1 s , therefore cross sections differential in one or more variables characterizing this structure are of interest. The obvious example is the measurement of jet cross sections and jet rates, where the probability of producing an additional jet is directly proportional to \u03b1 s (for a general discussion of jets and jet algorithms I refer the reader to, e.g., Refs. [4,2] and references therein). Besides jet quantities, another class of observables, so-called event shapes, have been widely used, in particular for measurements in e + e \u2212 annihilations, but also in lepton-proton and hadron collisions. The classic example of an event shape is the Thrust [14,15] (T or\u03c4 = 1 \u2212 T ) in e + e \u2212 annihilations, defined as",
            "paragraph_rank": 29,
            "section_rank": 4,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b2",
                    "start": 511,
                    "text": "[4,",
                    "end": 514
                },
                {
                    "type": "bibr",
                    "ref_id": "b0",
                    "start": 514,
                    "text": "2]",
                    "end": 516
                },
                {
                    "type": "bibr",
                    "ref_id": "b13",
                    "start": 801,
                    "text": "[14,",
                    "end": 805
                },
                {
                    "type": "bibr",
                    "ref_id": "b14",
                    "start": 805,
                    "text": "15]",
                    "end": 808
                }
            ]
        },
        {
            "section": "Observables",
            "text": "where p i are the momenta of the particles or the jets in the final-state and the maximum is obtained for the Thrust axis n \u03c4 . In the Born limit of the production of a perfect back-to-back quark-antiquark pair the limit\u03c4 \u2192 0 is obtained, whereas a perfectly symmetric many-particle configuration leads t\u00f4 \u03c4 \u2192 1/2. Figure 3 (left) shows an example of measurements by the ALEPH experiment at different centre-of-mass energies. Besides Thrust, many other similar observables such as C-parameter, Heavy Jet mass, Jet Broadening or the differential three-jet rate were proposed and used for \u03b1 s -determinations. They all provide a measure of the topology of an event, and typically are defined such that they take on small values for pencil-like (back-to-back) configurations, and large values for more spherical topologies that arise from single or multiple hard gluon radiation. This provides sensitivity to \u03b1 s at LO in perturbation theory, with normalized cross sections expressed as an expansion of the type eq. 4. As discussed further below, predictions are known up to NNLO and complemented by the all-orders resummation of logarithms of the event-shape variable (i.e., terms of the form \u03b1 n s ln m\u03c4 ). An important aspect of event-shape variables is their enhanced sensitivity to non-perturbative effects compared to more inclusive quantities, with power corrections of \u223c \u03bb/Q. For \u03b1 s determinations, analytical functions of these power corrections were used to complement the purely perturbative expansion, but the more widespread approach to correct for non-perturbative effects has been to use Monte Carlo simulations and their hadronization models in order to calculate the event shape at the partonic and hadronic level. As can be seen in Fig. 3 (right), these non-perturbative corrections can be sizeable, especially when approaching the two-jet region of the distribution, therefore the fit range has to be chosen carefully. Ultimately the model dependence of such corrections leads to systematic uncertainties on the extracted \u03b1 s values. The width of the band covers the predictions using different hadronization models; taken from Ref. [17].",
            "paragraph_rank": 30,
            "section_rank": 4,
            "ref_spans": [
                {
                    "type": "figure",
                    "ref_id": "fig_2",
                    "start": 315,
                    "text": "Figure 3",
                    "end": 323
                },
                {
                    "type": "figure",
                    "ref_id": "fig_2",
                    "start": 1748,
                    "text": "Fig. 3 (right)",
                    "end": 1762
                },
                {
                    "type": "bibr",
                    "ref_id": "b16",
                    "start": 2150,
                    "text": "[17]",
                    "end": 2154
                }
            ]
        },
        {
            "section": "Observables",
            "text": "A completely different approach to the determination of \u03b1 s (M 2 z ) consists in calculating QCD predictions on the lattice for observables such as hadron mass splittings. From the comparison of data to the predictions, obtained as a function of the lattice spacing a and extrapolated to a \u2192 0, first a lattice coupling is extracted which is then converted to its perturbative counter-part \u03b1 s (M 2 z ). During the last decades there has been enormous progress in this field; indeed, the most precise \u03b1 s (M 2 z ) determinations to date are obtained from lattice QCD, though it is fair to say that the community still intensively discusses the validity of the very small systematic uncertainties, claimed by some of the involved groups. A more detailed discussion of this approach can be found in the review by Sachrajda [18].",
            "paragraph_rank": 31,
            "section_rank": 4,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b17",
                    "start": 821,
                    "text": "[18]",
                    "end": 825
                }
            ]
        },
        {
            "text": "Brief historical overview",
            "section_rank": 5
        },
        {
            "section": "Brief historical overview",
            "text": "In the following I make an attempt to sketch some of the relevant developments that occurred during the last few decades, without any claim of being comprehensive and of covering all types of \u03b1 s studies in a balanced manner. In fact, a particular focus is put on results obtained by experiments at CERN.",
            "paragraph_rank": 32,
            "section_rank": 5
        },
        {
            "section": "Brief historical overview",
            "text": "The first extensive overview of \u03b1 s measurements was given by Altarelli [19] in 1989. In that review, he summarised measurements based on observables such as (i) the total hadronic cross section in e + e \u2212 annihilation (at that time known at NNLO in pQCD, i.e., up to \u03b1 3 s ) from data in the range 7 < Q < 56 GeV; (ii) scaling violations in deep inelastic leptoproduction with structure function data from BCDMS, EMC and CHARM; (iii) quarkonium decays, especially ratios of \u03a5 partial widths (\u0393 ggg /\u0393 \u00b5\u00b5 , \u0393 \u03b3 gg /\u0393 ggg ) measured by the CUSB, CLEO, ARGUS and Crystal Ball collaborations; and (iv) jet production, energy-energy correlations and the photon structure function from \u03b3\u03b3 reactions, using e + e \u2212 data collected by the PEP/PETRA experiments. A summary of these measurements is shown in Fig. 4. In an attempt to combine all those results, obtained at Q values from a few up to several tens of GeV, and using the QCD prediction for the running of \u03b1 s (Q 2 ), he arrived at a prediction for the coupling evaluated at the Z boson mass, \u03b1 s (Q \u2248 M z ) \u2248 0.11 \u00b1 0.01; that is, a determination of the strong coupling constant at the 10% level. Interestingly, he concluded with the following statement: Establishing that this prediction is experimentally true would be a very quantitative and accurate test of QCD, conceptually equivalent but more reasonable than trying to see the running in a given experiment. It is impressive to note that his prediction turned out to be nicely consistent with the current WA value [2] of \u03b1 s (M 2 z ) = 0.1185 \u00b1 0.0006. In addition, we see that over the past 25 years the relative uncertainty has been reduced by a factor of 18, gauging the enormous progress made during these decades.",
            "paragraph_rank": 33,
            "section_rank": 5,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b18",
                    "start": 72,
                    "text": "[19]",
                    "end": 76
                },
                {
                    "type": "figure",
                    "start": 798,
                    "text": "Fig. 4",
                    "end": 804
                },
                {
                    "type": "bibr",
                    "ref_id": "b0",
                    "start": 1523,
                    "text": "[2]",
                    "end": 1526
                }
            ]
        },
        {
            "section": "Brief historical overview",
            "text": "The year 1989 also saw the start of the LEP experiments (ALEPH, DEL-PHI, L3, OPAL), and the following decade was characterised by great advances, both experimentally and theoretically, in the field of pQCD in general and \u03b1 s measurements in particular. Extensive overviews can be found, for instance, in Refs. [21,20] which also discuss the application to earlier JADE data of the developments that occurred during the LEP era.",
            "paragraph_rank": 34,
            "section_rank": 5,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b20",
                    "start": 310,
                    "text": "[21,",
                    "end": 314
                },
                {
                    "type": "bibr",
                    "ref_id": "b19",
                    "start": 314,
                    "text": "20]",
                    "end": 317
                }
            ]
        },
        {
            "section": "Brief historical overview",
            "text": "Event-shape observables were studied in great detail by the LEP experiments, showing first that pQCD at NLO does not provide an adequate ac- Figure 4: Summary of the \u03b1 s determinations by Altarelli [19] in 1989. curacy in order to go well below the 10% level in relative uncertainty on \u03b1 s (M 2 z ). At the same time, calculations that resum large logarithms of the event-shape variable to all orders in \u03b1 s appeared and were used to improve the NLO prediction for a number of event-shape observables. This also triggered the development of a novel jet algorithm, the so-called Durham algorithm [22], with a modified metric compared to the previously used JADE algorithm [23]. The modification of the jet metric, which defines the distance in phase-space between two particles that might or might not be combined into a new pseudo-particle, was motivated by the fact that the pQCD predictions for jet rates and the differential 3-jet rate, based on the JADE metric, did not show the exponentiation behaviour of the large logarithms [24], whereas using the Durham metric led to exponentiation and ultimately to improved resummation predictions. Note that this Durham algorithm became the standard algorithm for jet finding at LEP, and was at the basis for later developments of iterative recombination algorithms, nowadays widely used at the LHC (cf. Ref. [2] and references therein). As a consequence of the combined NLO+resummed predictions, corrected for non-perturbative hadronization effects using phenomenological Monte Carlo (MC) models, the relative uncertainty of \u03b1 s was reduced to the \u223c 5% level, still dominated by theoretical uncertainties due to missing higher orders and estimated from variations of the renormalisation scale. Attempts to replace the MC models by analytical power corrections [25] of order \u03bb/Q did not lead to substantially different results. It became clear that only a complete NNLO calculation for jet rates and event shapes might lead to a significant reduction of uncertainties. Indeed, such a calculation [26,27] appeared after the end of LEP, and its first applications [28,29] to the 3-jet rate and to event shapes, including next-toleading log resummations, resulted in more precise \u03b1 s measurements at the 2-3% precision level.",
            "paragraph_rank": 35,
            "section_rank": 5,
            "ref_spans": [
                {
                    "type": "figure",
                    "start": 141,
                    "text": "Figure 4",
                    "end": 149
                },
                {
                    "type": "bibr",
                    "ref_id": "b18",
                    "start": 198,
                    "text": "[19]",
                    "end": 202
                },
                {
                    "type": "bibr",
                    "ref_id": "b21",
                    "start": 595,
                    "text": "[22]",
                    "end": 599
                },
                {
                    "type": "bibr",
                    "ref_id": "b22",
                    "start": 671,
                    "text": "[23]",
                    "end": 675
                },
                {
                    "type": "bibr",
                    "ref_id": "b23",
                    "start": 1032,
                    "text": "[24]",
                    "end": 1036
                },
                {
                    "type": "bibr",
                    "ref_id": "b0",
                    "start": 1355,
                    "text": "[2]",
                    "end": 1358
                },
                {
                    "type": "bibr",
                    "ref_id": "b24",
                    "start": 1807,
                    "text": "[25]",
                    "end": 1811
                },
                {
                    "type": "bibr",
                    "ref_id": "b25",
                    "start": 2042,
                    "text": "[26,",
                    "end": 2046
                },
                {
                    "type": "bibr",
                    "ref_id": "b26",
                    "start": 2046,
                    "text": "27]",
                    "end": 2049
                },
                {
                    "type": "bibr",
                    "ref_id": "b27",
                    "start": 2108,
                    "text": "[28,",
                    "end": 2112
                },
                {
                    "type": "bibr",
                    "ref_id": "b28",
                    "start": 2112,
                    "text": "29]",
                    "end": 2115
                }
            ]
        },
        {
            "section": "Brief historical overview",
            "text": "Most of the aforementioned determinations gave \u03b1 s (M 2 z ) values in a range of, very roughly speaking, 0.117 -0.125. However, as summarized in Ref. [2], more recent re-analyses of the Thrust distribution, based on novel developments in soft-collinear effective field theory, resummation at next-to-nextto-next-to-leading logarithmic accuracy, and analytic calculations of nonperturbative effects, resulted in values as low as 0.1131, while at the same time claiming very small uncertainties at the 2% level. Thus, further work will be required to understand this spread of results from jet and event-shape observables, which covers a larger range than most of the individually quoted uncertainties.",
            "paragraph_rank": 36,
            "section_rank": 5,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b0",
                    "start": 150,
                    "text": "[2]",
                    "end": 153
                }
            ]
        },
        {
            "section": "Brief historical overview",
            "text": "In terms of inclusive observables, the LEP experiments quoted precise \u03b1 s measurements by using the total hadronic cross section (or equivalently, the hadronic width of the Z boson), as well as by analyzing hadronic \u03c4 decays. Contrary to event shapes, NNLO predictions for these observables were already available in the nineties, leading to rather small renormalisation scale uncertainties. By now, they are even known to N 3 LO accuracy. This implies an almost negligible theoretical uncertainty in the case of the hadronic Z decay width; for instance, when included in a global fit [30] to electroweak precision data a value of \u03b1 s (M 2 z ) = 0.1193 \u00b1 0.0028 is found, where the dominant part of the uncertainty is of statistical nature.",
            "paragraph_rank": 37,
            "section_rank": 5,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b29",
                    "start": 585,
                    "text": "[30]",
                    "end": 589
                }
            ]
        },
        {
            "section": "Brief historical overview",
            "text": "Naively speaking, similarly precise results might not have been expected from the analyses of hadronic \u03c4 decays, since here the relevant scale is the \u03c4 mass, close to the scale where pQCD is supposed to break down. Thus, non-perturbative effects and missing higher order contributions should significantly impact the attainable precision. However, it was realised that measuring different moments of the \u03c4 spectral function allows to determine \u03b1 s (M 2 \u03c4 ) and to constrain non-perturbative power-suppressed contributions at the same time. Several methods were developed to estimate missing higherorder terms, beyond NNLO and N 3 LO, such as so-called contour-improved perturbative expansions, claiming very small scale uncertainties. It is worth noting [2] that these methods are still matter of intense discussions, in particular since some of the results obtained by different groups differ by several standard deviations in terms of the quoted uncertainties. In an attempt to combine all these results and to take into account the observed spread, Ref.",
            "paragraph_rank": 38,
            "section_rank": 5,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b0",
                    "start": 754,
                    "text": "[2]",
                    "end": 757
                }
            ]
        },
        {
            "section": "Brief historical overview",
            "text": "[2] quotes \u03b1 s (M 2 z ) = 0.1197\u00b10.0016. Thus, somewhat surprisingly, the study of \u03c4 decays results in one of the most precise \u03b1 s determinations, basically at the level of 1% relative uncertainty. It should be emphasized that the already precise measurements, obtained at the scale of the \u03c4 mass, turn into this even more precise result at the Z mass, because of the running of \u03b1 s as discussed at the end of Section 2.",
            "paragraph_rank": 39,
            "section_rank": 5
        },
        {
            "section": "Brief historical overview",
            "text": "Many of the developments of the LEP area, in the field of event shapes and jet observables, were also applied to HERA data of deep inelastic electronproton scattering (DIS). Although here the pQCD predictions are only known up to NLO approximation so far, and the \u03b1 s extraction from jet final states is somewhat more complicated because of the additional implicit \u03b1 s dependence of the PDFs, it is impressive to see that a combination [31] based on precise HERA data of inclusive jet cross sections in neutral current DIS at high Q 2 results in \u03b1 s (M 2 z ) = 0.1198 \u00b1 0.0032, which includes a theoretical uncertainty of \u00b10.0026. These HERA measurements also allow covering a large range of Q 2 values and thus probing directly the running of \u03b1 s .",
            "paragraph_rank": 40,
            "section_rank": 5,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b30",
                    "start": 436,
                    "text": "[31]",
                    "end": 440
                }
            ]
        },
        {
            "section": "Brief historical overview",
            "text": "More inclusive DIS observables, in particular structure functions and their scaling violations as discussed in Section 3, have been used in global PDF fits based on NNLO pQCD and resulted in smaller relative uncertainties, even at the 1% level as quoted by some groups. However, quite similarly to the case of event shapes and \u03c4 observables, also here a spread of \u03b1 s (M 2 z ) values (roughly covering a range of 0.113 [32] to 0.117 [33,34]) is observed [2] that is larger than some of the individually quoted uncertainties. Two remarks are in place here: (i) these differences are still matter of intense discussions among the various groups performing global PDF fits, and (ii) it is kind of a tradition that \u03b1 s determinations from DIS and global PDF fits result in smaller values than those obtained from e + e \u2212 annihilations, without understanding the origin of this apparent bias.",
            "paragraph_rank": 41,
            "section_rank": 5,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b0",
                    "start": 371,
                    "text": "2",
                    "end": 372
                },
                {
                    "type": "bibr",
                    "ref_id": "b31",
                    "start": 419,
                    "text": "[32]",
                    "end": 423
                },
                {
                    "type": "bibr",
                    "ref_id": "b32",
                    "start": 433,
                    "text": "[33,",
                    "end": 437
                },
                {
                    "type": "bibr",
                    "ref_id": "b33",
                    "start": 437,
                    "text": "34]",
                    "end": 440
                },
                {
                    "type": "bibr",
                    "ref_id": "b0",
                    "start": 454,
                    "text": "[2]",
                    "end": 457
                }
            ]
        },
        {
            "section": "Brief historical overview",
            "text": "Jet observables at hadron colliders, such as the inclusive jet cross section as a function of jet transverse momentum or invariant multi-jet masses, jet angular correlations or jet rates, are only known to NLO approximation so far. Furthermore, important systematic uncertainties due to the jet energy scale and choice of PDF set are expected to limit the attainable precision, and similarly to the case of DIS, the intrinsic \u03b1 s dependence of the PDFs has to be carefully taken into account in any \u03b1 s fit. As discussed in Ref. [2], first measurements at the Tevatron and at the LHC gave results consistent with the WA value and with relative uncertainties in the range of 4 to 8%. However, very important developments have taken place at the LHC recently, as e.g. summarised in Ref. [35]. First, in both the ATLAS and CMS experiments the jet energy scale uncertainty is now known at an impressive level of about 1-2% for jets in the \u223c 100 GeV range [36,37]. Since jet cross sections are steeply falling functions of jet momentum this has an enormous impact on the finally attainable precision. Furthermore, ratios of observables, such as the ratio of the 3-jet over the 2-jet rate, allow to eliminate this systematic uncertainty to a large extend, as shown in Refs. [38,39]. Finally, NNLO calculations for the inclusive jet cross section appear to be around the corner [40], which will further boost the importance of such measurements.",
            "paragraph_rank": 42,
            "section_rank": 5,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b0",
                    "start": 529,
                    "text": "[2]",
                    "end": 532
                },
                {
                    "type": "bibr",
                    "ref_id": "b34",
                    "start": 785,
                    "text": "[35]",
                    "end": 789
                },
                {
                    "type": "bibr",
                    "ref_id": "b35",
                    "start": 951,
                    "text": "[36,",
                    "end": 955
                },
                {
                    "type": "bibr",
                    "ref_id": "b36",
                    "start": 955,
                    "text": "37]",
                    "end": 958
                },
                {
                    "type": "bibr",
                    "ref_id": "b37",
                    "start": 1268,
                    "text": "[38,",
                    "end": 1272
                },
                {
                    "type": "bibr",
                    "start": 1272,
                    "text": "39]",
                    "end": 1275
                },
                {
                    "type": "bibr",
                    "ref_id": "b38",
                    "start": 1371,
                    "text": "[40]",
                    "end": 1375
                }
            ]
        },
        {
            "section": "Brief historical overview",
            "text": "In fact, recently the first \u03b1 s determination [41] at a hadron collider, using pQCD at NNLO, has been published. However, here an inclusive quantity, namely the top-pair production cross section, has been successfully exploited thanks to its strong sensitivity to both \u03b1 s and the top quark mass. Fixing the latter to its WA value allowed the CMS collaboration to extract \u03b1 s (M 2 z ) at an impressive relative precision of \u223c 3%, also thanks to the remarkable experimental precision (4%) of the top cross section measurement [42] that served as input. Because of this recent progress, and because of the large Q 2 range covered by the measurements at the LHC, the running of the strong coupling constant is now being precisely studied over an unprecedented energy range.",
            "paragraph_rank": 43,
            "section_rank": 5,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b39",
                    "start": 46,
                    "text": "[41]",
                    "end": 50
                },
                {
                    "type": "bibr",
                    "ref_id": "b40",
                    "start": 525,
                    "text": "[42]",
                    "end": 529
                }
            ]
        },
        {
            "section": "Brief historical overview",
            "text": "As mentioned at the end of Section 3, a discussion of \u03b1 s determinations using lattice QCD can be found in a separate review [18] in this volume. For completeness it should be stated here that this very complementary approach, compared to the measurements described above, results in the world's most precise \u03b1 s (M 2 z ) determinations to date, with some of the analyses quoting relative uncertainties at the 0.5% level (cf. Ref. [2]). However, the community is having intense discussions about the validity of these apparently rather optimistic estimates of systematic uncertainties. In any case, the lattice results dominate the current WA value: not including them in the averaging procedure results in \u03b1 s (M 2 z ) = 0.1183 \u00b1 0.0012 [2], i.e., the uncertainty doubles.",
            "paragraph_rank": 44,
            "section_rank": 5,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b17",
                    "start": 125,
                    "text": "[18]",
                    "end": 129
                },
                {
                    "type": "bibr",
                    "ref_id": "b0",
                    "start": 431,
                    "text": "[2]",
                    "end": 434
                },
                {
                    "type": "bibr",
                    "ref_id": "b0",
                    "start": 738,
                    "text": "[2]",
                    "end": 741
                }
            ]
        },
        {
            "section": "Brief historical overview",
            "text": "This historical overview can not be concluded without a brief discussion of the general issue of combining \u03b1 s determinations. As discussed in Ref. [2], this is a non-trivial exercise. Since most of the individual measurements are dominated by systematic uncertainties, which cannot be expected to follow a normal distribution, and since very often the correlations among these uncertainties are not very well known, simple averaging methods as applicable to measurements with statistical errors only might not be appropriate. In 1995 Schmelling [43] proposed a method for estimating such unknown correlations, which rescales individual uncertainties according to the assumption that the normalized \u03c7 2 of the combination should be 1. This method is also used for the current WA determination [2]. Furthermore, there is a certain arbitrariness in the choice of results included in the average. Finally, as mentioned earlier, often \u03b1 s determinations based on the same observable but using different methods give results that differ by a larger amount than would be expected from the individually quoted uncertainties, rendering the estimate of the combined uncertainty a difficult exercise.",
            "paragraph_rank": 45,
            "section_rank": 5,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b0",
                    "start": 148,
                    "text": "[2]",
                    "end": 151
                },
                {
                    "type": "bibr",
                    "ref_id": "b41",
                    "start": 546,
                    "text": "[43]",
                    "end": 550
                },
                {
                    "type": "bibr",
                    "ref_id": "b0",
                    "start": 793,
                    "text": "[2]",
                    "end": 796
                }
            ]
        },
        {
            "section": "Brief historical overview",
            "text": "year World average \u03b1 s (M 2 z ) Altarelli [19] 1989 0.11 \u00b1 0.01 Hinchcliffe [44] (PDG) 1992 0.1134 \u00b1 0.0035 Hinchcliffe [45] (PDG) 1995 0.118 \u00b1 0.003 Schmelling [46] 1997 0.118 \u00b1 0.003 Bethke [9] 2000 0.1184 \u00b1 0.0031 Bethke [48] 2006 Throughout these years, several individuals and/or groups have compiled the available \u03b1 s measurements and combined them into a single value. The earliest attempt by Altarelli has already been discussed above. During the nineties, the reference in terms of \u03b1 s (M 2 z ) was established by the PDG, in particular thanks to the PDG review on QCD by Hinchcliffe (see, e.g., Refs. [44,45]). An independent estimate of the WA value was published by Schmelling [46] in 1997, based on his proposal for handling unknown correlations. Then, during the first decade of this century, Bethke [9,47,48] provided a number of comprehensive studies, that established the de-facto WA value, despite the PDG still publishing an independent combination. Since a few years this situation has been resolved, with Bethke now being co-author (together with Dissertori and Salam) of the PDG review on QCD that also contains the WA determination of \u03b1 s . Figure 5 displays this, most likely incomplete, collection of WA results as a function of time, nicely showing the impressive progress made throughout the last decades. Finally, Fig. 6 presents an example [9] of inputs to the averaging procedure and the current experimental status of the running of \u03b1 s , showing excellent agreement with the theoretical expectation. ",
            "paragraph_rank": 46,
            "section_rank": 5,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b18",
                    "start": 42,
                    "text": "[19]",
                    "end": 46
                },
                {
                    "type": "bibr",
                    "ref_id": "b42",
                    "start": 76,
                    "text": "[44]",
                    "end": 80
                },
                {
                    "type": "bibr",
                    "ref_id": "b43",
                    "start": 120,
                    "text": "[45]",
                    "end": 124
                },
                {
                    "type": "bibr",
                    "ref_id": "b44",
                    "start": 161,
                    "text": "[46]",
                    "end": 165
                },
                {
                    "type": "bibr",
                    "ref_id": "b8",
                    "start": 192,
                    "text": "[9]",
                    "end": 195
                },
                {
                    "type": "bibr",
                    "ref_id": "b46",
                    "start": 224,
                    "text": "[48]",
                    "end": 228
                },
                {
                    "type": "bibr",
                    "ref_id": "b42",
                    "start": 611,
                    "text": "[44,",
                    "end": 615
                },
                {
                    "type": "bibr",
                    "ref_id": "b43",
                    "start": 615,
                    "text": "45]",
                    "end": 618
                },
                {
                    "type": "bibr",
                    "ref_id": "b44",
                    "start": 689,
                    "text": "[46]",
                    "end": 693
                },
                {
                    "type": "bibr",
                    "ref_id": "b8",
                    "start": 814,
                    "text": "[9,",
                    "end": 817
                },
                {
                    "type": "bibr",
                    "ref_id": "b45",
                    "start": 817,
                    "text": "47,",
                    "end": 820
                },
                {
                    "type": "bibr",
                    "ref_id": "b46",
                    "start": 820,
                    "text": "48]",
                    "end": 823
                },
                {
                    "type": "figure",
                    "ref_id": "fig_3",
                    "start": 1164,
                    "text": "Figure 5",
                    "end": 1172
                },
                {
                    "type": "figure",
                    "ref_id": "fig_4",
                    "start": 1342,
                    "text": "Fig. 6",
                    "end": 1348
                },
                {
                    "type": "bibr",
                    "ref_id": "b8",
                    "start": 1369,
                    "text": "[9]",
                    "end": 1372
                }
            ]
        },
        {
            "text": "Conclusions",
            "section_rank": 6
        },
        {
            "section": "Conclusions",
            "text": "The strong coupling constant is one of the fundamental parameters of the standard model of particle physics. In this article I have reviewed the theoretical and experimental developments that have led to a precise knowledge of this important parameter, representing a cornerstone in our understanding of the strong interactions sector of the standard model.",
            "paragraph_rank": 47,
            "section_rank": 6
        },
        {
            "text": "Figure 1 :",
            "section_rank": 7
        },
        {
            "section": "Figure 1 :",
            "text": "Figure 1: \u03b1 s (M 2 z ) determined from the scaled hadronic width of the Z boson, in LO, NLO and NNLO QCD, as a function of the renormalization scale x \u00b5 = \u00b5/M z ; taken from Ref. [9].",
            "paragraph_rank": 48,
            "section_rank": 7
        },
        {
            "text": "Figure 2 :",
            "section_rank": 8
        },
        {
            "section": "Figure 2 :",
            "text": "Figure 2: Examples of Feynman diagrams describing hadronic final states in processes which are used to measure \u03b1 s ; taken from Ref.[9].",
            "paragraph_rank": 49,
            "section_rank": 8,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b8",
                    "start": 132,
                    "text": "[9]",
                    "end": 135
                }
            ]
        },
        {
            "text": "Figure 3 :",
            "section_rank": 9
        },
        {
            "section": "Figure 3 :",
            "text": "Figure 3: Left: Thrust distribution measured by the ALEPH experiment at LEP for centre-of-mass energies between 91.2 and 206 GeV together with QCD predictions at NLO plus next-to-leading-logarithmic approximation (NLLA). The plotted distributions are scaled by arbitrary factors for presentation; taken from Ref.[16]. Right: Comparison of ALEPH data for the Thrust distribution to the fitted QCD prediction (NLO+NLLA) obtained at parton level (solid line) and corrected for hadronization effects (shaded band). The width of the band covers the predictions using different hadronization models; taken from Ref.[17].",
            "paragraph_rank": 50,
            "section_rank": 9,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b15",
                    "start": 312,
                    "text": "[16]",
                    "end": 316
                },
                {
                    "type": "bibr",
                    "ref_id": "b16",
                    "start": 609,
                    "text": "[17]",
                    "end": 613
                }
            ]
        },
        {
            "text": "Figure 5 :",
            "section_rank": 10
        },
        {
            "section": "Figure 5 :",
            "text": "Figure 5: A selection of world average values for \u03b1 s (M 2 z ) as a function of time; the yellow band indicates the current world average value [2] of \u03b1 s (M 2 z ) = 0.1185 \u00b1 0.0006.",
            "paragraph_rank": 51,
            "section_rank": 10
        },
        {
            "text": "Figure 6 :",
            "section_rank": 11
        },
        {
            "section": "Figure 6 :",
            "text": "Figure 6: Left: List of individual \u03b1 s (M 2 z ) measurements and their comparison to the world average from Ref. [9] in 2000; Right: current status of the running of \u03b1 s , as summarised in Ref. [2].",
            "paragraph_rank": 52,
            "section_rank": 11
        },
        {
            "text": "Acknowledgements",
            "section_rank": 13
        },
        {
            "section": "Acknowledgements",
            "text": "I would like to thank G. Rolandi and L. Maiani for inviting me to contribute to this collection of essays on the Standard Theory. I would also like to thank S. Bethke and G. Salam for many interesting discussions on the topic of \u03b1 s and for their comments on the manuscript. Finally, my thanks go to R. Miquel for providing me the coloured version of Fig. 3 (right).",
            "paragraph_rank": 53,
            "section_rank": 13,
            "ref_spans": [
                {
                    "type": "figure",
                    "start": 351,
                    "text": "Fig. 3 (right)",
                    "end": 365
                }
            ]
        },
        {
            "text": "A contribution to:",
            "section_rank": 15
        },
        {
            "section": "A contribution to:",
            "text": "The Standard Theory up to the Higgs discovery -60 years of CERN L. Maiani and G. Rolandi, eds.",
            "paragraph_rank": 54,
            "section_rank": 15
        }
    ]
}