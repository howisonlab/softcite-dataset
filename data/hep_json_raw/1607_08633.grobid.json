{
    "level": "paragraph",
    "abstract": [
        {
            "text": "Classification of jets as originating from light-flavor or heavy-flavor quarks is an important task for inferring the nature of particles produced in high-energy collisions. The large and variable dimensionality of the data provided by the tracking detectors makes this task difficult. The current state-of-the-art tools require expert data-reduction to convert the data into a fixed low-dimensional form that can be effectively managed by shallow classifiers. We study the application of deep networks to this task, attempting classification at several levels of data, starting from a raw list of tracks. We find that the highest-level lowest-dimensionality expert information sacrifices information needed for classification, that the performance of current state-of-the-art taggers can be matched or slightly exceeded by deep-network-based taggers using only track and vertex information, that classification using only lowest-level highest-dimensionality tracking information remains a difficult task for deep networks, and that adding lower-level track and vertex information to the classifiers provides a significant boost in performance compared to the state-of-the-art.",
            "paragraph_rank": 1,
            "section_rank": 1
        }
    ],
    "body_text": [
        {
            "text": "INTRODUCTION",
            "section_rank": 2
        },
        {
            "section": "INTRODUCTION",
            "text": "The search for new particles and interactions at the energy frontier is a rich program with enormous discovery potential. The power to discover this hypothetical new physics relies crucially on the ability to infer the nature of the interaction and the particles produced from the data provided by the detectors which surround the point of collision. One critical element is jet flavor classification, the distinction between hadronic jets produced from light-flavor (u, d, and s) and heavy-flavor (c and b) quarks. Such classification plays a central role in identifying heavy-flavor signals and reducing the enormous backgrounds from light-flavor processes [1,2].",
            "paragraph_rank": 2,
            "section_rank": 2,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b0",
                    "start": 659,
                    "text": "[1,",
                    "end": 662
                },
                {
                    "type": "bibr",
                    "ref_id": "b1",
                    "start": 662,
                    "text": "2]",
                    "end": 664
                }
            ]
        },
        {
            "section": "INTRODUCTION",
            "text": "Jets originating from heavy-flavor quarks tend to produce longer-lived particles than those found in jets from light-flavor quarks; these long-lived particles have decays which are displaced from the primary vertex. To identify such vertices, the central tracking chamber measures the trajectories of charged particles which allows for the reconstruction of vertex locations. The large and varying number of particles in a jet leads to a difficult classification problem with large and variable dimensionality without a natural ordering. The first step in typical approaches involves vertex-finding algorithms [3], which transform the task into one of reduced, but still variable, dimensionality. Finally, most state-of-the-art jet flavor classification tools used by experiments [4,5] rely heavily on expert-designed features which fix and fur- * The first two authors contributed equally ther reduce the dimensionality before applying shallow machine-learning techniques. Such techniques have excellent performance, but are primarily motivated by historical limitations in the ability of shallow learning methods to handle high-and variable-dimensionality datasets.",
            "paragraph_rank": 3,
            "section_rank": 2,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b3",
                    "start": 610,
                    "text": "[3]",
                    "end": 613
                },
                {
                    "type": "bibr",
                    "ref_id": "b4",
                    "start": 780,
                    "text": "[4,",
                    "end": 783
                },
                {
                    "type": "bibr",
                    "ref_id": "b5",
                    "start": 783,
                    "text": "5]",
                    "end": 785
                }
            ]
        },
        {
            "section": "INTRODUCTION",
            "text": "Recent applications of deep learning to similar problems in high-energy physics [6][7][8][9], combined with the lack of a clear analytical theory to provide dimensional reduction without loss of information, suggests that deep learning techniques applied to the lower-level higherdimensional data could yield improvements in the performance of jet-flavor classification algorithms. General methods for designing and applying recurrent and recursive neural networks to problems with data of variable size or structure have been developed in Refs. [10][11][12][13][14], and applied systematically to a variety of problems ranging from natural language processing [15], to protein structure prediction [16][17][18][19] to prediction of molecular properties [20,21] and to the game of go [22]; previous studies have discussed the extension of such strategies to tasks involving tracks in high energy physics [23,24].",
            "paragraph_rank": 4,
            "section_rank": 2,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b6",
                    "start": 80,
                    "text": "[6]",
                    "end": 83
                },
                {
                    "type": "bibr",
                    "ref_id": "b7",
                    "start": 83,
                    "text": "[7]",
                    "end": 86
                },
                {
                    "type": "bibr",
                    "ref_id": "b8",
                    "start": 86,
                    "text": "[8]",
                    "end": 89
                },
                {
                    "type": "bibr",
                    "ref_id": "b9",
                    "start": 89,
                    "text": "[9]",
                    "end": 92
                },
                {
                    "type": "bibr",
                    "ref_id": "b10",
                    "start": 546,
                    "text": "[10]",
                    "end": 550
                },
                {
                    "type": "bibr",
                    "ref_id": "b11",
                    "start": 550,
                    "text": "[11]",
                    "end": 554
                },
                {
                    "type": "bibr",
                    "ref_id": "b12",
                    "start": 554,
                    "text": "[12]",
                    "end": 558
                },
                {
                    "type": "bibr",
                    "ref_id": "b13",
                    "start": 558,
                    "text": "[13]",
                    "end": 562
                },
                {
                    "type": "bibr",
                    "ref_id": "b14",
                    "start": 562,
                    "text": "[14]",
                    "end": 566
                },
                {
                    "type": "bibr",
                    "ref_id": "b15",
                    "start": 661,
                    "text": "[15]",
                    "end": 665
                },
                {
                    "type": "bibr",
                    "ref_id": "b16",
                    "start": 699,
                    "text": "[16]",
                    "end": 703
                },
                {
                    "type": "bibr",
                    "ref_id": "b17",
                    "start": 703,
                    "text": "[17]",
                    "end": 707
                },
                {
                    "type": "bibr",
                    "ref_id": "b18",
                    "start": 707,
                    "text": "[18]",
                    "end": 711
                },
                {
                    "type": "bibr",
                    "ref_id": "b19",
                    "start": 711,
                    "text": "[19]",
                    "end": 715
                },
                {
                    "type": "bibr",
                    "ref_id": "b20",
                    "start": 754,
                    "text": "[20,",
                    "end": 758
                },
                {
                    "type": "bibr",
                    "ref_id": "b21",
                    "start": 758,
                    "text": "21]",
                    "end": 761
                },
                {
                    "type": "bibr",
                    "ref_id": "b22",
                    "start": 784,
                    "text": "[22]",
                    "end": 788
                },
                {
                    "type": "bibr",
                    "ref_id": "b23",
                    "start": 904,
                    "text": "[23,",
                    "end": 908
                },
                {
                    "type": "bibr",
                    "ref_id": "b24",
                    "start": 908,
                    "text": "24]",
                    "end": 911
                }
            ]
        },
        {
            "section": "INTRODUCTION",
            "text": "In this paper, we apply several deep learning techniques to this problem using a structured dataset with features at three levels of processing (tracks, vertices, expert), each of which is a strict function of the previous level(s). The data at the highest level of processing, with smallest dimensionality, is intended to mirror the typical approach used currently by experimental collaborations. The multi-layered structure of the dataset allows us to draw conclusions about the information loss at each stage of processing, and to gauge the ability of machine learning tools to find solutions in the lower-and higherdimensional levels. These lessons can guide the design of flavor-tagging algorithms used by experiments.",
            "paragraph_rank": 5,
            "section_rank": 2
        },
        {
            "text": "CLASSIFICATION AND DIMENSIONALITY",
            "section_rank": 3
        },
        {
            "section": "CLASSIFICATION AND DIMENSIONALITY",
            "text": "The task of the machine learning (ML) algorithm is to identify a function f (x) : IR N \u2192 IR 1 whose domain is the observed data at some level of processing (with potentially very large dimensionality N ) and which evaluates to a single real value that contains the information necessary to perform the classification. Perfect classification is not expected; instead, the upper bound is performance which matches classification provided by the true likelihood ratio between bottom (b) and light-flavor quarks (q): P (x|b)/P (x|q) evaluated in the high-dimensional domain.",
            "paragraph_rank": 6,
            "section_rank": 3
        },
        {
            "section": "CLASSIFICATION AND DIMENSIONALITY",
            "text": "Though we lack knowledge of an analytical expression for the likelihood, in principle one could recover such a function from labeled datasets with trivial algorithms, by estimating the likelihood directly in the original highdimensional space. In practice, this requires an enormous amount of data, making it impractical for problems with anything but the smallest dimensionality in their feature space.",
            "paragraph_rank": 7,
            "section_rank": 3
        },
        {
            "section": "CLASSIFICATION AND DIMENSIONALITY",
            "text": "Machine learning plays a critical role in approximating the function f (x) which reduces the dimensionality of the space to unity by finding the critical information needed to perform the classification task. Such a function may disregard some of the information from the higherdimensional space if it is not pertinent to the task at hand. However, for very high dimensional spaces (greater than \u2248 50), the task remains very difficult, and until the recent advent of deep learning it appeared to be overwhelming, though it can still require the generation of large samples of training data.",
            "paragraph_rank": 8,
            "section_rank": 3
        },
        {
            "section": "CLASSIFICATION AND DIMENSIONALITY",
            "text": "It would be very powerful to compare the performance of a given solution to the theoretical upper limit on performance, provided by the true likelihood. Unfortunately, without knowledge of the true likelihood, it is difficult to assess how well the ML algorithm has captured the necessary information. For this reason, in the studies presented here and in earlier work [6,7,9], we built structured datasets with at least two levels of dimensionality: an initial sample with lower-level data at high dimensionality and a reduced sample with expert features at lower dimensionality. Importantly, the expert features are a strict function of the lower-level features, so that they contain a subset of the information. The expertise lies solely in the design of the dimensionality-reducing function, without providing any new information.",
            "paragraph_rank": 9,
            "section_rank": 3,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b6",
                    "start": 369,
                    "text": "[6,",
                    "end": 372
                },
                {
                    "type": "bibr",
                    "ref_id": "b7",
                    "start": 372,
                    "text": "7,",
                    "end": 374
                },
                {
                    "type": "bibr",
                    "ref_id": "b9",
                    "start": 374,
                    "text": "9]",
                    "end": 376
                }
            ]
        },
        {
            "section": "CLASSIFICATION AND DIMENSIONALITY",
            "text": "This structure allows us to draw revealing conclusions about the information content of the intermediate and expert-level information and the power of classifiers to extract it. Since the higher-level data contains a subset of the information and benefits from expert knowledge, it can provide the basis for a performance benchmark for the tools using lower-level data in place of the unknown true likelihood. Therefore, if the performance of tools using lower-level data fails to match that of tools using the higher-level data (or a combination of both kinds of data), then we may conclude that the tools using the lower-level data have failed to extract the complete information. On the other hand, if the performance of tools using lower-level data exceeds that of tools using the higher-level data, then we may conclude that the higherlevel data does not contain all of the information relevant to the classification task, or that it has transformed the problem into a more difficult learning task for the algorithms considered. Regardless of the reason, in this case the transformation to the higher-level lower-dimensional data has failed in its goal.",
            "paragraph_rank": 10,
            "section_rank": 3
        },
        {
            "text": "DATA",
            "section_rank": 4
        },
        {
            "section": "DATA",
            "text": "Training samples were produced with realistic simulation tools widely used in particle physics. Samples were generated for three classes of jet:",
            "paragraph_rank": 11,
            "section_rank": 4
        },
        {
            "section": "DATA",
            "text": "\u2022 light-flavor: jets from u, d, s quarks or gluons;",
            "paragraph_rank": 12,
            "section_rank": 4
        },
        {
            "section": "DATA",
            "text": "\u2022 charm: jets from c quarks;",
            "paragraph_rank": 13,
            "section_rank": 4
        },
        {
            "section": "DATA",
            "text": "\u2022 bottom: jets from b quarks.",
            "paragraph_rank": 14,
            "section_rank": 4
        },
        {
            "section": "DATA",
            "text": "Collisions and immediate decays were generated with madgraph5 amc@nlo [25]  These simulated samples are meant to model real particle collisions, but the simulation is not perfectly faithful. To account for mismodeling, experimental applications generate simulated samples using several closely related models and measure the differences between simulation and data with designated calibration samples in which the b, c, or light-jet content is enriched [28,29]. This allows for the tuning and selection of the models to match the data. In the studies presented here no such comparison is possible, as no experimental data is available. Further, the focus of the studies here is on the amount of information lost in the several layers of processing, rather than the absolute level of performance, so the fidelity of the sample is less important. This approach is in line with the machine-learning-based flavor tagging used by both both CMS [30] and ATLAS [31].",
            "paragraph_rank": 15,
            "section_rank": 4,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b25",
                    "start": 70,
                    "text": "[25]",
                    "end": 74
                },
                {
                    "type": "bibr",
                    "ref_id": "b28",
                    "start": 453,
                    "text": "[28,",
                    "end": 457
                },
                {
                    "type": "bibr",
                    "ref_id": "b29",
                    "start": 457,
                    "text": "29]",
                    "end": 460
                },
                {
                    "type": "bibr",
                    "ref_id": "b30",
                    "start": 939,
                    "text": "[30]",
                    "end": 943
                },
                {
                    "type": "bibr",
                    "ref_id": "b31",
                    "start": 954,
                    "text": "[31]",
                    "end": 958
                }
            ]
        },
        {
            "section": "DATA",
            "text": "The delphes detector simulation was augmented with a simple tracking model that smears truth particles to yield tracks similar to those expected at ATLAS [32]. Tracks follow helical paths in a perfectly homogeneous 2 T magnetic field. No attempt was made to account for material interactions or remove strange hadrons. As a result the tracking model lacks the sophistication of models developed by LHC collaborations while retaining enough realism to run vertex reconstruction and compare the relative performance of various machine learning approaches.",
            "paragraph_rank": 16,
            "section_rank": 4,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b32",
                    "start": 154,
                    "text": "[32]",
                    "end": 158
                }
            ]
        },
        {
            "section": "DATA",
            "text": "Jets are reconstructed from calorimeter energy deposits with the anti-k T clustering algorithm [33] as implemented in FastJet [34], with a distance parameter of R = 0.4. Tracks are assigned to jets by requiring that they be within a cone of \u2206R \u2261 (\u2206\u03b7 2 + \u2206\u03c6 2 ) 1/2 < 0.4 of the jet axis. Jets are labeled by matching to partons within a cone of \u2206R < 0.5. If a b or c quark is found within this cone the jet is labeled bottom or charm flavor respectively, with b taking precedence if both are found. Otherwise the jet is labeled light-flavor.",
            "paragraph_rank": 17,
            "section_rank": 4,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b33",
                    "start": 95,
                    "text": "[33]",
                    "end": 99
                },
                {
                    "type": "bibr",
                    "ref_id": "b34",
                    "start": 126,
                    "text": "[34]",
                    "end": 130
                }
            ]
        },
        {
            "section": "DATA",
            "text": "To reconstruct secondary vertices, we use the adaptive vertex reconstruction algorithm implemented in RAVE v6.24 [3,35]. The algorithm begins by fitting a primary vertex to the event and removing all compatible tracks. For each jet, secondary vertices are then reconstructed iteratively: a vertex is fit to a point that minimizes \u03c7 2 with respect to all tracks in the jet, less compatible tracks are down-weighted, and the vertex fit is repeated until the fit stabilizes.",
            "paragraph_rank": 18,
            "section_rank": 4,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b3",
                    "start": 113,
                    "text": "[3,",
                    "end": 116
                },
                {
                    "type": "bibr",
                    "ref_id": "b35",
                    "start": 116,
                    "text": "35]",
                    "end": 119
                }
            ]
        },
        {
            "section": "DATA",
            "text": "Since a b-hadron decay typically cascades through a c-hadron, jets may include multiple secondary vertices. To account for this, tracks with large weights in the secondary vertex fit are removed and the fit is repeated with the remaining tracks. The process repeats until all tracks are assigned to a secondary vertex.",
            "paragraph_rank": 19,
            "section_rank": 4
        },
        {
            "section": "DATA",
            "text": "As described earlier, we organize the information in three levels of decreasing dimensionality and increasing pre-processing using expert knowledge where each level is a strict function of the lower-level information. The classification is done per-jet rather than per-event, and at every level the transverse momentum and pseudorapidity of the jet is included.",
            "paragraph_rank": 20,
            "section_rank": 4
        },
        {
            "section": "DATA",
            "text": "The lowest-level information considered is the list of reconstructed tracks. Each helical track has five parameters in addition to a 5 \u00d7 5 symmetric covariance matrix with 15 independent entries. In the case of flavor tagging, two of the five track parameters are of particular importance: the track d 0 , defined as distance between the primary vertex and the track at perigee, projected into the plane transverse to the beam; and the track z 0 , the analogous quantity projected parallel to the beam. The number of tracks varies from 1 to 33 in these samples, with a mean of 4.",
            "paragraph_rank": 21,
            "section_rank": 4
        },
        {
            "section": "DATA",
            "text": "The intermediate-level information comes from the output of the vertexing algorithm. The features are the vertex mass, number of tracks associated to the vertex, the fraction of the total energy in jet tracks which is associated to those tracks, vertex displacement, vertex displacement significance, and angular separation in \u2206\u03b7 and \u2206\u03c6 with respect to the jet axis for each vertex. In cases where both low and intermediate level features are used the track to vertex association weight is also included. The number of vertices varies from 1 to 13 in these samples, with a mean of 1.5.",
            "paragraph_rank": 22,
            "section_rank": 4
        },
        {
            "section": "DATA",
            "text": "The highest-level information is designed to model the typical features used as shallow network or BDT inputs in current experimental applications; see Fig. 1 for distributions of these features for each jet class. There are fourteen such features:",
            "paragraph_rank": 23,
            "section_rank": 4,
            "ref_spans": [
                {
                    "type": "figure",
                    "ref_id": "fig_0",
                    "start": 152,
                    "text": "Fig. 1",
                    "end": 158
                }
            ]
        },
        {
            "section": "DATA",
            "text": "\u2022 The d 0 and z 0 significance of the 2nd and 3rd tracks attached to a vertex, ordered by d 0 significance.",
            "paragraph_rank": 24,
            "section_rank": 4
        },
        {
            "section": "DATA",
            "text": "\u2022 The number of tracks with d 0 significance greater than 1.8\u03c3.",
            "paragraph_rank": 25,
            "section_rank": 4
        },
        {
            "section": "DATA",
            "text": "\u2022 The JetProb [36] light jet probability, calculated as the product over all tracks in the jet of the probability for a given track to have come from a lightquark jet.",
            "paragraph_rank": 26,
            "section_rank": 4,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b36",
                    "start": 14,
                    "text": "[36]",
                    "end": 18
                }
            ]
        },
        {
            "section": "DATA",
            "text": "\u2022 The width of the jet in \u03b7 and \u03c6, calculated for \u03b7 as",
            "paragraph_rank": 27,
            "section_rank": 4
        },
        {
            "section": "DATA",
            "text": "and analogously for \u03c6.",
            "paragraph_rank": 28,
            "section_rank": 4
        },
        {
            "section": "DATA",
            "text": "\u2022 The combined vertex significance,",
            "paragraph_rank": 29,
            "section_rank": 4
        },
        {
            "section": "DATA",
            "text": "where d is the vertex displacement and \u03c3 is the uncertainty in vertex position along the displacement axis.",
            "paragraph_rank": 30,
            "section_rank": 4
        },
        {
            "section": "DATA",
            "text": "\u2022 The number of secondary vertices.",
            "paragraph_rank": 31,
            "section_rank": 4
        },
        {
            "section": "DATA",
            "text": "\u2022 The number of secondary-vertex tracks.",
            "paragraph_rank": 32,
            "section_rank": 4
        },
        {
            "section": "DATA",
            "text": "\u2022 The angular distance \u2206R between the jet and vertex.",
            "paragraph_rank": 33,
            "section_rank": 4
        },
        {
            "section": "DATA",
            "text": "\u2022 The decay chain mass, calculated as the sum of the invariant masses of all reconstructed vertices, where particles are assigned the pion mass.",
            "paragraph_rank": 34,
            "section_rank": 4
        },
        {
            "section": "DATA",
            "text": "\u2022 The fraction of the total track energy in the jet associated to secondary vertices 1 The dataset consists of 10 million labeled simulated jets. The corresponding target labels are light-flavor, charm, and bottom. The data contains 44, 11, 45 percent of each class respectively. This data is available from the UCI Machine Learning in Physics Web portal at http://mlphysics.ics.uci.edu/.",
            "paragraph_rank": 35,
            "section_rank": 4
        },
        {
            "text": "METHODS",
            "section_rank": 5
        },
        {
            "section": "METHODS",
            "text": "When training neural nets, we typically use 8 million jets for training, one million for validation, and one million for testing. Since there are three labels but we are interested in the study of signal vs background and classification, the labels are converted to binary by mapping bottom quark to one, and both charm and light quark to zero. We study the light-quark and charm-quark rejection separately.",
            "paragraph_rank": 36,
            "section_rank": 5
        },
        {
            "text": "Machine Learning Approaches",
            "section_rank": 6
        },
        {
            "section": "Machine Learning Approaches",
            "text": "To each simulated collision is attached a set of tracks and a set of vertices. This poses challenges for a machine learning approach in that the size of these sets is variable as seen in Fig. 2 and the sets are unordered, although as usual an arbitrary order is often used to list their elements. To address and explore these challenges we use three different deep learning approaches: feedforward neural networks, recurrent neural networks with LSTM (Long Short Term Memory) units, and outer recursive neural networks.",
            "paragraph_rank": 37,
            "section_rank": 6,
            "ref_spans": [
                {
                    "type": "figure",
                    "ref_id": "fig_1",
                    "start": 187,
                    "text": "Fig. 2",
                    "end": 193
                }
            ]
        },
        {
            "text": "Feedforward Neural Networks",
            "section_rank": 7
        },
        {
            "section": "Feedforward Neural Networks",
            "text": "The track feature set and the vertex feature set have variable size for a given collision. However, the struc- ture of feedforward networks requires a fixed-size input to make predictions. Thus the use of feedforward neural networks requires first an arbitrary ordering and then a capping of the size of the input set, with zero padding for sets that are smaller than the capped size. To resolve the arbitrary ordering the tracks were sorted by decreasing absolute d 0 significance. This ordering also ensures that tracks from a secondary vertex, which typically have large d 0 , are unlikely to be removed by the capping. Random ordering before adding the padding was also tested but the performance was lower than using the absolute d 0 significance ordering.",
            "paragraph_rank": 38,
            "section_rank": 7
        },
        {
            "section": "Feedforward Neural Networks",
            "text": "To create a fixed size input, the number of tracks was limited to 15, from a maximum of 33. Using 15 as the cutoff value ensures that 99.97% of the samples preserve all their original tracks; see Fig. 2. Tracks are associated to vertices by concatenating the track parameters with those from the associated vertex. Before training, the samples are preprocessed by shifting and scaling such that each feature has a mean of zero and a standard deviation of one. Jets with fewer than 15  average of 1.5; see Fig. 2.",
            "paragraph_rank": 39,
            "section_rank": 7,
            "ref_spans": [
                {
                    "type": "figure",
                    "ref_id": "fig_1",
                    "start": 196,
                    "text": "Fig. 2",
                    "end": 202
                },
                {
                    "type": "bibr",
                    "ref_id": "b15",
                    "start": 481,
                    "text": "15",
                    "end": 483
                },
                {
                    "type": "figure",
                    "ref_id": "fig_1",
                    "start": 505,
                    "text": "Fig. 2",
                    "end": 511
                }
            ]
        },
        {
            "section": "Feedforward Neural Networks",
            "text": "The feedforward neural networks were trained on 8 million training jets with one million more for validation using stochastic gradient descent with mini-batches of 100 samples. They were trained for 100 epochs and the best model was chosen based on the validation error. Momentum for the weights updated was used and linearly increased from zero to a final value over a specified number of epochs. Learning rate decayed linearly from 0.01 to a final value starting and finishing at a specified number of epochs. Dropout (in which nodes are removed during training) with values of p from 0.0 to 0.5 were used at several combinations of layers to add regularization [37,38]. These networks had 9 fully connected hidden layers with rectified linear units [39,40].",
            "paragraph_rank": 40,
            "section_rank": 7,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b37",
                    "start": 664,
                    "text": "[37,",
                    "end": 668
                },
                {
                    "type": "bibr",
                    "ref_id": "b38",
                    "start": 668,
                    "text": "38]",
                    "end": 671
                },
                {
                    "type": "bibr",
                    "ref_id": "b39",
                    "start": 752,
                    "text": "[39,",
                    "end": 756
                },
                {
                    "type": "bibr",
                    "ref_id": "b40",
                    "start": 756,
                    "text": "40]",
                    "end": 759
                }
            ]
        },
        {
            "section": "Feedforward Neural Networks",
            "text": "Shared weights for each track object were used at the first layer to preserve information about the structure of the data; see Fig 3. When adding the vertex and high level variables to the tracks, these were also included within the set of variables with shared weights. The weights for all but the last layer were initialized from a uniform distribution between [\u2212 6/C, 6/C] where C is the total number of incoming and outgoing connections [41]. The weights for the last layer were initialized from a uniform distribution between -0.05 and 0.05. A manual optimization was performed over all the hyperparameters to find the best model.",
            "paragraph_rank": 41,
            "section_rank": 7,
            "ref_spans": [
                {
                    "type": "figure",
                    "start": 127,
                    "text": "Fig 3.",
                    "end": 133
                },
                {
                    "type": "bibr",
                    "ref_id": "b41",
                    "start": 441,
                    "text": "[41]",
                    "end": 445
                }
            ]
        },
        {
            "text": "LSTM Networks",
            "section_rank": 8
        },
        {
            "section": "LSTM Networks",
            "text": "A natural approach to handling variable-sized input is to use recursive neural networks. Broadly speaking, there are two classes of approaches for designing such architectures, the inner approach and the outer approach [42]. In the inner approach, neural networks are used inside the data graphs to crawl the corresponding edges and compute the final output. This process requires the data graphs to be directed and acyclic. Since here the data consists of a set of vertices and tracks, we first convert the data into a sequence by ordering the vertices and tracks as described previously and then use recursive neural networks for sequences, in combination with Long Short Term Memory units [43,44] to better capture long range dependencies. In the underlying acyclic graph, the variables associated with each node are a function of the variables associated with the parent nodes. Each such function can be parameterized by a neural network. Because the directed acyclic graph has a regular structure, the same network can be applied at different locations of the graph, ultimately producing the LSTM grid network in Figure 4.",
            "paragraph_rank": 42,
            "section_rank": 8,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b42",
                    "start": 219,
                    "text": "[42]",
                    "end": 223
                },
                {
                    "type": "bibr",
                    "ref_id": "b43",
                    "start": 692,
                    "text": "[43,",
                    "end": 696
                },
                {
                    "type": "bibr",
                    "ref_id": "b44",
                    "start": 696,
                    "text": "44]",
                    "end": 699
                },
                {
                    "type": "figure",
                    "start": 1118,
                    "text": "Figure 4",
                    "end": 1126
                }
            ]
        },
        {
            "section": "LSTM Networks",
            "text": "We follow the standard implementation of LSTMs with three gates (input, forget, output) and initialize the connections to random orthonormal matrices. The input data consists of a sequence of concatenated track, vertex, and expert features (or different sub-combinations thereof) which are sorted by their absolute d 0 significance, as was the case with the fully connected models. The main difference is that we do not need zeropadding as the LSTM networks can handle sequences of arbitrary length, though we retain the same maximum of 15 tracks for comparability 2 . The final model consists of one LSTM layer comprising between 50 and 500 neurons, and a feedforward neural network with one to four hidden layers that receives its input from the LSTM network and produces the final predictions (where each layer has between 50 and 500 units). We add dropout layers in between the LSTM and each hidden fully connected layer. For hyperparameter-optimization we performed a random search over these parameters as well as the individual dropout rates that are part of the model. We trained the LSTM networks for 100 epochs using SGD 2 Since only 1 in roughly 3000 jets will be truncated by the 15 track maximum this should have a negligible effect on the AUC with a momentum of 0.9 and decay the step-size parameter from initially 2 \u2022 10 \u22123 down to 10 \u22124 over the course of training.",
            "paragraph_rank": 43,
            "section_rank": 8,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b1",
                    "start": 1131,
                    "text": "2",
                    "end": 1132
                }
            ]
        },
        {
            "text": "Outer Recursive Networks",
            "section_rank": 9
        },
        {
            "section": "Outer Recursive Networks",
            "text": "Alternatively, to handle inputs of variable size, we can use an outer recursive approach, where neural networks are built in a direction perpendicular to the original data graph, with horizontal weight sharing. The outer approach can be used to build more symmetric deep architectures; see Fig. 5. For instance, in our case the input consists of up to 15 tracks, from which we can sample all possible pairs of tracks and use a shared neural network that processes these in the first layer of the outer approach. In this case, there are at most 15 2 = 105 unordered pairs, or 210 ordered pairs, which is manageable especially considering that there is a single network shared by all pairs. Using ordered pairs would yield the most symmetric overall network. At the next level of the architecture, one can for instance use a network for each track t i that combines the outputs of all the networks from the first layer associated with pairs containing t i , and so forth. In the second level of the outer architecture, for simplicity here we use a fully connected feedforward network that computes the final output using the outputs of all the pair networks. More specifically, for each data sample we compute the list of stacked track features for all 210 pairs and process each pair with a shared nonlinear hidden layer (with 5 to 20 neurons). The resulting outputs for all pairs are then concatenated and fed into a multilayer perceptron as was the case for the LSTM models, with one to four hidden layers containing between 100 and 600 hidden units. We again use dropout layers in between the hidden layers and opti-mize the dropout rates and network depth and size using random search.",
            "paragraph_rank": 44,
            "section_rank": 9,
            "ref_spans": [
                {
                    "type": "figure",
                    "ref_id": "fig_2",
                    "start": 290,
                    "text": "Fig. 5",
                    "end": 296
                },
                {
                    "type": "bibr",
                    "start": 544,
                    "text": "15 2",
                    "end": 548
                }
            ]
        },
        {
            "text": "Hardware and Software Implementations",
            "section_rank": 10
        },
        {
            "section": "Hardware and Software Implementations",
            "text": "All computations were performed using machines with Intel Xeon cores, NVIDIA Titan graphics processors, and 64 GB memory. All neural networks were trained using the GPU-accelerated Theano software library [45] and, for the feed forward neural networks, also the Keras software library [46].",
            "paragraph_rank": 45,
            "section_rank": 10,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b45",
                    "start": 205,
                    "text": "[45]",
                    "end": 209
                },
                {
                    "type": "bibr",
                    "start": 285,
                    "text": "[46]",
                    "end": 289
                }
            ]
        },
        {
            "text": "RESULTS",
            "section_rank": 11
        },
        {
            "section": "RESULTS",
            "text": "The best feedforward neural networks have 9 fully connected hidden layers with 400 rectified linear units and a single sigmoid unit at the end. On the first layer the networks have shared weights. The first five tracks have one set of shared weights per track, tracks 6 to 10 have a second set of shared weights per track and the last five tracks have a third set of shared weights per track. They have a momentum term of 0 which starts to linearly increase at the first epoch and reaches its final value of 0.5 at epoch 100. Initially, the learning rate is set at 0.01 and, starting at epoch 80, it is linearly decreased to a final value of 0.001 at epoch 100. Dropout was used in the first two layers with a value of p=0.3. The same architecture was used across all the combinations of features except in the case of using only high level features, in which case the first layer is fully connected without any shared weights.",
            "paragraph_rank": 46,
            "section_rank": 11
        },
        {
            "section": "RESULTS",
            "text": "We found that the main characteristic of the best LSTM models is a relatively small size of the hidden state representation of the LSTM module (about 70 units), while the size of the MLP, which is sitting on top of it, is of secondary importance for overall performance of the model. The best models using the outer recursive approach contain between two and three hidden layers on top of the shared-weight layer (which operates on all paired tracks) and those contain 17 or more neurons.",
            "paragraph_rank": 47,
            "section_rank": 11
        },
        {
            "section": "RESULTS",
            "text": "Final results are shown in Table I. The metric used is the Area Under the Curve (AUC), calculated in signal efficiency versus background efficiency, where a larger AUC indicates better performance. In Fig. 6, the signal efficiency is shown versus background rejection, the inverse of background efficiency. Figures 7 and 8 show the efficiency versus jet p T and pseudorapidity for fixed values of background rejection. Figures 9 and 10 show the rejection versus jet p T and pseudorapidity for fixed values of signal efficiency.",
            "paragraph_rank": 48,
            "section_rank": 11,
            "ref_spans": [
                {
                    "type": "table",
                    "start": 27,
                    "text": "Table I",
                    "end": 34
                },
                {
                    "type": "figure",
                    "ref_id": "fig_3",
                    "start": 201,
                    "text": "Fig. 6",
                    "end": 207
                },
                {
                    "type": "figure",
                    "ref_id": "fig_4",
                    "start": 307,
                    "text": "Figures 7 and 8",
                    "end": 322
                },
                {
                    "type": "figure",
                    "ref_id": "fig_0",
                    "start": 419,
                    "text": "Figures 9 and 10",
                    "end": 435
                }
            ]
        },
        {
            "section": "RESULTS",
            "text": "The results can be analyzed to draw conclusions regarding the power of the learning algorithms to extract information at different levels of preprocessing, and to compare the three learning approaches. I: Performance results for networks using track-level, vertex-level or expert-level information. In each case the jet pT and pseudorapidity are also used. Shown for each method is the Area Under the Curve (AUC), the integral of the background efficiency versus signal efficiency, which have a statistical uncertainty of 0.001 or less. Signal efficiency and background rejections are shown in Figs. 6-10.",
            "paragraph_rank": 49,
            "section_rank": 11
        },
        {
            "text": "Inputs",
            "section_rank": 12
        },
        {
            "section": "Inputs",
            "text": "Technique The state-of-the-art performance is represented by the networks which use only the expert-level features. Networks using only tracking or vertexing features do not match this performance, though networks using both tracking and vertexing do slightly exceed it. In addition, networks which combine expert-level information with track and/or vertex information outperform the expertonly benchmark, in some cases by a significant margin.",
            "paragraph_rank": 50,
            "section_rank": 12
        },
        {
            "section": "Inputs",
            "text": "For any given set of features, the feedforward deep networks most often give the best performance, though in some cases by a small margin over the LSTM approach. This may be somewhat unexpected since LSTMs were created to handle variable sized input data as is the case here. We must note, however, that unlike truly sequential data like speech or text there is no natural order in the data that we are working on. The tracks have been ordered by absolute d 0 significance, which tends to cluster tracks belonging to the same vertex, but a sequential model with this ordering may not be superior to processing tracks in parallel, as in the connected DNN with tied weights.",
            "paragraph_rank": 51,
            "section_rank": 12
        },
        {
            "section": "Inputs",
            "text": "While one cannot probe the strategy of the ML algorithm, it is possible to compare distributions of events categorized as signal-like by the different algorithms in order to understand how the classification is being accomplished. To compare distributions between different algorithms, we study simulated events with equivalent background rejection, see Fig. 11 for a comparison of the selected regions in the expert features for classifiers with and without the lower-level information.",
            "paragraph_rank": 52,
            "section_rank": 12,
            "ref_spans": [
                {
                    "type": "figure",
                    "ref_id": "fig_0",
                    "start": 354,
                    "text": "Fig. 11",
                    "end": 361
                }
            ]
        },
        {
            "text": "DISCUSSION",
            "section_rank": 13
        },
        {
            "section": "DISCUSSION",
            "text": "Our results support four conclusions.",
            "paragraph_rank": 53,
            "section_rank": 13
        },
        {
            "section": "DISCUSSION",
            "text": "The existing expert strategies for dimensional reduction sacrifice or distort useful information. Networks which include lower-level information outperform networks using exclusively higher-level information. For example, if the vertex-level information contained all of the classification power of the track-level information but with lower dimensionality, one would expect the vertex-only network to match the performance of the tracks-and-vertex network, as the lower-dimensional problem should be simpler to learn. Instead, networks using tracks and vertices outperform those which use only vertices. Similarly, networks using tracks and expert fea- tures outperform those with only expert features. We note that these conclusions apply to the expert strategies considered here, and in the case of the simulated environment we have studied; however, we feel that both are representative of the current state-of-the-art.",
            "paragraph_rank": 54,
            "section_rank": 13
        },
        {
            "section": "DISCUSSION",
            "text": "The task remains a challenge for deep networks. Networks which use only the lower-level information do not match the performance of networks which use the higher-level information. Since the higher-level features are strict functions of the lower-level features, the lowerlevel features are a superset of the information contained in the high-level features. The performance of the networks which use the high-level features then provides a baseline against which to measure the ability of the network to extract the relevant information in the more difficult higher-dimensional space of lower-level features. Networks using only track information do not match the performance of those which use only the high-level features (but note that track-only networks outperform vertex-only networks, giving a clue as to the area of difficulty).",
            "paragraph_rank": 55,
            "section_rank": 13
        },
        {
            "section": "DISCUSSION",
            "text": "Networks using track and vertex information outperform those with expert features. Networks trained with track and vertex information but without  the benefit of expert-level guidance and dimensional reduction manage to achieve better performance than those which use only expert-level features. This is remarkable, as the dimensionality of the tracks+vertices features is very large and expert-only networks represent the current state-of-the-art. Note, however, that for high signal efficiency (> 75%) the expert-only networks outperform the networks using tracks+vertices. Networks which combine expert features with low-level information have the best performance. Combining the lowest-level information for completeness with the low-dimensional hints from expert features significantly outperforms the state-of-the-art networks which use only expert features. While in principle all of the information exists in the lowest-level features and it should be possible to train a network which matches or exceeds this performance without expert knowledge, this is neither necessary nor desirable. Expert knowledge exists and is well-established, and there is no reason to discard it.",
            "paragraph_rank": 56,
            "section_rank": 13
        },
        {
            "section": "DISCUSSION",
            "text": "In addition, this expert guidance encourages the network to identify discrimination strategies based on wellunderstood properties of the jet flavor problem and decreases the likelihood of relying on learning strategies based on spurious or poorly-modeled corners of the space. We note that the use of high-dimensional lower-level data will require careful validation of the simulation models; reasonable strategies exist, such as a combination of the validation of individual features in one-dimensional projections with validation of the network output in control samples, which probes the use of information in multifeature correlations. These improvements in the performance of the tagger can give important boosts to physics studies which rely on the identification of jet flavor.  ",
            "paragraph_rank": 57,
            "section_rank": 13
        },
        {
            "text": "FIG. 1 :",
            "section_rank": 14
        },
        {
            "section": "FIG. 1 :",
            "text": "FIG. 1: Distributions in simulated samples of high-level jet flavor variables widely used to discriminate between jets from light-flavor and heavy-flavor quarks.",
            "paragraph_rank": 58,
            "section_rank": 14
        },
        {
            "text": "FIG. 2 :",
            "section_rank": 15
        },
        {
            "section": "FIG. 2 :",
            "text": "FIG. 2: Top: Distribution of the number of tracks associated to a jet in simulated samples. Bottom: Distribution of the number of vertices associated to a jet in simulated samples, before and after removing tracks which exceed the maximum allowed value of 15.",
            "paragraph_rank": 59,
            "section_rank": 15
        },
        {
            "text": "FIG. 5 :",
            "section_rank": 16
        },
        {
            "section": "FIG. 5 :",
            "text": "FIG. 5: Architecture of the outer recursive networks as described in the text.",
            "paragraph_rank": 60,
            "section_rank": 16
        },
        {
            "text": "FIG. 6 :",
            "section_rank": 17
        },
        {
            "section": "FIG. 6 :",
            "text": "FIG. 6: Signal efficiency versus background rejection (inverse of efficiency) for deep networks trained on track-level, vertex-level or expert-level features. The top pane shows the performance for b-quarks versus light-flavor quarks, the bottom pane for b-quarks versus c-quarks.",
            "paragraph_rank": 61,
            "section_rank": 17
        },
        {
            "text": "FIG. 7 :",
            "section_rank": 18
        },
        {
            "section": "FIG. 7 :",
            "text": "FIG. 7:Signal efficiency versus minimum jet pT relative to light quarks (top) or charm quarks (bottom). In each case, efficiency is shown for fixed values of background rejection for networks trained with only expert features or networks trained with all features (tracks, vertices and expert features).",
            "paragraph_rank": 62,
            "section_rank": 18
        },
        {
            "text": "FIG. 8 :",
            "section_rank": 19
        },
        {
            "section": "FIG. 8 :",
            "text": "FIG. 8:Signal efficiency versus minimum jet pseudo-rapidity relative to light quarks (top) or charm quarks (bottom). In each case, efficiency is shown for fixed values of background rejection for networks trained with only expert features or networks trained with all features (tracks, vertices and expert features).",
            "paragraph_rank": 63,
            "section_rank": 19
        },
        {
            "text": "FIG. 9 :",
            "section_rank": 20
        },
        {
            "section": "FIG. 9 :",
            "text": "FIG. 9:Rejection of light quarks (top) or charm quarks (bottom) versus minimum jet pT. In each case, rejection is shown for fixed values of signal efficiency for networks trained with only expert features or networks trained with all features (tracks, vertices and expert features).",
            "paragraph_rank": 64,
            "section_rank": 20
        },
        {
            "text": "FIG. 11 :",
            "section_rank": 21
        },
        {
            "section": "FIG. 11 :",
            "text": "FIG. 11:Distributions of expert-level features for heavy-flavor and light-flavor classes. Also shown are distributions of lightflavor and charm jets surviving network threshold selections chosen to given rejection of 10 and 50, for networks using only expert information and networks using expert information in addition to lower-level information.",
            "paragraph_rank": 65,
            "section_rank": 21
        },
        {
            "text": "future work; here we assume that pileup effects will not alter the relative performance of the dif- ferent methods, and is not likely to have a large impact at luminosities recorded to date, given effective techniques to isolate pileup tracks and vertices from the vertices of interest to this study.",
            "section_rank": 22
        },
        {
            "text": "tracks are zeropadded after preprocessing. After the cut on the number of tracks, the maximum number of vertices is 12 with an . . . . . . . . . . . . Feedforward neural network architecture. In the first layer, connections of the same color represent the same value of the shared weight. The others layers are fully connected without shared weights.",
            "paragraph_rank": 67,
            "section_rank": 23
        },
        {
            "text": "TABLE",
            "section_rank": 24
        },
        {
            "text": "Rejection of light quarks (top) or charm quarks (bottom) versus minimum jet pseudo-rapidity. In each case, rejection is shown for fixed values of signal efficiency for networks trained with only expert features or networks trained with all features (tracks, vertices and expert features).",
            "paragraph_rank": 69,
            "section_rank": 25
        },
        {
            "text": "The vertex energy fraction is not a strict fraction; it can be greater than unity if tracks are assigned to multiple vertices.",
            "paragraph_rank": 70,
            "section_rank": 25
        },
        {
            "text": "ACKNOWLEDGEMENTS",
            "section_rank": 27
        },
        {
            "section": "ACKNOWLEDGEMENTS",
            "text": "We thank David Kirkby, Gordon Watts, Shimon Whiteson, David Casper, and Kyle Cranmer for useful comments and helpful discussion. We thank Yuzo Kanomata for computing support. We also wish to acknowledge a hardware grant from NVIDIA and NSF grant IIS-1321053 to PB.",
            "paragraph_rank": 71,
            "section_rank": 27
        }
    ]
}