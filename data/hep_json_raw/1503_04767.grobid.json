{
    "level": "paragraph",
    "abstract": [
        {
            "text": "Many extensions to the Standard Model of particle physics hypothesize the existence of new low-mass particles. Typically there are few theoretical constraints on the mass or lifetime of such particles. This requires the experimentalist to perform a search in which both the mass and lifetime of the particle are unknown. Such searches for low-mass particles are complicated by the possible presence of resonances and other non-monotonic backgrounds. This paper presents a simple and fast approach to assigning significance and setting limits in such searches.",
            "paragraph_rank": 1,
            "section_rank": 1
        }
    ],
    "body_text": [
        {
            "text": "Introduction",
            "section_rank": 2
        },
        {
            "section": "Introduction",
            "text": "Many extensions to the Standard Model (SM) of particle physics hypothesize the existence of new low-mass particles. Typically there are few theoretical constraints on the mass or lifetime of such particles. Therefore, the experimentalist is required to perform a search in which both the mass and lifetime of the particle are unknown. Such searches for low-mass particles are complicated by the possible presence of resonances and other non-monotonic backgrounds whose probability density functions (PDFs) are not well known or constrained.",
            "paragraph_rank": 2,
            "section_rank": 2
        },
        {
            "section": "Introduction",
            "text": "The null results from numerous searches for new particles that decay into leptons has shifted the focus of many theorists towards leptophobic bosons. For example, Tulin [1] proposes a force that couples weakly to quarks at the QCD scale. Experimentally, this requires a search for a MeV or GeV mass boson that decays to \u03c0 + \u03c0 \u2212 \u03c0 0 . If such a boson decays promptly 1 , then searching for  Figure 1. Simulated PDF including interference between B \u2192 K * \u00b5 \u00b5 and B \u2192 K * \u03c1(\u00b5 \u00b5) for two arbitrary phase differences between the two amplitudes (shown as solid and dashed lines).",
            "paragraph_rank": 3,
            "section_rank": 2,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b0",
                    "start": 169,
                    "text": "[1]",
                    "end": 172
                },
                {
                    "type": "bibr",
                    "ref_id": "b0",
                    "start": 366,
                    "text": "1",
                    "end": 367
                },
                {
                    "type": "figure",
                    "start": 390,
                    "text": "Figure 1",
                    "end": 398
                }
            ]
        },
        {
            "section": "Introduction",
            "text": "it requires dealing with an irreducible resonance background. It is preferable to perform this search with the data blinded; however, the reaction under study is governed by non-perturbative QCD and so the only a priori assumption about the SM PDF that can be made is: any resonance that couples to the same final state as the new boson may contribute to the SM background. The number of known resonances is large and new resonances are still being discovered, even those that only contain light quarks [2]. Furthermore, parametrization of resonances is reaction-dependent and interference effects are often significant. Therefore, the number of nuisance parameters in the background PDF is very large and there will likely always be some component to the PDF that is simply not modeled accurately to better than a few percent. Even in dedicated amplitude analyses performed without blinding the data, it is often difficult to evaluate the systematic uncertainties associated with the fit PDF due to the large number of nuisance parameters. It is also difficult to obtain a fit model which describes the data at the percent level for all masses. Seaches for new bosons decaying to leptons are not immune to the problem of resonances. Consider, e.g., the rare decay B \u2192 K * \u00b5 \u00b5 which is an excellent laboratory to search for a new low-mass boson that couples to mass [3]. This decay can have a contribution from B \u2192 K * \u03c1(\u00b5 \u00b5). Using the published LHCb B \u2192 K * \u00b5 \u00b5 results [4] and the ratio of branching fractions B(B \u2192 K * \u03c1)B(\u03c1 \u2192 \u00b5 \u00b5)/B(B \u2192 K * \u00b5 \u00b5) from the PDG [5] one obtains a predicted yield of B \u2192 K * \u03c1(\u00b5 \u00b5) that is less than one event; however, this calculation ignores interference. Figure 1 shows a toy prediction obtained using the measured branching fractions B(B \u2192 K * \u00b5 \u00b5), B(B \u2192 K * \u03c1) and B(\u03c1 \u2192 \u00b5 \u00b5), and the assumption that the full amplitudes for B \u2192 K * \u00b5 \u00b5 and B \u2192 K * \u03c1(\u00b5 \u00b5) interfere 2 . Naively using the ratio of branching fractions suggests B \u2192 K * \u03c1(\u00b5 \u00b5) is negligible, but the interference cross term could be large enough to generate a \"local\" 5% effect. In principle the \u03c1 contribution could be parametrized (with some uncertainty); however, the branching fractions of many resonance decays to \u00b5 + \u00b5 \u2212 and of B \u2192 K * resonance have yet to be measured and so such contributions would be unconstrained. Since the effect generated can also depend on interference, a full model allowing all resonances to interfere (which requires introducing unknown relative phases for each resonance) must be constructed.",
            "paragraph_rank": 4,
            "section_rank": 2,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b1",
                    "start": 503,
                    "text": "[2]",
                    "end": 506
                },
                {
                    "type": "bibr",
                    "ref_id": "b2",
                    "start": 1366,
                    "text": "[3]",
                    "end": 1369
                },
                {
                    "type": "bibr",
                    "ref_id": "b3",
                    "start": 1472,
                    "text": "[4]",
                    "end": 1475
                },
                {
                    "type": "bibr",
                    "ref_id": "b4",
                    "start": 1564,
                    "text": "[5]",
                    "end": 1567
                },
                {
                    "type": "figure",
                    "start": 1693,
                    "text": "Figure 1",
                    "end": 1701
                },
                {
                    "type": "bibr",
                    "ref_id": "b1",
                    "start": 1907,
                    "text": "2",
                    "end": 1908
                }
            ]
        },
        {
            "section": "Introduction",
            "text": "For the case of displaced decays, the largest background contributions are often due to some mis-reconstructed candidates. These include material interactions, photon conversions, etc. Given that the searches considered in this paper must scan a large mass and lifetime range, predicting (blindly) the full list of displaced backgrounds and parametrizing them will often not be possible. Furthermore, resonances may contribute to the displaced search via decays of beauty or charm particles. E.g., in the aforementioned B \u2192 K * \u00b5 \u00b5 search, it is possible to obtain the K * and \u00b5 + \u00b5 \u2212 from two different B decays at LHCb. This could result in a candidate that contains what appears to be a long-lived charmonium resonance. It is desirable to approach such a search by making as few assumptions as possible about the backgrounds.",
            "paragraph_rank": 5,
            "section_rank": 2
        },
        {
            "section": "Introduction",
            "text": "This paper presents a simple approach for performing a search for a new low-mass particle of unknown mass and lifetime. The method is described in Sec. 2. Obtaining global p-values, including the so-called trials factor or look elsewhere effect, is discussed in Sec. 3, while setting upper limits and the coverage properties of such limits is discussed in Sec. 4. Discussion and summarizing are provided in Sec. 5.",
            "paragraph_rank": 6,
            "section_rank": 2
        },
        {
            "text": "Search Strategy",
            "section_rank": 3
        },
        {
            "section": "Search Strategy",
            "text": "The strategy that will be employed is to perform a scan in mass since the new particle mass is not known. The step size of the scan will be \u03c3 (m)/2, where \u03c3 (m) is the detector mass resolution. For each test mass value a search will be conducted for evidence of a particle with no constraints on its lifetime. For illustrative purposes, the following toy model is used:",
            "paragraph_rank": 7,
            "section_rank": 3
        },
        {
            "section": "Search Strategy",
            "text": "\u2022 the searched region in mass is 1 GeV wide 3 ;",
            "paragraph_rank": 8,
            "section_rank": 3
        },
        {
            "section": "Search Strategy",
            "text": "\u2022 the mass resolution is taken to be \u03c3 (m) = 2 MeV, giving a step size of 1 MeV;",
            "paragraph_rank": 9,
            "section_rank": 3
        },
        {
            "section": "Search Strategy",
            "text": "\u2022 the expected number of prompt events (in the absence of signal) is 1000;",
            "paragraph_rank": 10,
            "section_rank": 3
        },
        {
            "section": "Search Strategy",
            "text": "\u2022 the expected number of displaced events (in the absence of signal) is 100;",
            "paragraph_rank": 11,
            "section_rank": 3
        },
        {
            "section": "Search Strategy",
            "text": "\u2022 both prompt and displaced events are generated uniformly in mass.",
            "paragraph_rank": 12,
            "section_rank": 3
        },
        {
            "section": "Search Strategy",
            "text": "This toy data set is sufficient for illustrating how the proposed method works. Some variations are considered later in the text.",
            "paragraph_rank": 13,
            "section_rank": 3
        },
        {
            "text": "Mass",
            "section_rank": 4
        },
        {
            "section": "Mass",
            "text": "As an alternative to fitting the mass distribution in data with a PDF whose accuracy is a priori unknown and difficult to validate, I propose the following simple approach: For each test mass value in the scan, m(test), use the mass sidebands to estimate the expected background 4 yield. The signal region (where signal events would be observed if a new particle of mass close to m(test) exists) is defined as |m(test) \u2212 m| < 2\u03c3 (m), while the background region is the sidebands defined as 3\u03c3 (m) < |m(test) \u2212 m| < (2x + 3)\u03c3 (m), where x is the ratio of the size of the signal to sideband regions (these regions do not need to be the same size). The factors that contribute to choosing x are discussed at the end of this subsection.",
            "paragraph_rank": 14,
            "section_rank": 4
        },
        {
            "section": "Mass",
            "text": "If the background PDF is approximately linear in the region |m(test) \u2212 m| < (2x + 3)\u03c3 (m), then the observed yield in the background region provides an estimate of the expected yield in the signal region. The presence of resonances and some types of background will violate this socalled local-linear approximation; this is dealt with below. Under the local-linear assumption, the likelihood is given by",
            "paragraph_rank": 15,
            "section_rank": 4
        },
        {
            "section": "Mass",
            "text": "where P denotes the Poisson PDF, n (s,b) denote the yields in the (signal, background) regions and s and b are the signal and background rates in the signal region. It is straightforward to account for uncertainty in the relationship between the sideband and search window background yields, or equivalently in deviations from local linearity in the background PDF, by augmenting the likelihood as follows:",
            "paragraph_rank": 16,
            "section_rank": 4
        },
        {
            "section": "Mass",
            "text": "where G denotes the Gaussian PDF and \u03c3 y is the uncertainty on the scaling factor between the background yield in the signal and background regions 5 . If one can estimate the size of possible deviations from local linearity, e.g., due to resonance contributions, then these can be incorporated into the likelihood via \u03c3 y . The profile likelihood can then be used to obtain the significance of any excess of events (see, e.g., Ref. [6]) and/or to set limits [7]. See Appendix A for a more detailed discussion on the likelihood. In the previous paragraph the nominal background PDF is taken to be linear but in principle any background PDF can be used. For a low-statistics search the linear approximation will be sufficient; however, for cases where the sample sizes are large, then the uncertainty due to deviations from local-linearity may be large compared to the statistical uncertainties. All that is required to use this method is that based on the observed yields in the sidebands, an estimate for the expected yield in the signal region can be obtained. If a non-linear background PDF is chosen as nominal, then x represents the scaling factor between the expected background yields in the sidebands and signal region, which may no longer be simply the ratio of the size of the regions. The \u03c3 y term is again the uncertainty in the scale factor (independent of how x is defined). See Sec. 5 for discussion on non-linear background PDFs. For the remainder of the method overview, I will assume a nominal background PDF of local linear.",
            "paragraph_rank": 17,
            "section_rank": 4,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b4",
                    "start": 148,
                    "text": "5",
                    "end": 149
                },
                {
                    "type": "bibr",
                    "ref_id": "b5",
                    "start": 433,
                    "text": "[6]",
                    "end": 436
                },
                {
                    "type": "bibr",
                    "ref_id": "b6",
                    "start": 459,
                    "text": "[7]",
                    "end": 462
                }
            ]
        },
        {
            "section": "Mass",
            "text": "Appendix B provides a study of the deviation from local linearity due to resonance contributions. These deviations are a function of the width of the resonance, \u0393, the fraction of the total PDF near m(test) due to the resonance, and of \u03c3 (m) and x. The conclusions are as follows: \u2022 for \u0393/\u03c3 (m) < 5, the deviation from local-linear is large unless the resonance contribution is small (narrow resonances must be vetoed);",
            "paragraph_rank": 18,
            "section_rank": 4
        },
        {
            "section": "Mass",
            "text": "\u2022 for 5 < \u0393/\u03c3 (m) < 20, the local-linear approximation is valid at the 10% level for moderate resonance contributions, but not valid if the resonance is dominant.",
            "paragraph_rank": 19,
            "section_rank": 4
        },
        {
            "section": "Mass",
            "text": "Assuming a O(10%) value is chosen for \u03c3 y , then wide resonances can effectively be ignored (including those that have yet to be discovered). If nothing is known about the possible size of a contribution from a narrow resonance, then the region near the resonance mass should be vetoed. If some limits can be placed on the size of a resonance contribution, then this veto may not be required. Such limits must be determined in each analysis independently. The key point is that narrow resonances are typically well known (including their branching fractions to many final states), whereas wide resonances are not well known. The sideband approach allows the analyst to ignore wide resonances by accounting for their non-linear effects via the \u03c3 y term in the likelihood. Narrow resonances likely must be vetoed, but there are few of these and their properties are well measured. Intermediate-width resonances are also accounted for automatically by \u03c3 y provided they do not dominate the local PDF. Such cases should be rare and can be studied using alternative decays of the resonance or via the data directly using bins O(10\u03c3 (m)) wide.",
            "paragraph_rank": 20,
            "section_rank": 4
        },
        {
            "section": "Mass",
            "text": "Other categories of background that have a broad peaking structure are also handled naturally in this approach. The study in Appendix B can be applied to non-resonant backgrounds with the same conclusions drawn: only a background whose peak is narrow relative to \u03c3 (m) must be vetoed; all other backgrounds are accounted for via \u03c3 y (the analyst does not need to know what these are or account for them individually). E.g., in the B \u2192 K * \u00b5 \u00b5 search, partially reconstructed charm particle decays can be ignored, while any J/\u03c8 \u2192 \u00b5 + \u00b5 \u2212 contribution must be vetoed.",
            "paragraph_rank": 21,
            "section_rank": 4
        },
        {
            "section": "Mass",
            "text": "The parameter x should be optimized for each analysis. The larger x is chosen to be, the less statistical uncertainty there is on the background rate; however, this also increases the size of the region in which local-linearity is assumed. Figure 2 shows a comparison of using sidebands to estimate b to when b is known (which I assume here is not possible; this is added for illustrative purposes to show the best possible performance which could be obtained if b could be known). Setting \u03c3 y /x = 0.1 typically loses very little power (at most 10-20% sensitivity to the signal rate relative to the ideal situation of a known background PDF), so a good rule of thumb would be to choose x as large as possible such that \u03c3 y /x = 0.1 is still valid.",
            "paragraph_rank": 22,
            "section_rank": 4,
            "ref_spans": [
                {
                    "type": "figure",
                    "ref_id": "fig_1",
                    "start": 240,
                    "text": "Figure 2",
                    "end": 248
                }
            ]
        },
        {
            "text": "Lifetime",
            "section_rank": 5
        },
        {
            "section": "Lifetime",
            "text": "I now proceed to considering lifetime, \u03c4, information and assume that the background can be factorized into two components: (1) a prompt background where the signal candidate's children all originate from the same spatial point as the rest of the final state and (2) a displaced background where they do not. The lifetime PDF for type (1) is given by the detector resolution. The lifetime PDF for type (2) is assumed to be unknown. In principle it may be obtainable from the mass sidebands; however, looking at these sidebands is forbidden in a blind analysis when the mass is unknown. Furthermore, in many cases the displaced background data will be sparse making obtaining a reliable estimate of its PDF (which likely depends on m) impossible even if the data is not blinded.",
            "paragraph_rank": 23,
            "section_rank": 5
        },
        {
            "section": "Lifetime",
            "text": "If the lifetime distribution of both prompt and displaced backgrounds is the same in the signal and background regions, then a non-parametric two-sample goodness-of-fit test can be used to test the null hypothesis at each m(test). Appendix C discusses several such distribution-free tests, while Appendix D shows the results of applying these to the toy data set described above. The conclusion of these studies is that such tests are far from optimal and require introducing the assumption that the displaced background \u03c4 PDF does not vary with m (which may not be true and would be difficult to validate).",
            "paragraph_rank": 24,
            "section_rank": 5
        },
        {
            "section": "Lifetime",
            "text": "An alternative (and simple) approach is to define two \u03c4 regions at each m(test): (1) a prompt region (e.g., \u03c4 < 3\u03c3 (\u03c4)) and (2) a displaced region (\u03c4 > 3\u03c3 (\u03c4)). The mass sidebands in each region can be used to estimate the expected background rate. The joint likelihood is defined as",
            "paragraph_rank": 25,
            "section_rank": 5
        },
        {
            "section": "Lifetime",
            "text": "i.e., the likelihood is the product of the likelihoods from each region individually (each obtained in the same manner as discussed in the previous section). The profile likelihood (\u039b), which is the ratio of the maximum likelihood with the signal rate fixed to the maximum likelihood with the signal rate free to vary (see Appendix A), for this two-region test is the product of the profile likelihoods from each region. This is true because no assumption is made about the lifetime of the new particle and so there is no assumed relationship about the signal yields in each region. The asymptotic distribution of \u22122 log \u039b is a \u03c7 2 with two DOFs. Appendix D shows that this simple approach, which only uses the lifetime information to determine which region each candidate falls in, is nearly optimal except when \u03c4 \u223c \u03c3 (\u03c4).",
            "paragraph_rank": 26,
            "section_rank": 5
        },
        {
            "section": "Lifetime",
            "text": "The approach presented in this paper only assumes that variation in the lifetime distribution vs mass is slow enough that the linear approximation holds in both the prompt and displaced regions. The possible deviations from linearity are accounted for by \u03c3 y . In this paper I use a constant \u03c3 y in the notation but \u03c3 y is allowed to depend on m and different values of \u03c3 y can be used in the prompt and displaced regions. This is also true of x: it can be chosen to be different values for different test masses and in the prompt and displaced regions. Using variable \u03c3 y and x does not introduce any additional complexity.",
            "paragraph_rank": 27,
            "section_rank": 5
        },
        {
            "section": "Lifetime",
            "text": "Finally, I note that one can run both the two-region profile likelihood test and a two-sample test. Appendix D shows that this approach provides a small increase in performance in the region near \u03c4 \u223c \u03c3 (\u03c4); however, there is an important assumption required to use the two-sample test. This approach requires that all lifetime PDFs are the same in the signal and background regions, i.e., that locally the \u03c4 PDF is independent of m. This may not be true for displaced backgrounds and would be difficult to validate unless the number of displaced-background candidates is large. Given that the gain in performance is small and the additional complexity introduced into the analysis is nonnegligible, I conclude that unless one has a reason to expect \u03c4 \u223c \u03c3 (\u03c4) and a method for validating the \u03c4 PDF m dependence, that the two-region profile likelihood test is the best option.",
            "paragraph_rank": 28,
            "section_rank": 5
        },
        {
            "section": "Lifetime",
            "text": "While it may be surprising to the reader that such a simple approach performs so well, it is often the case that analysis-specific information provides great power. In this case, factorizing the background into two categories using the known detector resolution is key to this search. Note that this procedure allows the analyst to completely ignore the m and \u03c4 distributions for any background that does not form a narrow peak relative to \u03c3 (m). One instead relies on the fact that such backgrounds will populate the signal and sideband regions and that locally their PDFs are approximately linear. The possible deviation from linearity is incorporated into the likelihood via \u03c3 y . Narrow peaking backgrounds, e.g., J/\u03c8 \u2192 \u00b5 \u00b5 in the B \u2192 K * \u00b5 \u00b5 search, must be vetoed in each lifetime region unless it can be shown (or known) that they can only contribute to one region. Such backgrounds should be few and easy to identify.",
            "paragraph_rank": 29,
            "section_rank": 5
        },
        {
            "text": "p-Values",
            "section_rank": 6
        },
        {
            "section": "p-Values",
            "text": "The full procedure involves first determining the local p-values at each m(test), then obtaining the global p-value of the most significant excess observed in the full mass range. An outline of the procedure is as follows:",
            "paragraph_rank": 30,
            "section_rank": 6
        },
        {
            "section": "p-Values",
            "text": "\u2022 The full mass range is scanned in steps of \u03c3 (m)/2. , x are the inputs to the local two-region likelihood.",
            "paragraph_rank": 31,
            "section_rank": 6
        },
        {
            "section": "p-Values",
            "text": "\u2022 The profile likelihood provides the local test statistic (see Appendix A). Since the lifetime of the new particle is unknown, there is no assumed relationship between the signal rate in the prompt and displaced regions.",
            "paragraph_rank": 32,
            "section_rank": 6
        },
        {
            "section": "p-Values",
            "text": "\u2022 For each m(test) an approximate local p-value is obtained using the asymptotic formula for the profile likelihood test statistic. As discussed below, the accuracy of the asymptotic formula for this test only needs to be good enough to properly select the most significant local excess.",
            "paragraph_rank": 33,
            "section_rank": 6
        },
        {
            "section": "p-Values",
            "text": "\u2022 The minimum p-value from the full mass scan is selected as the test statistic to which a significance is to be assigned. This approach is motivated by the assumption that there is at most one new particle contributing to the data sample.",
            "paragraph_rank": 34,
            "section_rank": 6
        },
        {
            "section": "p-Values",
            "text": "The global p-value is not the minimum local p-value as this would ignore the so-called trials factor, or look elsewhere effect, induced by the fact that a large number of tests have been performed. Appendix E discusses the fact that this test has been designed such that it can be run on 10M data sets for O(1000) m(test) values in about 2 hours on a single CPU core. This permits determining the global p-value without the need for using asymptotic formulae. The only reliance on asymptotic formulae is in selecting the minimum local p-value; therefore, the accuracy of the asymptotic formula only needs to be sufficient to properly select the most significant local excess. There is no need to interpret the local p-values as probabilities under the null hypothesis. Figure 3 shows the local p-values obtained from a single simulated toy data set. Since the step size in m(test) is smaller than the signal and sideband regions, neighboring tests are correlated. This produces a jagged-looking distribution. The test mass near 520 has the minimum p-value so would be selected to be assigned a significance in this data set. To obtain the significance the procedure is as follows:",
            "paragraph_rank": 35,
            "section_rank": 6,
            "ref_spans": [
                {
                    "type": "figure",
                    "ref_id": "fig_3",
                    "start": 769,
                    "text": "Figure 3",
                    "end": 777
                }
            ]
        },
        {
            "section": "p-Values",
            "text": "\u2022 Obtain the minimum local p-value from the data as described above.",
            "paragraph_rank": 36,
            "section_rank": 6
        },
        {
            "section": "p-Values",
            "text": "\u2022 Get an approximate null PDF from the data by ignoring the region near the most significant excess and obtaining a smooth PDF from the remaining data (interpolating into the removed region). Below I show that the details of how this is done are not important.",
            "paragraph_rank": 37,
            "section_rank": 6
        },
        {
            "section": "p-Values",
            "text": "\u2022 Generate an ensemble of simulated data sets from the PDF from the previous bullet point. The global p-value is the fraction of simulated data sets that have a minimum local p-value less than that observed in the data.",
            "paragraph_rank": 38,
            "section_rank": 6
        },
        {
            "section": "p-Values",
            "text": "\u2022 The number of generated data sets determines the statistical uncertainty on the global pvaule. The most likely outcome, no evidence for a new particle, will require only O(100) data sets. To confirm > 3\u03c3 requires O(1000) while > 5\u03c3 requires O(10 7 ).",
            "paragraph_rank": 39,
            "section_rank": 6
        },
        {
            "section": "p-Values",
            "text": "\u2022 Confirming > 5\u03c3 can be done on a single CPU core on a laptop. In the unlikely (and exciting) event that not a single simulated data set in 10 8 has a minimum p-value less that that of the data, the asymptotic formula from Ref. [8] can be used to obtain an approximate significance (if one is desired). Figure 4 shows the cumulative distribution of minimum local p-values obtained from 10M toy data sets. One can see that to obtain a global 3\u03c3 in this example requires a local p-value of about e \u221213 . The trials factor then is O(400) which is roughly the width of the full mass range divided by \u03c3 (m) (that is 500 in this example). The asymptotic distribution provides an underestimate of the significance in this example for small p-values due to the small sample sizes used in each local test. Note that the true PDF used in the toy model is local-linear and so setting \u03c3 y /y = 0.1 is an overestimate of the local non-linearity. In this example, such an overestimate produces only a minor shift (towards lower significance; it produces a conservative estimate). Figure 5 shows that the cumulative p-value distribution obtained using an alternate (highly non-monotonic) PDF but the same sample size and detector resolution (and, thus, test mass values). There is very little dependence on the data PDF. This means that the exact PDF used to generate the pseudo data sets is not important. For example, one could simply bin the data in a histogram with  Figure 4. Cumulative distribution of minimum local p-values obtained using simulated (black) toy-model events compared to the (blue) asymptotic expectation [8]. Two variations of the test are shown (both use x = 1): (solid) no uncertainty in the relationship between the signal and background regions and (dashed red) \u03c3 y /y = 0.1. The discrepancy at very small p-values between the asymptotic and solid distributions is due to low statistics in each local test region. The data sets were generated using a PDF that is local linear so it is expected that using \u03c3 y /y = 0.1 is an overestimate of the scaling uncertainty which results in an underestimate of the significance.",
            "paragraph_rank": 40,
            "section_rank": 6,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b7",
                    "start": 229,
                    "text": "[8]",
                    "end": 232
                },
                {
                    "type": "figure",
                    "start": 304,
                    "text": "Figure 4",
                    "end": 312
                },
                {
                    "type": "figure",
                    "start": 1067,
                    "text": "Figure 5",
                    "end": 1075
                },
                {
                    "type": "figure",
                    "start": 1457,
                    "text": "Figure 4",
                    "end": 1465
                },
                {
                    "type": "bibr",
                    "ref_id": "b7",
                    "start": 1613,
                    "text": "[8]",
                    "end": 1616
                }
            ]
        },
        {
            "section": "p-Values",
            "text": "wide bins (relative to \u03c3 (m)), remove the most-significant excess region, and use spline interpolation to obtain a background PDF (which interpolates into the removed most-significant region). This will be accurate enough to produce a reliable cumulative p-value distribution.",
            "paragraph_rank": 41,
            "section_rank": 6
        },
        {
            "section": "p-Values",
            "text": "To summarize: One can confirm up to > 5\u03c3 without using asymptotic formulae in about an hour on a laptop. Assignment of a significance beyond this level can be done using the asymptotic formula as an estimate (if this is desired). Figure 5 demonstrates that even an oscillatory PDF is handled naturally (I did not input any knowledge of this PDF to the method except that \u03c3 y /y = 0.1) provided the features of the PDF are wide relative to \u03c3 (m).",
            "paragraph_rank": 42,
            "section_rank": 6,
            "ref_spans": [
                {
                    "type": "figure",
                    "start": 230,
                    "text": "Figure 5",
                    "end": 238
                }
            ]
        },
        {
            "text": "Limits",
            "section_rank": 7
        },
        {
            "section": "Limits",
            "text": "Upper limits are to be set as a function of m and \u03c4 for all m. In the event that a globally significant excess is observed, the analyst can additionally perform PDF-based fits to determine estimators for  Figure 5. Cumulative distribution of minimum local p-values obtained using simulated toy-model events distributed according to the PDFs shown at right (black is the nominal linear model, while red is a nonmonotonic alternative). The distribution of local p-values has little dependence on how the data is distributed as expected. This distribution is driven by the size of the mass range being searched and \u03c3 (m). Both results here are shown for \u03c3 y /y = 0.1 and x = 1.",
            "paragraph_rank": 43,
            "section_rank": 7,
            "ref_spans": [
                {
                    "type": "figure",
                    "start": 205,
                    "text": "Figure 5",
                    "end": 213
                }
            ]
        },
        {
            "section": "Limits",
            "text": "the new particle mass and lifetime. The two-region profile likelihood can be used to set the limits after making the following modifications to the likelihood function:",
            "paragraph_rank": 44,
            "section_rank": 7
        },
        {
            "section": "Limits",
            "text": "\u2022 for each value of \u03c4, there is a relationship between the number of signal events expected in the prompt and displaced regions;",
            "paragraph_rank": 45,
            "section_rank": 7
        },
        {
            "section": "Limits",
            "text": "\u2022 a Gaussian term is added to the likelihood to account for uncertainty (due to detector efficiency) in the fraction of signal expected in the prompt and displaced regions;",
            "paragraph_rank": 46,
            "section_rank": 7
        },
        {
            "section": "Limits",
            "text": "\u2022 another Gaussian term is added to the likelihood to account for uncertainty in the absolute detector efficiency scale of the signal (most likely relative to a normalization decay mode).",
            "paragraph_rank": 47,
            "section_rank": 7
        },
        {
            "section": "Limits",
            "text": "The likelihood for each (m(test), \u03c4) is then given by",
            "paragraph_rank": 48,
            "section_rank": 7
        },
        {
            "section": "Limits",
            "text": "where f is the fraction of signal events in the prompt region with expected value from simulation f MC (\u03c4) and uncertainty \u03c3 ( f ), and \u03b5 is the efficiency (typically relative to some normalization reaction) with expected value from simulation \u03b5 MC (\u03c4) and uncertainty \u03c3 (\u03b5). The limits are then set by scanning the profile likelihood using the same method, including the handling of special circumstances, discussed in detail in Ref. [7] but using the likelihood given in Eq. 4.1 6 . For setting limits there is no need to generate 10M toy data sets; thus, I do not provide analytic solutions and, instead, use MINUIT to numerically scan the profile likelihood.",
            "paragraph_rank": 49,
            "section_rank": 7,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b6",
                    "start": 435,
                    "text": "[7]",
                    "end": 438
                },
                {
                    "type": "bibr",
                    "ref_id": "b5",
                    "start": 481,
                    "text": "6",
                    "end": 482
                }
            ]
        },
        {
            "section": "Limits",
            "text": "In the toy analysis I choose to normalize the new particle yield to the observed prompt yield in the full mass region. This emulates the situation where the prompt background is dominantly In the left plot only the prompt region is used with TRolke, while in the right plot only the displaced region is used (TRolke only handles one region). For very small and large \u03c4, using only one region is an excellent approximation to the full method presented here; thus, the agreement between TRolke and this method for these \u03c4 values is expected.",
            "paragraph_rank": 50,
            "section_rank": 7
        },
        {
            "section": "Limits",
            "text": "a well-known SM process. I take the ratio of the efficiency for detecting the new particle to the normalization process to be one. In reality there will be some dependence of \u03b5 on m and \u03c4 and there will be some candidates in the prompt region that do not come from the normalization mode, but these just scale the limits (and contribute to \u03c3 (\u03b5)) so I will not discuss them here. For the cases where the test \u03c4 is \u03c3 (\u03c4) or \u03c3 (\u03c4), to a good approximation only the prompt or displaced region matters. Therefore, in such cases the one-region results obtained using the TRolke class in ROOT [9] should be close to those produced here provided \u03c3 y /y is small. Figure 6 shows that this is the case. With \u03c3 y /y = 0.1, the limits returned by this method are slightly larger than TRolke (which here takes the uncertainty on b to be purely Poisson). If \u03c3 y = 0 then for small or large \u03c4 the limits produced by this method are the same as TRolke. Figure 7 shows how the limits depend on \u03c4 for a single toy-model data set. Since in this example n",
            "paragraph_rank": 51,
            "section_rank": 7,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b8",
                    "start": 587,
                    "text": "[9]",
                    "end": 590
                },
                {
                    "type": "figure",
                    "ref_id": "fig_5",
                    "start": 656,
                    "text": "Figure 6",
                    "end": 664
                },
                {
                    "type": "figure",
                    "ref_id": "fig_6",
                    "start": 938,
                    "text": "Figure 7",
                    "end": 946
                }
            ]
        },
        {
            "section": "Limits",
            "text": ", the limits decrease with increasing \u03c4. Figure 8 shows the dependence of the limits on \u03c4 expected (obtained from the 10M toy data sets generated). For the toy-model data, whose PDF is uniform in m, the expected limits have no m dependence. In general this will not be the case. Figure 9 shows the coverage obtained for various configurations of this method and various background rates expected in the prompt and displaced regions. The coverage properties are good except for at small s. At small s the method over covers but this is expected and unavoidable. Otherwise, the method tends to over cover by only a few percent. Note that the small over coverage shown in Fig. 9 is due to the very low statistics of the samples studied. The possible observations are discrete, and with such low statistics it is not possible to obtain perfect coverage.",
            "paragraph_rank": 52,
            "section_rank": 7,
            "ref_spans": [
                {
                    "type": "figure",
                    "start": 41,
                    "text": "Figure 8",
                    "end": 49
                },
                {
                    "type": "figure",
                    "start": 279,
                    "text": "Figure 9",
                    "end": 287
                },
                {
                    "type": "figure",
                    "start": 669,
                    "text": "Fig. 9",
                    "end": 675
                }
            ]
        },
        {
            "text": "Extensions",
            "section_rank": 8
        },
        {
            "section": "Extensions",
            "text": "The method is not restricted for use where the local-linear approximation is applicable. For example, in high-statistics searches, one could consider using multiple sideband regions for each m(test) and using spline interpolation (of whatever order is sufficiently constrained) instead of the locallinear approximation. Another approach could be to unblind some small fraction of the data and obtain estimates for the background PDF there. However the background estimates are obtained for each m(test), they will have some uncertainty \u03c3 y and so this method can still be applied using Eq. A.10 in Appendix A. There is no restriction to any particular local background shape.",
            "paragraph_rank": 53,
            "section_rank": 8
        },
        {
            "text": "Summary & Discussion",
            "section_rank": 9
        },
        {
            "section": "Summary & Discussion",
            "text": "This paper presents a simple likelihood-based approach for searching for a particle of unknown mass and lifetime in the presence of unknown non-monotonic backgrounds. Instead of exhaustively fitting the data with background PDFs containing hundreds of nuisance parameters, I propose to use the local-linear (or alternative local PDF) assumption and simple sideband counting. Deviations from the nominal local PDF are parametrized via a single parameter \u03c3 y in the likelihood. If \u03c3 y is overestimated, then only minor underestimation of the p-values is found. This allows the analyst to concentrate on a small number of possible narrow peaking backgrounds and effectively ignore all other contributions. The lifetime information is used in a simple two-region approach which is nearly optimal except when \u03c4 \u223c \u03c3 (\u03c4). This permits the avoidance of attempting to parametrize displaced background PDFs from a few sparse observed data. This method is fast enough to verify significance > 5\u03c3 in an hour on a laptop; thus, reliance on asymptotic formulae is not required. Furthermore, when setting limits only the integrated detector efficiency in each lifetime region is required to be determined for each mass. A detailed determination of the uncertainty on the detector efficiency vs lifetime is not required since the \u03c4 information is only used to classify candidates as prompt or displaced. For limit setting, uncertainties on the integrated detector efficiency and on the fraction of events that fall in each region are included in the likelihood and the coverage is shown to be good.",
            "paragraph_rank": 54,
            "section_rank": 9
        },
        {
            "section": "Summary & Discussion",
            "text": "Finally, Fig. 10 shows the low-recoil (high M(\u00b5 \u00b5)) data observed by LHCb in the decay B \u2192 K\u00b5 \u00b5 [14]. There is a clear and sizable contribution from the \u03c8(4160) resonance. The size of this contribution was unexpected. What would have happened if a blind analysis of this data had been performed to search for a new prompt particle that decays to \u00b5 \u00b5? If a fit using a monotonic background PDF and no \u03c8(4160) term had been used 7 , then I estimate that the local p-value near 4200 would have given a local significance of 4 \u2212 5\u03c3 . Using the method presented in this paper (in only the prompt region), I estimate that the p-value is about 0.3(0.4) for \u03c3 y = 0(0.1) for x = 1. This is due to the fact that \u0393(\u03c8(4160)) \u223c 10\u03c3 (m) and so locally the resonant peak only deviates from local-linearity by about 15%. No false claim of a discovery would have been made. Furthermore, given the large number of nuisance parameters and sample size, a fit-based approach would not provide much (possibly no) additional sensitivity to new particles (no matter how much effort was put into developing the fit model).",
            "paragraph_rank": 55,
            "section_rank": 9,
            "ref_spans": [
                {
                    "type": "figure",
                    "start": 9,
                    "text": "Fig. 10",
                    "end": 16
                },
                {
                    "type": "bibr",
                    "ref_id": "b16",
                    "start": 96,
                    "text": "[14]",
                    "end": 100
                },
                {
                    "type": "bibr",
                    "ref_id": "b6",
                    "start": 427,
                    "text": "7",
                    "end": 428
                }
            ]
        },
        {
            "section": "Summary & Discussion",
            "text": "contributions. The more likely scenario would be that a fit would have been performed that contained every \u03c8 state with masses and widths free to vary within their nominal values; various other resonant shape parameters free to vary; and the relative phase of each amplitude free. This would then have required a serious systematic study to determine the p-values. Here I am simply using this as an example of how an unexpectedly large wide resonance contribution is handled naturally in the method presented in this paper.   Figure 11. p-values obtained from the asymptotic formula for the profile likelihood as maximized (black points) numerically using MINUIT and (red line) using the analytic approximation. This example used n b = 50, x = 5 and \u03c3 y /y = 0.1. The (blue squares) show the p-values for \u03c3 y = 0 (no uncertainty in the scaling between regions).",
            "paragraph_rank": 56,
            "section_rank": 9,
            "ref_spans": [
                {
                    "type": "figure",
                    "start": 526,
                    "text": "Figure 11",
                    "end": 535
                }
            ]
        },
        {
            "section": "Summary & Discussion",
            "text": "statistical uncertainty in the background rate. In my tests I find that for \u03c3 y /y = 0.1 this holds for n b up to about 200. For the case where \u03c3 y /y > 1/ \u221a n b , the analytic approximation given is not valid so the value\u015d s,\u0177,b must be found numerically, e.g., using MINUIT. This is a valid approach but increases the CPU time required by a factor O(100). In such cases, however, n b is large enough that the Poisson term for n b can be replaced by a Gaussian term. The likelihood is then given by",
            "paragraph_rank": 57,
            "section_rank": 9
        },
        {
            "section": "Summary & Discussion",
            "text": "where the statistical and scale factor uncertainties on the background rate are now included in a single term \u03c3 2 b = (n b /x 2 )(1 +n b \u03c3 2 y /x 2 ). For large n b this reduces to \u03c3 b = (n b /x 2 )\u03c3 y . The likelihood in this case can again be maximized analytically giving the following results: (\u015d,b) = (n s \u2212n b /x, n b /x) and for s = 0b",
            "paragraph_rank": 58,
            "section_rank": 9
        },
        {
            "section": "Summary & Discussion",
            "text": "Thus, it is possible to provide analytical solutions for all cases. When searching for a new particle the physical region is s \u2265 0 and the test statistic used for discovery is \u039b(s = 0) if\u015d \u2265 0. If\u015d < 0, I take\u015d = 0 which gives \u039b = 1. One would expect this to happen at half of all m(test) values considered which is handled naturally by the pseudoexperiment method when obtaining global p-values. When setting limits, I use the bounded method from Ref. [7] where the increase in the likelihood is taken from s = 0 instead of\u015d for the case wher\u00ea s < 0. This produces limits that are slightly more conservative but also never produces unphysical limits.",
            "paragraph_rank": 59,
            "section_rank": 9,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b6",
                    "start": 453,
                    "text": "[7]",
                    "end": 456
                }
            ]
        },
        {
            "text": "B. Resonances",
            "section_rank": 10
        },
        {
            "section": "B. Resonances",
            "text": "This appendix considers the relationship between deviations from the local-linear approximation due to resonance contributions. Figure 12 shows the expected deviations from local-linear for a resonance with \u0393/\u03c3 (m) = 20. For this case, even if the resonance makes up close to 100% of the total PDF the choice x = 1 is still local-linear to about 10%. For smaller resonance contributions larger values of x are local-linear at this level. Figure 13 and 14 show similar plots for \u0393/\u03c3 (m) = 10 and \u0393/\u03c3 (m) = 5. For the case \u0393/\u03c3 (m) = 10, the local-linear approximation is valid at the 10% level up to resonance contributions of about 50% of the total PDF, while for \u0393/\u03c3 (m) = 5 it is only valid at this level for small resonance contributions.",
            "paragraph_rank": 60,
            "section_rank": 10,
            "ref_spans": [
                {
                    "type": "figure",
                    "ref_id": "fig_1",
                    "start": 128,
                    "text": "Figure 12",
                    "end": 137
                },
                {
                    "type": "figure",
                    "ref_id": "fig_3",
                    "start": 438,
                    "text": "Figure 13 and 14",
                    "end": 454
                }
            ]
        },
        {
            "section": "B. Resonances",
            "text": "These results are not surprising. For the case \u0393/\u03c3 (m) = 5, the signal region (which is \u00b12\u03c3 (m)) contains almost half of the resonance probability. Any large contribution from such a resonance will need to be vetoed. Contributions from wide resonances, however, are safe even if they make up the entire PDF. For resonances with widths in the range 20 < \u0393/\u03c3 (m) < 5, applying a veto of the region |m \u2212 m(resonance)| < \u0393 will typically be safe for any size resonance contribution. However, if it is known (or can be shown) that the resonance is not dominant, then such a veto may not be required. ",
            "paragraph_rank": 61,
            "section_rank": 10
        },
        {
            "text": "C. Two-Sample Non-Parametric Goodness-of-Fit Tests",
            "section_rank": 11
        },
        {
            "section": "C. Two-Sample Non-Parametric Goodness-of-Fit Tests",
            "text": "Assuming that the lifetime distributions of all non-signal PDFs are the same in the signal and background regions, a two-sample non-parametric goodness-of-fit test to the hypothesis that the \u03c4 PDF for the signal and background data is the same can be used to test whether a new particle with a resolvable lifetime is contributing to the data. A well-known test is the Kolmogorov-Smirnov (KS) [10] test which uses as test statistic",
            "paragraph_rank": 62,
            "section_rank": 11,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b9",
                    "start": 392,
                    "text": "[10]",
                    "end": 396
                }
            ]
        },
        {
            "section": "C. Two-Sample Non-Parametric Goodness-of-Fit Tests",
            "text": "where F denotes the cumulative distribution. The asymptotic formula is used in this study to obtain an approximate p-value.",
            "paragraph_rank": 63,
            "section_rank": 11
        },
        {
            "section": "C. Two-Sample Non-Parametric Goodness-of-Fit Tests",
            "text": "Despite the KS test's popularity in particle physics, it is known to not be as powerful as the Cramer-von-Mises (CvM) [11] and Anderson-Darling (AD) [12] tests under many conditions. The CvM and AD tests build statistics",
            "paragraph_rank": 64,
            "section_rank": 11,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b11",
                    "start": 118,
                    "text": "[11]",
                    "end": 122
                },
                {
                    "type": "bibr",
                    "ref_id": "b13",
                    "start": 149,
                    "text": "[12]",
                    "end": 153
                }
            ]
        },
        {
            "section": "C. Two-Sample Non-Parametric Goodness-of-Fit Tests",
            "text": "and",
            "paragraph_rank": 65,
            "section_rank": 11
        },
        {
            "section": "C. Two-Sample Non-Parametric Goodness-of-Fit Tests",
            "text": "respectively. Both are based on the cumulative distributions like the KS test but instead of using only the maximum discrepancy, they use an integrated discrepancy. The AD test provides more weight to the \"tails\". The approximate p-values for these tests are obtained using toy simulated data sets in this study. These two-sample tests are for shape only (in how they are used in this study). To test both size and shape two tests are run: (1) either the KS, CvM or AD tests for a shape comparison of sidebands to search window and (2) Poisson profile likelihood for yield comparisons. As the size and shape tests are uncorrelated (to very good approximation), Fisher's method [13] is used to combine the two p-values to get a single p-value. In this way both shape and size anomalies are tested and a single p-value is obtained.  Figure 15. The (circles,squares) represent the fraction of data sets with p < (3, 5)\u03c3 . The test labels are defined as follows: (opt) the cheat PDF-based likelihood fit; (KS,CvM,AD) the 2-sample tests (see text); (\u039b(2, 3)) the profile likelihood test, including the (2, 3)-region versions; (KS + \u039b(2)) combination of both the KS and profile likelihood tests. N.b., the size of the markers has no meaning; this was done to aid in viewing markers for tests with similar power.",
            "paragraph_rank": 66,
            "section_rank": 11,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b15",
                    "start": 677,
                    "text": "[13]",
                    "end": 681
                },
                {
                    "type": "figure",
                    "start": 831,
                    "text": "Figure 15",
                    "end": 840
                }
            ]
        },
        {
            "text": "\u2022Figure 2 .",
            "section_rank": 12
        },
        {
            "section": "\u2022Figure 2 .",
            "text": "Figure 2. The p-value vs n s for (left) b = 10, (middle) b = 100, (right) b = 1000 for (solid black) b known with no uncertainty, (dotted red) x = 1 and (solid red) x = 5.",
            "paragraph_rank": 67,
            "section_rank": 12
        },
        {
            "text": "\u2022",
            "section_rank": 13
        },
        {
            "section": "\u2022",
            "text": "An independent test is performed for each m(test) where the signal and background regions are defined as |m \u2212 m(test)| < 2\u03c3 (m) and 3\u03c3 (m) < |m(test) \u2212 m| < (2x + 3)\u03c3 (m), respectively (x should be optimized for each analysis).\u2022 The signal and background regions are divided into prompt and displaced sub-regions. The quantities n",
            "paragraph_rank": 68,
            "section_rank": 13
        },
        {
            "text": "Figure 3 .",
            "section_rank": 14
        },
        {
            "section": "Figure 3 .",
            "text": "Figure 3. Local p-values obtained from a single simulated data set (x = 1).",
            "paragraph_rank": 69,
            "section_rank": 14
        },
        {
            "text": "Figure 6 .",
            "section_rank": 15
        },
        {
            "section": "Figure 6 .",
            "text": "Figure 6. Comparison between limits obtained for a single simulated data set from (black) the TRolke class (only Poisson uncertainties) and (dashed red) the method discussed in this work (with \u03c3 y /y = 0.1, x = 1) for (left) test \u03c4 = 0 and (right) \u03c4 = 1000\u03c3 (\u03c4). In the left plot only the prompt region is used with TRolke, while in the right plot only the displaced region is used (TRolke only handles one region). For very small and large \u03c4, using only one region is an excellent approximation to the full method presented here; thus, the agreement between TRolke and this method for these \u03c4 values is expected.",
            "paragraph_rank": 70,
            "section_rank": 15
        },
        {
            "text": "Figure 7 .",
            "section_rank": 16
        },
        {
            "section": "Figure 7 .",
            "text": "Figure 7. Upper limits from a single data set on the ratio of new particle production to the SM reaction for \u03c4 values (lighter to darker) \u03c4 = (0, 1, 5, 10, 50, 100, 500, 1000)\u03c3 (\u03c4).",
            "paragraph_rank": 71,
            "section_rank": 16
        },
        {
            "text": "Figure 8 .Figure 9 .Figure 10 .",
            "section_rank": 17
        },
        {
            "section": "Figure 8 .Figure 9 .Figure 10 .",
            "text": "Figure 8. Expected upper limits on the signal rate relative to the normalization mode vs m for simulated toy-model data (\u03c3 y /y = 0.1, x = 1). The lines are (black) mean, (dark gray) enclose the 1\u03c3 and (light gray) 2\u03c3 intervals. From left to right, top to bottom \u03c4 = (0, 1, 5, 10, 50, 100, 500, 1000)\u03c3 (\u03c4).",
            "paragraph_rank": 72,
            "section_rank": 17
        },
        {
            "text": "Figure 12 .Figure 13 .Figure 14 .",
            "section_rank": 18
        },
        {
            "section": "Figure 12 .Figure 13 .Figure 14 .",
            "text": "Figure 12.Deviations from local-linear for a resonance with m = 1000 MeV, \u0393 = 100 Mev, where \u03c3 (m) = 5 MeV. The left plots show the yield distribution (in arbitrary units) for various choices of resonance contribution fraction. The right plots show the ratio of events expected in a signal region to the prediction from the sidebands for various test masses (x-axis) and for various sideband to signal region size ratios x.",
            "paragraph_rank": 73,
            "section_rank": 18
        },
        {
            "section": "Figure 12 .Figure 13 .Figure 14 .",
            "text": "Throughout this paper I refer to decays as either: (prompt) the separation of the decay point from the production point is too small to be resolved by the detector and (displaced) this separation is resolvable.",
            "paragraph_rank": 74,
            "section_rank": 18
        },
        {
            "section": "Figure 12 .Figure 13 .Figure 14 .",
            "text": "I ignore muon spin states here which produces an overestimate of the size of this interference effect, but for illustrative purposes this model is sufficient.",
            "paragraph_rank": 75,
            "section_rank": 18
        },
        {
            "section": "Figure 12 .Figure 13 .Figure 14 .",
            "text": "I just take the mass region searched to be 0 to 1000 MeV since the absolute mass values do not matter; i.e., shifting the mass window to a range that starts at some allowed kinematic limit has no affect on applying this method. For this toy study I also do not specify what the decay products of the particle being searched for are since these also do not matter apart from identifying which resonances may contribute to the data.4 Any candidate that does not come from the decay of a new particle is considered as a background in the search.",
            "paragraph_rank": 76,
            "section_rank": 18,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b3",
                    "start": 430,
                    "text": "4",
                    "end": 431
                }
            ]
        },
        {
            "section": "Figure 12 .Figure 13 .Figure 14 .",
            "text": "This likelihood can be used for any value of n b ; however, Appendix A shows that for the case where n b is large the background region Poisson term can be replaced with a Gaussian one which results in a much faster algorithm.",
            "paragraph_rank": 77,
            "section_rank": 18
        },
        {
            "section": "Figure 12 .Figure 13 .Figure 14 .",
            "text": "This method actually returns a confidence interval whose lower limit may be > 0. Since the significance is discussed above, here I only study upper limits but the method will produce a lower limit as well.",
            "paragraph_rank": 78,
            "section_rank": 18
        },
        {
            "section": "Figure 12 .Figure 13 .Figure 14 .",
            "text": "This is unlikely to have happened since prior to observing this data one would still have expected some charmonium",
            "paragraph_rank": 79,
            "section_rank": 18
        },
        {
            "section": "Figure 12 .Figure 13 .Figure 14 .",
            "text": "This requires differentiating log L with respect to s and b, setting these to zero, then solving the system of equations.",
            "paragraph_rank": 80,
            "section_rank": 18
        },
        {
            "text": "Acknowledgments",
            "section_rank": 20
        },
        {
            "section": "Acknowledgments",
            "text": "I thank Tim Gershon for useful comments that helped improve this paper. This work was supported by US NSF grant PHY-1306550.",
            "paragraph_rank": 81,
            "section_rank": 20
        },
        {
            "text": "A. Profile Likelihood",
            "section_rank": 22
        },
        {
            "section": "A. Profile Likelihood",
            "text": "Asymptotically, \u22122 log \u039b behaves as a \u03c7 2 with 1 DOF and so an approximate p-value can be obtained from \u039b. N.b., one may worry about the possibility that when s < 0b(s) could become imaginary. See discussion at the end of this appendix on how the s < 0 case is dealt with. It is straightforward to account for uncertainty in the relationship between the sideband and search window yields by augmenting the likelihood as follows:",
            "paragraph_rank": 82,
            "section_rank": 22
        },
        {
            "section": "A. Profile Likelihood",
            "text": "where the Gaussian PDF is defined as",
            "paragraph_rank": 83,
            "section_rank": 22
        },
        {
            "section": "A. Profile Likelihood",
            "text": "Now y is the scale factor that relates the yields in the sideband and search regions whose PDF is taken to be a Gaussian with mean x and uncertainty of \u03c3 y . Following the same approach to maximizing L produces an algebraically intractable set of three equations. Making the approximation that \u03c3 y /y < 1/ \u221a n b , then to leading order in \u03c3 y /\u0177",
            "paragraph_rank": 84,
            "section_rank": 22
        },
        {
            "section": "A. Profile Likelihood",
            "text": "whereb(\u015d, \u03c3 y = 0) is the result given in Eq. A.4 andb(\u015d, \u03c3 y = 0, x \u2192\u0177) uses the same equation but replaces x with\u0177 everywhere. Figure 11 shows that this approximation is accurate out to p-values of about O(10 \u221212 ) when the relative uncertainty on the scale factor is smaller than the relative Figure 15 shows the results of a study of lifetime-only based testing for a given test mass. Pseudodatasets are generated using a Gaussian \u03c4 PDF for prompt backgrounds, and an exponential with effective lifetime of 10\u03c3 (\u03c4) for displaced backgrounds. Signal events are also generated using an exponential PDF (of varying choices of \u03c4) convolved with a Gaussian to mimic the resolution. The parameters for the background are taken to be < n prompt s >= 10, < n displ s >= 0.1 and x = 5. In each case 10 signal events are added. The following tests are studied:",
            "paragraph_rank": 85,
            "section_rank": 22,
            "ref_spans": [
                {
                    "type": "figure",
                    "start": 129,
                    "text": "Figure 11",
                    "end": 138
                },
                {
                    "type": "figure",
                    "start": 296,
                    "text": "Figure 15",
                    "end": 305
                }
            ]
        },
        {
            "text": "D. Lifetime Test Results",
            "section_rank": 23
        },
        {
            "section": "D. Lifetime Test Results",
            "text": "\u2022 For comparison an \"optimal\" test is run where the true background PDF (including for displaced backgrounds) is used. This is a cheat since I assume that the displaced background PDF is unknown. The signal PDF is used but with \u03c4 as a free parameter.",
            "paragraph_rank": 86,
            "section_rank": 23
        },
        {
            "section": "D. Lifetime Test Results",
            "text": "\u2022 Pure shape-based two-sample tests (see Appendix C) are shown in the top left panel. There is not much difference between the KS, CvM and AD tests for this particular data set. As expected these tests provide no power for \u03c4 \u03c3 (\u03c4) and increase in power with increasing \u03c4.",
            "paragraph_rank": 87,
            "section_rank": 23
        },
        {
            "section": "D. Lifetime Test Results",
            "text": "\u2022 Profile likelihood tests are shown in the top right panel. The \u039b test ignores lifetime information (hence its performance is independent of \u03c4). As expected this test is optimal for \u03c4 \u03c3 (\u03c4). The \u039b2 test is a two-region (prompt and displaced) counting experiment, where the likelihoods from each region are combined (via a product as usual). This simple test is nearly optimal except when \u03c4 \u223c \u03c3 (\u03c4). Adding a third region does not improve the performance for this particular displaced background (it may if the displaced background had some additional structure).",
            "paragraph_rank": 88,
            "section_rank": 23
        },
        {
            "section": "D. Lifetime Test Results",
            "text": "\u2022 The bottom left panel shows the results of performing the profile likelihood tests along with the shape-based KS test. This greatly enhances the performance of the KS test.",
            "paragraph_rank": 89,
            "section_rank": 23
        },
        {
            "section": "D. Lifetime Test Results",
            "text": "\u2022 The bottom right panel, however, shows that performing the KS test with the two-region profile likelihood test only provides a small gain in the region \u03c4 \u223c \u03c3 (\u03c4).",
            "paragraph_rank": 90,
            "section_rank": 23
        },
        {
            "section": "D. Lifetime Test Results",
            "text": "While the combination of KS and \u039b2 in theory improves the performance for \u03c4 \u223c \u03c3 (\u03c4), it adds some complexity to the analysis. For example, an additional assumption has now been employed: that all lifetime PDFs are the same in the signal and sideband regions. This is likely to be true for the prompt SM background; however, it may not be true for other types of background. Furthermore, the gain in significance obtained by increasing the number of generated signal events from 10 to 11 for \u03c4 \u223c \u03c3 (\u03c4) is much greater than that obtained by performing the KS test along with the profile likelihood; i.e., the maximal gain in sensitivity in this example of using both the KS and \u039b2 tests is < 10% in signal rate sensitivity. This gain requires adding an assumption which may not be valid and is expected to be difficult to validate/study. Thus, my conclusion is that the nominal test should simply be \u039b2.",
            "paragraph_rank": 91,
            "section_rank": 23
        },
        {
            "text": "E. Fast Algorithm",
            "section_rank": 24
        },
        {
            "section": "E. Fast Algorithm",
            "text": "The two-region profile likelihood method can be made extremely fast. In Appendix A I obtained analytic expressions that maximize the likelihood (including the case where a Gaussian uncertainty is included on the scaling factor that relates the sideband yields to those in the signal region); thus, no numerical minimization routine, e.g., MINUIT, needs to be run. Note that the data can be binned in mass (in both the prompt and displaced regions) to perform this test. This means that time consuming event loops are not required. Furthermore, as one scans the mass range, rather than summing up the yields in the signal and background regions, it is faster to simply subtract the bin(s) removed from each region and then add only the bin(s) added. Combining all of these optimizations results in a test that can be performed on 10M data sets with O(1000) test masses in about 2 hours on a single CPU core. This process is trivially parallelized to run on multiple cores. Therefore, it is possible to confirm a significance of > 5\u03c3 without the need to rely on an asymptotic p-value. This is a desirable feature since for rare decays the asymptotic formulae tend to underestimate the significance for very low p-values.",
            "paragraph_rank": 92,
            "section_rank": 24
        }
    ]
}