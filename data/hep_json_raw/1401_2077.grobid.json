{
    "level": "paragraph",
    "abstract": [
        {
            "text": "We present an overview of a comprehensive analysis framework aimed at performing direct extraction of all possible effective Higgs couplings to neutral electroweak gauge bosons in the decay to electrons and muons, the so called 'golden channel'. Our framework is based primarily on a maximum likelihood method constructed from analytic expressions of the fully differential cross sections for h \u2192 4 and for the dominant irreducible qq \u2192 4 background, where 4 = 2e2\u00b5, 4e, 4\u00b5. Detector effects are included by an explicit convolution of these analytic expressions with the appropriate transfer function over all center of mass variables. Utilizing the full set of observables, we construct an unbinned detector-level likelihood which is continuous in the effective couplings. We consider possible ZZ, Z\u03b3, and \u03b3\u03b3 couplings simultaneously, allowing for general CP odd/even admixtures. A broad overview is given of how the convolution is performed and we discuss the principles and theoretical basis of the framework. This framework can be used in a variety of ways to study Higgs couplings in the golden channel using data obtained at the LHC and other future colliders.",
            "paragraph_rank": 2,
            "section_rank": 1
        }
    ],
    "body_text": [
        {
            "text": "Introduction",
            "section_rank": 2
        },
        {
            "section": "Introduction",
            "text": "The recent discovery of the Higgs boson at the LHC [1,2] with properties resembling those predicted by the Standard Model, shifts our attention to the determination of its precise nature and to establish whether or not the Higgs boson possesses any anomalous couplings to Standard Model particles. In this study we focus on couplings to neutral electroweak gauge bosons. Since these 'anomalous effects' are expected to be small if at all present, constraining or measuring of these couplings should preferably be done through direct parameter extraction with minimal theoretical assumptions. The vast literature  on",
            "paragraph_rank": 3,
            "section_rank": 2,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b0",
                    "start": 51,
                    "text": "[1,",
                    "end": 54
                },
                {
                    "type": "bibr",
                    "ref_id": "b1",
                    "start": 54,
                    "text": "2]",
                    "end": 56
                }
            ]
        },
        {
            "text": "JHEP01(2015)125",
            "section_rank": 3
        },
        {
            "section": "JHEP01(2015)125",
            "text": "Higgs decays to four charged leptons (electrons and muons) through neutral electroweak gauge bosons, suggests that the so called 'golden channel', can be a powerful means towards accomplishing this goal.",
            "paragraph_rank": 4,
            "section_rank": 3
        },
        {
            "section": "JHEP01(2015)125",
            "text": "A number of frameworks have been established utilizing the Matrix Element Method to study the golden channel aiming to determine these potentially anomalous couplings. These primarily rely on Monte Carlo generators such as the JHU generator [13,17,32] or on Madgraph implementations [22,31]. They have the advantage of flexibility to include various Higgs production and decay channels and are especially useful for constructing kinematic discriminators to distinguish between competing hypotheses.",
            "paragraph_rank": 5,
            "section_rank": 3,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b12",
                    "start": 241,
                    "text": "[13,",
                    "end": 245
                },
                {
                    "type": "bibr",
                    "ref_id": "b16",
                    "start": 245,
                    "text": "17,",
                    "end": 248
                },
                {
                    "type": "bibr",
                    "ref_id": "b32",
                    "start": 248,
                    "text": "32]",
                    "end": 251
                },
                {
                    "type": "bibr",
                    "ref_id": "b21",
                    "start": 283,
                    "text": "[22,",
                    "end": 287
                },
                {
                    "type": "bibr",
                    "ref_id": "b31",
                    "start": 287,
                    "text": "31]",
                    "end": 290
                }
            ]
        },
        {
            "section": "JHEP01(2015)125",
            "text": "Focusing on the golden channel only, 1 we propose a novel analysis framework largely based on an analytic implementation. It is designed to maximize the information contained in each event with the aim of direct extraction of the various effective Higgs couplings. It is generally acknowledged in the literature that analytic methods are optimal for performing this direct multi-parameter extraction within practical and reasonable computational processing resources [13,17,32]. In this work, we also demonstrate that within an analytic framework one can readily include the relevant detector effects and obtain a detector-level likelihood function in terms of the full set of observables available in the four lepton final state. This is accomplished by the explicit convolution of analytic expressions for the 'truth level' fully differential cross sections with a transfer function which parametrizes the detector resolution and acceptance effects.",
            "paragraph_rank": 6,
            "section_rank": 3,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b0",
                    "start": 37,
                    "text": "1",
                    "end": 38
                },
                {
                    "type": "bibr",
                    "ref_id": "b12",
                    "start": 467,
                    "text": "[13,",
                    "end": 471
                },
                {
                    "type": "bibr",
                    "ref_id": "b16",
                    "start": 471,
                    "text": "17,",
                    "end": 474
                },
                {
                    "type": "bibr",
                    "ref_id": "b32",
                    "start": 474,
                    "text": "32]",
                    "end": 477
                }
            ]
        },
        {
            "section": "JHEP01(2015)125",
            "text": "This analysis framework has already proved useful in constraining effective Higgs couplings as demonstrated in a recent CMS analysis [38,39]. It was shown that for simplified cases of constraining one or two parameters, our framework gives comparable performance to other established analysis methods for the golden channel [13,17,22,31,32,38,39]. In this work we present an overview of the framework and discuss the principles and theoretical basis. In particular we sketch how the various components of the detector level likelihood are constructed with emphasis on how the convolution integral is performed as well as various validations. How the likelihood can then be used to perform multi-parameter extraction of effective Higgs couplings is also discussed. We hope that the additional features of our framework are also found useful in the next phase of the LHC and future colliders. Much more information on the framework including technical details can be found in [19,35,[37][38][39][40][41].",
            "paragraph_rank": 7,
            "section_rank": 3,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b38",
                    "start": 133,
                    "text": "[38,",
                    "end": 137
                },
                {
                    "type": "bibr",
                    "ref_id": "b39",
                    "start": 137,
                    "text": "39]",
                    "end": 140
                },
                {
                    "type": "bibr",
                    "ref_id": "b12",
                    "start": 324,
                    "text": "[13,",
                    "end": 328
                },
                {
                    "type": "bibr",
                    "ref_id": "b16",
                    "start": 328,
                    "text": "17,",
                    "end": 331
                },
                {
                    "type": "bibr",
                    "ref_id": "b21",
                    "start": 331,
                    "text": "22,",
                    "end": 334
                },
                {
                    "type": "bibr",
                    "ref_id": "b31",
                    "start": 334,
                    "text": "31,",
                    "end": 337
                },
                {
                    "type": "bibr",
                    "ref_id": "b32",
                    "start": 337,
                    "text": "32,",
                    "end": 340
                },
                {
                    "type": "bibr",
                    "ref_id": "b38",
                    "start": 340,
                    "text": "38,",
                    "end": 343
                },
                {
                    "type": "bibr",
                    "ref_id": "b39",
                    "start": 343,
                    "text": "39]",
                    "end": 346
                },
                {
                    "type": "bibr",
                    "ref_id": "b18",
                    "start": 974,
                    "text": "[19,",
                    "end": 978
                },
                {
                    "type": "bibr",
                    "ref_id": "b35",
                    "start": 978,
                    "text": "35,",
                    "end": 981
                },
                {
                    "type": "bibr",
                    "ref_id": "b37",
                    "start": 981,
                    "text": "[37]",
                    "end": 985
                },
                {
                    "type": "bibr",
                    "ref_id": "b38",
                    "start": 985,
                    "text": "[38]",
                    "end": 989
                },
                {
                    "type": "bibr",
                    "ref_id": "b39",
                    "start": 989,
                    "text": "[39]",
                    "end": 993
                },
                {
                    "type": "bibr",
                    "ref_id": "b40",
                    "start": 993,
                    "text": "[40]",
                    "end": 997
                },
                {
                    "type": "bibr",
                    "ref_id": "b41",
                    "start": 997,
                    "text": "[41]",
                    "end": 1001
                }
            ]
        },
        {
            "text": "Overview of framework",
            "section_rank": 4
        },
        {
            "section": "Overview of framework",
            "text": "Though 'truth' level (or generator) studies of h \u2192 4 (4 = 2e2\u00b5, 4e, 4\u00b5) give a good approximate estimate of the expected sensitivity to the Higgs ZZ, Z\u03b3, and \u03b3\u03b3 couplings [37], when analyzing data obtained at the LHC (or future colliders) a detector level likelihood which accounts for the various detector effects is necessary. Since generally detector level likelihoods are obtained via the use of Monte Carlo methods, it becomes difficult to obtain the full multi-dimensional likelihood for the 4 final state. Typically one needs to fill large multi-dimensional templates that require an impractical amount of computing time. There",
            "paragraph_rank": 8,
            "section_rank": 4,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b37",
                    "start": 171,
                    "text": "[37]",
                    "end": 175
                }
            ]
        },
        {
            "text": "JHEP01(2015)125",
            "section_rank": 5
        },
        {
            "section": "JHEP01(2015)125",
            "text": "are also potential collateral binning and 'smoothing' side-effects often associated with these methods. In the case of the golden channel this necessitates the use of kinematic discriminants which 'collapse' the fully multi-dimensional likelihood into two or perhaps three detector level observables [32]. This approach is normally taken to facilitate the inclusion of detector effects, but is not optimal when fitting to a large number of parameters simultaneously [17]. This is unfortunate in the case of the golden channel where in principle there are twelve observables which can be used to extract a large number of parameters at once, including their correlations. It would be satisfying and useful to have a framework which is free of these issues and capable of utilizing all available information in the four lepton final state at detector level.",
            "paragraph_rank": 9,
            "section_rank": 5,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b32",
                    "start": 300,
                    "text": "[32]",
                    "end": 304
                },
                {
                    "type": "bibr",
                    "ref_id": "b16",
                    "start": 466,
                    "text": "[17]",
                    "end": 470
                }
            ]
        },
        {
            "text": "From 'truth' to 'detector' level",
            "section_rank": 6
        },
        {
            "section": "From 'truth' to 'detector' level",
            "text": "This is accomplished in our framework by performing an explicit convolution of the generator ('truth') level probability density, formed out of the signal and background differential cross sections, with a transfer function which encapsulates the relevant detector effects. This can be represented schematically as follows,",
            "paragraph_rank": 10,
            "section_rank": 6
        },
        {
            "section": "From 'truth' to 'detector' level",
            "text": "Here we take X to represent the full set of center of mass variables, of which there are twelve in the golden channel, to be discussed more below, and A represents some set of lagrangian parameters. The transfer function T ( X R | X G ) takes us from generator (G) level to reconstructed (R) level observables and represents the probability of reconstructing the observables X R given the generator level observable X G . It is treated as a function of X R which takes X G as input. As will be described more in section 5.1, once the integration in eq. (2.1) is performed we must then normalize over all twelve reconstructed level observables to obtain the detector level pdf. The integral in eq. (2.1) is the defining feature of our framework and has been obtained for both the h \u2192 4 signal as well as the dominant qq \u2192 4 background, which have been computed analytically in accompanying studies [19,35,42]. We emphasize that the integral has not been obtained via Monte Carlo methods. Instead we have explicitly performed the integration by utilizing various analytic and well-established numerical methods [41,43] (for studies that perform similar convolutions using Monte Carlo methods see [31,[44][45][46]). This ensures that (arbitrarily) high precision is maintained at each step, producing what is effectively an 'analytic function' in terms of detector level variables once the convolution has been performed. After performing this 12-dimensional integration and normalizing, we are left with a probability density function (pdf ) from which we construct an un-binned twelve-dimensional detector level likelihood which is a continuous function of the effective couplings (or Lagrangian parameters) and takes as its input, up to twelve reconstructed (detector-level) center of mass observables. In the current implementation we will average over the four production variables to reduce the systematic uncertainties, thus obtaining an eight-dimensional likelihood in terms of just decay observables. However, this step is in principle not necessary.",
            "paragraph_rank": 11,
            "section_rank": 6,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b18",
                    "start": 897,
                    "text": "[19,",
                    "end": 901
                },
                {
                    "type": "bibr",
                    "ref_id": "b35",
                    "start": 901,
                    "text": "35,",
                    "end": 904
                },
                {
                    "type": "bibr",
                    "ref_id": "b42",
                    "start": 904,
                    "text": "42]",
                    "end": 907
                },
                {
                    "type": "bibr",
                    "ref_id": "b41",
                    "start": 1109,
                    "text": "[41,",
                    "end": 1113
                },
                {
                    "type": "bibr",
                    "ref_id": "b43",
                    "start": 1113,
                    "text": "43]",
                    "end": 1116
                },
                {
                    "type": "bibr",
                    "ref_id": "b31",
                    "start": 1194,
                    "text": "[31,",
                    "end": 1198
                },
                {
                    "type": "bibr",
                    "ref_id": "b44",
                    "start": 1198,
                    "text": "[44]",
                    "end": 1202
                },
                {
                    "type": "bibr",
                    "ref_id": "b45",
                    "start": 1202,
                    "text": "[45]",
                    "end": 1206
                },
                {
                    "type": "bibr",
                    "ref_id": "b46",
                    "start": 1206,
                    "text": "[46]",
                    "end": 1210
                }
            ]
        },
        {
            "text": "JHEP01(2015)125",
            "section_rank": 7
        },
        {
            "section": "JHEP01(2015)125",
            "text": "We also emphasize that the convolution integral is largely independent of detector transfer function and generator level differential cross section and in particular how accurate the descriptions of the 'truth' level pdfs or the detector properties are. This means the framework can in principle be adapted to any detector which studies h \u2192 4 (or any X \u2192 4 ) and, in addition, as theoretical calculations of the generator level differential cross sections improve they can easily be incorporated into the convolution integral. Thus, there is ample for room optimization in our framework as time goes on. The generality of the convolution also allows for other beyond the Standard Model physics such as exotic Higgs decays [47] to be easily be incorporated into the h \u2192 4 framework.",
            "paragraph_rank": 12,
            "section_rank": 7,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b47",
                    "start": 722,
                    "text": "[47]",
                    "end": 726
                }
            ]
        },
        {
            "text": "Analytic parameterizations",
            "section_rank": 8
        },
        {
            "section": "Analytic parameterizations",
            "text": "As we discuss below, it is essential to first have analytic parameterizations of the 'truth' level differential cross sections in order to perform the convolution integral in eq. (2.1). These can be obtained in essentially two different ways. The first is to simply analytically compute the differential cross section starting from Feynman diagrams, which of course is not always possible. The second is to obtain an 'analytic' parameterization by fitting to a large Monte Carlo sample with some appropriately parametrized function. This becomes quite difficult when the function is multi-dimensional as is the case in the golden channel with twelve center of mass observables and requires large samples and an accurate interpolation procedure. In this framework we have implemented a hybrid of these two approaches with the primary component coming from analytic expressions of the leading order h \u2192 4 and qq \u2192 4 fully differential cross sections [19,35,42].",
            "paragraph_rank": 13,
            "section_rank": 8,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b18",
                    "start": 948,
                    "text": "[19,",
                    "end": 952
                },
                {
                    "type": "bibr",
                    "ref_id": "b35",
                    "start": 952,
                    "text": "35,",
                    "end": 955
                },
                {
                    "type": "bibr",
                    "ref_id": "b42",
                    "start": 955,
                    "text": "42]",
                    "end": 958
                }
            ]
        },
        {
            "section": "Analytic parameterizations",
            "text": "Since NLO effects in the golden channel are generally small [48][49][50], these leading order differential cross sections represent the dominant contributions to the 4 'truth' level likelihood. There are however, a number of sub-dominant effects which appear at higher order and should be accounted for. These include production and additional background effects. In these cases, the second method of parametric fits to simulated data is typically the optimal route. To do this we follow a similar procedure as found in [32] while further details on the implementation into our framework can be found in [41]. We emphasize however that the convolution integral is independent of these matters allowing for easy implementation of more precise 'truth' level likelihoods as they become available over time.",
            "paragraph_rank": 14,
            "section_rank": 8,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b49",
                    "start": 60,
                    "text": "[48]",
                    "end": 64
                },
                {
                    "type": "bibr",
                    "ref_id": "b50",
                    "start": 64,
                    "text": "[49]",
                    "end": 68
                },
                {
                    "type": "bibr",
                    "ref_id": "b51",
                    "start": 68,
                    "text": "[50]",
                    "end": 72
                },
                {
                    "type": "bibr",
                    "ref_id": "b32",
                    "start": 520,
                    "text": "[32]",
                    "end": 524
                },
                {
                    "type": "bibr",
                    "ref_id": "b41",
                    "start": 604,
                    "text": "[41]",
                    "end": 608
                }
            ]
        },
        {
            "section": "Analytic parameterizations",
            "text": "Of course when considering the detector level likelihood there are additional, but again sub-dominant, effects not present at 'truth' level which should be accounted for, such as detector momentum resolution and acceptance effects. These can be parametrized via transfer functions which can be optimized for a particular detector as done recently in [38,39] which incorporates a parameterization of the CMS detector into our framework. Since these typically would be supplied by the experimentalist we do not discuss their construction in detail here, but note that as knowledge of the detectors improves and parameterizations of the transfer functions become more accurate, they can easily be incorporated into the convolution integral in eq. (2.1), but again the integration is independent of these matters. More details on the construction and implementation, as well as the validation, of the transfer functions is found in [38,39,41].",
            "paragraph_rank": 15,
            "section_rank": 8,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b38",
                    "start": 350,
                    "text": "[38,",
                    "end": 354
                },
                {
                    "type": "bibr",
                    "ref_id": "b39",
                    "start": 354,
                    "text": "39]",
                    "end": 357
                },
                {
                    "type": "bibr",
                    "ref_id": "b38",
                    "start": 928,
                    "text": "[38,",
                    "end": 932
                },
                {
                    "type": "bibr",
                    "ref_id": "b39",
                    "start": 932,
                    "text": "39,",
                    "end": 935
                },
                {
                    "type": "bibr",
                    "ref_id": "b41",
                    "start": 935,
                    "text": "41]",
                    "end": 938
                }
            ]
        },
        {
            "text": "JHEP01(2015)125 2.3 Fast parameter extraction",
            "section_rank": 9
        },
        {
            "section": "JHEP01(2015)125 2.3 Fast parameter extraction",
            "text": "The convolution integral in eq. (2.1) allows us to (effectively) obtain an analytic function in both detector level observables and lagrangian parameters. This is because, via the explicit 12-dimensional integration over all center of mass variables, we are able to obtain a 1-to-1 mapping from the 'truth' level likelihood to the 'detector' level likelihood. This allows us, during parameter extraction, to effectively work directly with the lagrangian parameters, but at detector level which gives us the ability to easily perform multi-parameter extraction with the same speed and flexibility as was done at generator level [35,37]. Being able to fit to multiple parameters simultaneously is important since it allows for strong tests of models which often predict correlations between the various parameters. We point out that our framework allows us to do this while avoiding relying on hypothesis testing or on the construction of kinematic discriminants which is less optimal when extracting multiple parameters than maximizing the full likelihood [51] where all observables are used. Furthermore, the analytic nature of our framework allows for a great deal of flexibility in performing a variety of types of parameter extractions and re-parameterizations.",
            "paragraph_rank": 16,
            "section_rank": 9,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b35",
                    "start": 627,
                    "text": "[35,",
                    "end": 631
                },
                {
                    "type": "bibr",
                    "ref_id": "b37",
                    "start": 631,
                    "text": "37]",
                    "end": 634
                },
                {
                    "type": "bibr",
                    "ref_id": "b52",
                    "start": 1055,
                    "text": "[51]",
                    "end": 1059
                }
            ]
        },
        {
            "text": "Comments on assumptions and approximations",
            "section_rank": 10
        },
        {
            "section": "Comments on assumptions and approximations",
            "text": "In performing the convolution integral in eq. (2.1) we have relied on two key assumptions. The first is that angular resolution effects due to detector smearing can be neglected, which is an excellent approximation for the LHC detectors [52][53][54]. Second, we have assumed in the transfer function that each lepton is independent of the others which again is a very good approximation since leptons are clean and well-measured objects in the CMS and ATLAS detectors once standard lepton selection criteria are imposed [52][53][54]. With these simplifying assumptions the convolution integral can then be performed as will be described below and in much more detail in [40] and [41].",
            "paragraph_rank": 17,
            "section_rank": 10,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b53",
                    "start": 237,
                    "text": "[52]",
                    "end": 241
                },
                {
                    "type": "bibr",
                    "ref_id": "b54",
                    "start": 241,
                    "text": "[53]",
                    "end": 245
                },
                {
                    "type": "bibr",
                    "start": 245,
                    "text": "[54]",
                    "end": 249
                },
                {
                    "type": "bibr",
                    "ref_id": "b53",
                    "start": 520,
                    "text": "[52]",
                    "end": 524
                },
                {
                    "type": "bibr",
                    "ref_id": "b54",
                    "start": 524,
                    "text": "[53]",
                    "end": 528
                },
                {
                    "type": "bibr",
                    "start": 528,
                    "text": "[54]",
                    "end": 532
                },
                {
                    "type": "bibr",
                    "ref_id": "b40",
                    "start": 670,
                    "text": "[40]",
                    "end": 674
                },
                {
                    "type": "bibr",
                    "ref_id": "b41",
                    "start": 679,
                    "text": "[41]",
                    "end": 683
                }
            ]
        },
        {
            "section": "Comments on assumptions and approximations",
            "text": "Even after the convolution is performed however, we must still normalize the detector level differential cross section. Since this can not be done analytically one must resort to Monte Carlo techniques. Thus, strictly speaking the final pdf is not analytic. However, as we will discuss more in section 5.1, due to the manner in which the analytic expressions are organized, a high precision on the normalization can be obtained in a short amount of computing time. Furthermore, by fitting to ratios of couplings, we can circumvent the need for the absolute normalization which greatly simplifies the computational procedure and allows us to achieve a high precision [40,41] leading in the end to a detector level pdf which is effectively analytic in reconstructed observables and lagrangian parameters.",
            "paragraph_rank": 18,
            "section_rank": 10,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b40",
                    "start": 666,
                    "text": "[40,",
                    "end": 670
                },
                {
                    "type": "bibr",
                    "ref_id": "b41",
                    "start": 670,
                    "text": "41]",
                    "end": 673
                }
            ]
        },
        {
            "section": "Comments on assumptions and approximations",
            "text": "Of course there are components of both the 'truth' and detector level likelihoods which can not be included in the convolution integral of eq. (2.1). These correspond to any components for which a sufficiently accurate analytic parameterization can not be obtained. These may include potential higher order contributions to both signal and background differential cross sections as well as additional fake backgrounds such as Z + X. For these one must resort to more conventional Monte Carlo techniques and the construction of large (binned) 'look-up' tables. The effects of binning can me mitigated through a linear multidimensional interpolation technique which is described in more detail in [41]. Fortunately",
            "paragraph_rank": 19,
            "section_rank": 10,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b41",
                    "start": 695,
                    "text": "[41]",
                    "end": 699
                }
            ]
        },
        {
            "text": "JHEP01(2015)125",
            "section_rank": 11
        },
        {
            "section": "JHEP01(2015)125",
            "text": "these components are sub dominant in the golden channel and can be assigned systematics to study their effects [38,39,41].",
            "paragraph_rank": 20,
            "section_rank": 11,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b38",
                    "start": 111,
                    "text": "[38,",
                    "end": 115
                },
                {
                    "type": "bibr",
                    "ref_id": "b39",
                    "start": 115,
                    "text": "39,",
                    "end": 118
                },
                {
                    "type": "bibr",
                    "ref_id": "b41",
                    "start": 118,
                    "text": "41]",
                    "end": 121
                }
            ]
        },
        {
            "section": "JHEP01(2015)125",
            "text": "There are also various other systematics associated with both detector and theoretical uncertainties which should properly be accounted for. Since these components do not have a large effect on the final sensitivity (especially once sizable data sets are accumulated) and are not directly related to the convolution, we will discuss them only briefly below, but see [38,39,41] for more details on how they are implemented into the framework in a real experimental analysis.",
            "paragraph_rank": 21,
            "section_rank": 11,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b38",
                    "start": 366,
                    "text": "[38,",
                    "end": 370
                },
                {
                    "type": "bibr",
                    "ref_id": "b39",
                    "start": 370,
                    "text": "39,",
                    "end": 373
                },
                {
                    "type": "bibr",
                    "ref_id": "b41",
                    "start": 373,
                    "text": "41]",
                    "end": 376
                }
            ]
        },
        {
            "section": "JHEP01(2015)125",
            "text": "In constructing the detector level likelihood we have overcome many of the technical challenges which in the past have made it impossible to use the fully multi-dimensional likelihood during parameter fitting. Below we sketch in more detail how these various challenges have been overcome, but many of the details are technically beyond the scope of this paper so we refer the reader to [19,35,[37][38][39][40][41] for more details.",
            "paragraph_rank": 22,
            "section_rank": 11,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b18",
                    "start": 387,
                    "text": "[19,",
                    "end": 391
                },
                {
                    "type": "bibr",
                    "ref_id": "b35",
                    "start": 391,
                    "text": "35,",
                    "end": 394
                },
                {
                    "type": "bibr",
                    "ref_id": "b37",
                    "start": 394,
                    "text": "[37]",
                    "end": 398
                },
                {
                    "type": "bibr",
                    "ref_id": "b38",
                    "start": 398,
                    "text": "[38]",
                    "end": 402
                },
                {
                    "type": "bibr",
                    "ref_id": "b39",
                    "start": 402,
                    "text": "[39]",
                    "end": 406
                },
                {
                    "type": "bibr",
                    "ref_id": "b40",
                    "start": 406,
                    "text": "[40]",
                    "end": 410
                },
                {
                    "type": "bibr",
                    "ref_id": "b41",
                    "start": 410,
                    "text": "[41]",
                    "end": 414
                }
            ]
        },
        {
            "text": "The 'truth level' pdf",
            "section_rank": 12
        },
        {
            "section": "The 'truth level' pdf",
            "text": "Before obtaining the detector level likelihood one must of course first construct the 'truth' level (or generator level) likelihood. As we discuss, the generator level likelihood is composed of a 'decay' and 'production' differential spectrum. In our framework, the primary component is constructed out of analytic expressions for the h \u2192 4 signal and the dominant qq \u2192 4 background differential cross sections. Analytic expressions have been shown to be useful in likelihood methods where the full kinematics of an event can be exploited. This is especially true for the golden channel as has been demonstrated in numerous studies [13-15, 17, 18, 29, 32, 35, 37]. For a detailed description of the analytic calculations for the signal and background fully differential cross sections as well as their validation we refer the reader to accompanying studies [19,35].",
            "paragraph_rank": 23,
            "section_rank": 12,
            "ref_spans": [
                {
                    "type": "bibr",
                    "start": 632,
                    "text": "[13-15, 17, 18, 29, 32, 35, 37]",
                    "end": 663
                },
                {
                    "type": "bibr",
                    "ref_id": "b18",
                    "start": 857,
                    "text": "[19,",
                    "end": 861
                },
                {
                    "type": "bibr",
                    "ref_id": "b35",
                    "start": 861,
                    "text": "35]",
                    "end": 864
                }
            ]
        },
        {
            "section": "The 'truth level' pdf",
            "text": "Below we give an overview of how the 'truth' level likelihood is constructed and define the twelve center of mass variables in the four lepton final state. We briefly discuss our parameterization of the Higgs couplings to electroweak gauge bosons and how the analytic expressions are combined with the appropriate production spectra to form the full truth level differential cross section. We also discuss in this section how the production spectrum is obtained and comment on the additional backgrounds present in the golden channel.",
            "paragraph_rank": 24,
            "section_rank": 12
        },
        {
            "text": "Center of mass observables",
            "section_rank": 13
        },
        {
            "section": "Center of mass observables",
            "text": "Here we describe the various center of mass variables which will be used as our set of observables when constructing the likelihood. The kinematics of four lepton events are illustrated in figure 1. The invariant masses are defined as the following:",
            "paragraph_rank": 25,
            "section_rank": 13
        },
        {
            "section": "Center of mass observables",
            "text": "The invariant mass of the four lepton system or the Higgs mass in case of signal.",
            "paragraph_rank": 26,
            "section_rank": 13
        },
        {
            "section": "Center of mass observables",
            "text": "\u2022 M 1 -The invariant mass of the lepton pair system which reconstructs closest to the Z mass.",
            "paragraph_rank": 27,
            "section_rank": 13
        },
        {
            "text": "JHEP01(2015)125",
            "section_rank": 14
        },
        {
            "section": "JHEP01(2015)125",
            "text": "\u0398 Figure 1. Definition of angles in the four lepton center of mass frame X.",
            "paragraph_rank": 28,
            "section_rank": 14,
            "ref_spans": [
                {
                    "type": "figure",
                    "start": 2,
                    "text": "Figure 1",
                    "end": 10
                }
            ]
        },
        {
            "section": "JHEP01(2015)125",
            "text": "\u2022 M 2 -The invariant mass of the other lepton pair system and interpreted as M 2 < M 1 . This condition holds as long as \u221a\u015d 2m Z .",
            "paragraph_rank": 29,
            "section_rank": 14
        },
        {
            "section": "JHEP01(2015)125",
            "text": "These invariant masses are all independent subject to the constraint (M 1 + M 2 ) \u2264 \u221a\u015d and serve as the most strongly discriminating observables between different signal hypothesis as well as between signal and background. Note also that the 4e/4\u00b5 final state can be reconstructed in two different ways due to the identical final state interference. This is a quantum mechanical effect that occurs at the amplitude level and thus both reconstructions are valid. The definitions M 1 and M 2 remain unchanged however.",
            "paragraph_rank": 30,
            "section_rank": 14
        },
        {
            "section": "JHEP01(2015)125",
            "text": "The angular variables are defined as:",
            "paragraph_rank": 31,
            "section_rank": 14
        },
        {
            "section": "JHEP01(2015)125",
            "text": "\u2022 \u0398 -The production angle between the momentum vectors of the lepton pair which reconstructs to M 1 and the total 4 system momentum.",
            "paragraph_rank": 32,
            "section_rank": 14
        },
        {
            "section": "JHEP01(2015)125",
            "text": "\u2022 \u03b8 1,2 -Polar angle of the momentum vectors of e \u2212 , \u00b5 \u2212 in the lepton pair rest frame.",
            "paragraph_rank": 33,
            "section_rank": 14
        },
        {
            "section": "JHEP01(2015)125",
            "text": "\u2022 \u03a6 1 -The angle between the plane formed by the M 1 lepton pair and the 'production plane' formed out of the momenta of the incoming partons and the momenta of the two lepton pair systems.",
            "paragraph_rank": 34,
            "section_rank": 14
        },
        {
            "section": "JHEP01(2015)125",
            "text": "\u2022 \u03a6 -The angle between the decay planes of the final state lepton pairs in the rest frame of the 4 system.",
            "paragraph_rank": 35,
            "section_rank": 14
        },
        {
            "section": "JHEP01(2015)125",
            "text": "We group the angular variables as follows \u2126 = (\u0398, cos \u03b8 1 , cos \u03b8 2 , \u03a6 1 , \u03a6). These angular variables are useful in aiding to distinguish different signal hypothesis and in particular between those with different CP properties, as well as in discriminating signal from background. There are also additional production variables associated with the initial partonic state four momentum:",
            "paragraph_rank": 36,
            "section_rank": 14
        },
        {
            "section": "JHEP01(2015)125",
            "text": "\u2022 p T -The momentum in the transverse direction.",
            "paragraph_rank": 37,
            "section_rank": 14
        },
        {
            "section": "JHEP01(2015)125",
            "text": "\u2022 Y -Defined as the motion along the longitudinal direction.",
            "paragraph_rank": 38,
            "section_rank": 14
        },
        {
            "text": "JHEP01(2015)125",
            "section_rank": 15
        },
        {
            "section": "JHEP01(2015)125",
            "text": "\u2022 \u03c6 -Defines a global rotation of the event in the 4 rest frame and in general does not aid greatly in discriminating power.",
            "paragraph_rank": 39,
            "section_rank": 15
        },
        {
            "section": "JHEP01(2015)125",
            "text": "Including Y and p T as observables in the likelihood increases the discriminating power of the golden channel. However, including these production variables can introduce large uncertainties since their spectra includes parton distribution functions as well as NLO contributions which should be included. Thus, they are often integrated out of the final likelihood [38,39] and not used during parameter extraction. This reduces the potential discriminating power, but this is compensated by the smaller systematic uncertainties one obtains by not including them in the likelihood (or averaging over them). We will discuss more below how in our framework one can easily either include them in the final likelihood or average over them to mitigate the effects of the uncertainties associated with these variables. However as with \u03c6, it is crucial to include them when performing the convolution with the transfer function in order to obtain the proper detector level likelihood. These variables exhaust the twelve possible center of mass observables available in the golden channel.",
            "paragraph_rank": 40,
            "section_rank": 15,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b38",
                    "start": 365,
                    "text": "[38,",
                    "end": 369
                },
                {
                    "type": "bibr",
                    "ref_id": "b39",
                    "start": 369,
                    "text": "39]",
                    "end": 372
                }
            ]
        },
        {
            "text": "Parameterization of scalar-tensor couplings",
            "section_rank": 16
        },
        {
            "section": "Parameterization of scalar-tensor couplings",
            "text": "Assuming only Lorentz invariance, the general couplings of a spin-0 particle to two spin-1 vector bosons can be parametrized in terms of effective couplings by the following tensor structure,",
            "paragraph_rank": 41,
            "section_rank": 16
        },
        {
            "section": "Parameterization of scalar-tensor couplings",
            "text": "where in the golden channel i = ZZ, Z\u03b3, \u03b3\u03b3. The variables k 1 and k 2 represent the four momentum of the intermediate vector bosons with v the Higgs vacuum expectation value (vev) which we have chosen as our overall normalization. The A i n are dimensionless and in principle arbitrary complex form factors with possible momentum dependence (or more precisely a\u015d, k 2 1 , k 2 2 dependence) making eq. (3.1) completely general. Note that the tensor structure for A i 5 is only distinguishable from A i 1 for off-shell Higgs decays as discussed in [36]. For a purely Standard Model Higgs we have A ZZ 1 = 2 at tree level while all other effective couplings are generated at higher loop order and at most O( 10 \u22122 ).",
            "paragraph_rank": 42,
            "section_rank": 16,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b36",
                    "start": 546,
                    "text": "[36]",
                    "end": 550
                }
            ]
        },
        {
            "section": "Parameterization of scalar-tensor couplings",
            "text": "Of course it is often possible to expand the A i n in a power series of momenta keeping only the leading (constant) terms. By keeping the leading terms in this expansion there is a one-to-one mapping from this vertex onto the effective Lagrangian, 2",
            "paragraph_rank": 43,
            "section_rank": 16
        },
        {
            "section": "Parameterization of scalar-tensor couplings",
            "text": "where the A i no represent the leading, momentum independent coefficients and electromagnetic gauge invariance requires A Z\u03b3,\u03b3\u03b3 1o = A Z\u03b3,\u03b3\u03b3 4o = A Z\u03b3,\u03b3\u03b3 5o = 0. We have defined Z \u00b5 and A \u00b5 as the Z and photon fields respectively while V \u00b5\u03bd = \u2202 \u00b5 V \u03bd \u2212 \u2202 \u03bd V \u00b5 are the usual bosonic field strengths and the dual field strengths are defined as V \u00b5\u03bd = 1 2 \u00b5\u03bd\u03c1\u03c3 V \u03c1\u03c3 . The effective lagrangian in eq. (32) is composed of the leading terms in a derivative expansion (up to two derivatives) and is useful for parametrizing potentially large new physics effects generated by loops of heavy particles and a convenient framework for assessing the potential sensitivity to the leading operators [37] involving photons and Z bosons.",
            "paragraph_rank": 44,
            "section_rank": 16,
            "ref_spans": [
                {
                    "start": 398,
                    "text": "(3",
                    "end": 400
                },
                {
                    "type": "bibr",
                    "ref_id": "b37",
                    "start": 686,
                    "text": "[37]",
                    "end": 690
                }
            ]
        },
        {
            "section": "Parameterization of scalar-tensor couplings",
            "text": "Thus, although eq. (3.1) is a redundant parameterization of the tensor structure, it is a convenient, yet more general, parametrization for fitting to effective Lagrangian parameters that might be generated in various models at dimension five or less as in eq. (3.2). The parameterization in eq. (3.1) can of course be mapped, with appropriate translation of the parameters, onto Lagrangians with dimension greater than five or to an underlying dimension six lagrangian in a theory of electroweak symmetry breaking such as in the Standard Model. We will work explicitly with the vertex in eq. (3.1) which has been used to calculate the fully differential cross section for h \u2192 4 and when performing parameter extraction, but other parameterizations can be easily accommodated.",
            "paragraph_rank": 45,
            "section_rank": 16
        },
        {
            "section": "Parameterization of scalar-tensor couplings",
            "text": "This flexibility of parameterization also allows for other new physics, such as exotic Higgs decays involving exotic fermions or vector bosons [47], to be easily included in the framework. Furthermore, by using this parameterization, explicit computations of either Standard Model or new physics loop effects which would generate these momentum dependent form factors can easily be included into the framework. This allows for the ability to in principle extract the parameters from whichever underlying theory is responsible for generating them. We leave a more detailed investigation of these loop effects to ongoing work.",
            "paragraph_rank": 46,
            "section_rank": 16,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b47",
                    "start": 143,
                    "text": "[47]",
                    "end": 147
                }
            ]
        },
        {
            "text": "Signal and background fully differential cross sections",
            "section_rank": 17
        },
        {
            "section": "Signal and background fully differential cross sections",
            "text": "In the case of signal we have computed analytically the fully differential cross section in the observables described in section 3.1 for the process h \u2192 ZZ + Z\u03b3 + \u03b3\u03b3 \u2192 4 using the parameterization in eq. (3.1). We have included all possible interference effects between tensor structures as well as identical final states in the case of 4e/4\u00b5. For the irreducible background we have computed analytically the process qq \u2192 ZZ + Z\u03b3 + \u03b3\u03b3 \u2192 4 which includes the s-channel (resonant) 4 process as well as the t-channel (diboson production) 4 process and again have included all possible interference effects. All vector bosons are allowed to be on or off-shell and we do not distinguish between them in what follows. The details of these calculations can be found in [19,35,37,42] along with the validation procedures and studies of the distributions as well as the various interference effects. We have combined these analytic expressions with functions parametrizing the production spectra and implemented them into our analysis framework.",
            "paragraph_rank": 47,
            "section_rank": 17,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b18",
                    "start": 762,
                    "text": "[19,",
                    "end": 766
                },
                {
                    "type": "bibr",
                    "ref_id": "b35",
                    "start": 766,
                    "text": "35,",
                    "end": 769
                },
                {
                    "type": "bibr",
                    "ref_id": "b37",
                    "start": 769,
                    "text": "37,",
                    "end": 772
                },
                {
                    "type": "bibr",
                    "ref_id": "b42",
                    "start": 772,
                    "text": "42]",
                    "end": 775
                }
            ]
        },
        {
            "section": "Signal and background fully differential cross sections",
            "text": "We note that it is important to include all possible Higgs couplings including the Z\u03b3 and \u03b3\u03b3 contributions in the signal differential cross section since the Higgs appears to be mostly Standard Model-like [58] and we are primarily searching for small anomalous deviations from the Standard Model prediction. Thus when attempting to extract specific",
            "paragraph_rank": 48,
            "section_rank": 17,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b58",
                    "start": 205,
                    "text": "[58]",
                    "end": 209
                }
            ]
        },
        {
            "text": "JHEP01(2015)125",
            "section_rank": 18
        },
        {
            "section": "JHEP01(2015)125",
            "text": "couplings we must be sure that one small effect is not being mistaken for another. This is particularly relevant since many of the couplings may be correlated with one another.",
            "paragraph_rank": 49,
            "section_rank": 18
        },
        {
            "section": "JHEP01(2015)125",
            "text": "Furthermore, it has been shown recently [37] that for 'true' points near the Standard Model, the greatest sensitivity to the anomalous couplings (non A ZZ 1o ) is for the Z\u03b3 and especially \u03b3\u03b3 operators (see eq. (3.2)). Including all possible couplings and doing a simultaneous fit ensures that we minimize the possibility of misinterpretation or of introducing a bias when attempting to extract these couplings. Searching for these small effects is also why it is important to include the interference effects between the identical final state leptons as well as the relevant detector effects and background.",
            "paragraph_rank": 50,
            "section_rank": 18,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b37",
                    "start": 40,
                    "text": "[37]",
                    "end": 44
                }
            ]
        },
        {
            "section": "JHEP01(2015)125",
            "text": "We also comment that in principal there are NLO contributions to the h \u2192 4 decay processes, but these are expected to be small at \u223c 125 GeV [48,49] and not relevant until higher precision is obtained once larger data sets are gathered. Eventually however, these effects should be included and their implementation into our framework is currently ongoing.",
            "paragraph_rank": 51,
            "section_rank": 18,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b49",
                    "start": 140,
                    "text": "[48,",
                    "end": 144
                },
                {
                    "type": "bibr",
                    "ref_id": "b50",
                    "start": 144,
                    "text": "49]",
                    "end": 147
                }
            ]
        },
        {
            "text": "Combining production and decay",
            "section_rank": 19
        },
        {
            "section": "Combining production and decay",
            "text": "To be able to perform a fit for the effective Higgs couplings, we must first construct the fully differential cross section for the observables as a function of the undetermined parameters ( A). This differential cross section consists of two components which we assume to be factorized: the parton level ('decay') differential cross section as discussed in section 3.3, and the 'production' spectrum. The full production plus decay fully differential cross section can be expressed as the following,",
            "paragraph_rank": 52,
            "section_rank": 19
        },
        {
            "section": "Combining production and decay",
            "text": "where, since the Higgs is a spin-0 particle, we can explicitly assume that the decay process can be factorized from the production mechanism. For the background this explicit factorization does not occur, but still turns out to be an adequate approximation [41] especially if the p T and Y variables are averaged over once the convolution is performed. The parton level fully differential cross section (\u03c3 4 ) is treated as being at fixed\u015d where one obtains the input\u015d value from the production spectrum (W prod ). The production spectrum for the signal and background depend on the parton distribution functions and can not be computed analytically. For the signal which we assume decays on-shell, the\u015d spectrum is taken to be a delta function centered at m 2 h , which for a Standard Model Higgs at 125 GeV is an excellent approximation. Note however that this assumption can be relaxed in our framework to consider more general\u015d spectra as would be found for example in the case of a new heavy scalar with a large width.",
            "paragraph_rank": 53,
            "section_rank": 19,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b41",
                    "start": 257,
                    "text": "[41]",
                    "end": 261
                }
            ]
        },
        {
            "text": "Comments production spectra",
            "section_rank": 20
        },
        {
            "section": "Comments production spectra",
            "text": "Here we discuss how the W prod (\u015d, p T , Y, \u03c6) production spectrum in eq. (3.3) is obtained. This function involves higher order effects as well as parton distribution functions and thus can not be computed analytically. To include them in the total differential cross sections there are various options. One can in principal generate enough Monte Carlo events",
            "paragraph_rank": 54,
            "section_rank": 20
        },
        {
            "text": "JHEP01(2015)125",
            "section_rank": 21
        },
        {
            "section": "JHEP01(2015)125",
            "text": "to accurately fill the full spectrum in the (\u015d, Y, p T , \u03c6) variables. As this is computationally intensive we take an approximate approach in which we interpolate analytic functions for the signal and background from one or two dimensional projections generated from the Madgraph [59] and POWHEG [60] Monte Carlo generators following a similar procedure as found in [32]. Having an analytic parameterization for these functions also allows for faster integration when implementing them into the convolution procedure described above. This procedure of interpolating the one or two dimensional projections neglects correlations between the production variables. However, since in the signal case there is an explicit factorization between production and decay, the effects of this approximation on parameter extraction in our analysis are small.",
            "paragraph_rank": 55,
            "section_rank": 21,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b59",
                    "start": 281,
                    "text": "[59]",
                    "end": 285
                },
                {
                    "type": "bibr",
                    "ref_id": "b60",
                    "start": 297,
                    "text": "[60]",
                    "end": 301
                },
                {
                    "type": "bibr",
                    "ref_id": "b32",
                    "start": 367,
                    "text": "[32]",
                    "end": 371
                }
            ]
        },
        {
            "section": "JHEP01(2015)125",
            "text": "In addition, to mitigate these effects further, one can always average over Y and p T as well as fit to ratios of couplings while taking the Higgs mass and overall normalization as input from the total rate (so called 'geolocating' [36]) as was done in a recent implementation of our framework into a CMS experimental analysis [38,39]. Note however, the overall normalization and Higgs mass can in principle be extracted in our framework, but as this requires extra careful treatment of the production spectra and additional backgrounds we defer a discussion of this to future work.",
            "paragraph_rank": 56,
            "section_rank": 21,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b36",
                    "start": 232,
                    "text": "[36]",
                    "end": 236
                },
                {
                    "type": "bibr",
                    "ref_id": "b38",
                    "start": 327,
                    "text": "[38,",
                    "end": 331
                },
                {
                    "type": "bibr",
                    "ref_id": "b39",
                    "start": 331,
                    "text": "39]",
                    "end": 334
                }
            ]
        },
        {
            "text": "Comments on additional backgrounds",
            "section_rank": 22
        },
        {
            "section": "Comments on additional backgrounds",
            "text": "For the background there are also the higher order contributions such as the gg \u2192 4 and Z + X processes. These make up the parts of the likelihood which can not currently be included in the convolution integral since a sufficiently accurate analytic parameterization has yet to be obtained. Thus for these components we must resort to constructing large 'look-up' tables via Monte Carlo generation. Again, the effects of the necessary binning can be mitigated through a linear multi-dimensional interpolation technique [41]. Additionally, there will be systematic uncertainties associated with these components. Fortunately, the gg \u2192 4 component only makes up \u223c 3 \u2212 5% relative to qq \u2192 4 around 125 GeV [50]. The Z + X background on the other hand does make up a sizable contribution of the total background which is comparable to, but smaller than, the largest qq component (see table 2 in [38] or table 3 in [39]). This component however can in principle be reduced further in the future by requiring more stringent lepton acceptance criteria once more data is collected. For now the use of the linearly interpolated 'look-up' templates and associated systematics is found to be sufficient. Once the templates are built, including these components in the final likelihood is straightforward as we briefly sketch in section 5.2 and discussed in more detail in [38,39,41].",
            "paragraph_rank": 57,
            "section_rank": 22,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b41",
                    "start": 519,
                    "text": "[41]",
                    "end": 523
                },
                {
                    "type": "bibr",
                    "ref_id": "b51",
                    "start": 703,
                    "text": "[50]",
                    "end": 707
                },
                {
                    "type": "bibr",
                    "ref_id": "b38",
                    "start": 891,
                    "text": "[38]",
                    "end": 895
                },
                {
                    "type": "bibr",
                    "ref_id": "b39",
                    "start": 910,
                    "text": "[39]",
                    "end": 914
                },
                {
                    "type": "bibr",
                    "ref_id": "b38",
                    "start": 1361,
                    "text": "[38,",
                    "end": 1365
                },
                {
                    "type": "bibr",
                    "ref_id": "b39",
                    "start": 1365,
                    "text": "39,",
                    "end": 1368
                },
                {
                    "type": "bibr",
                    "ref_id": "b41",
                    "start": 1368,
                    "text": "41",
                    "end": 1370
                }
            ]
        },
        {
            "text": "The 'detector level' pdf",
            "section_rank": 23
        },
        {
            "section": "The 'detector level' pdf",
            "text": "The convolution integral in eq. (2.1) is conceptually straightforward, but in practice is challenging to perform, both for computational and algorithmic reasons. The key assumption which makes it possible is that the direction of lepton momenta are measured with infinite precision which at CMS and ATLAS is a very good approximation. This allows",
            "paragraph_rank": 58,
            "section_rank": 23
        },
        {
            "text": "JHEP01(2015)125",
            "section_rank": 24
        },
        {
            "section": "JHEP01(2015)125",
            "text": "us, through a change of variables, to reduce the 12-dimensional integral into a more manageable 4-dimensional integral over the four energies of the leptons which are altered by detector resolution effects. Typically this 4-dimensional integral is done using Monte Carlo techniques [31], thus losing the advantage of having analytic control over the likelihood or assuming that the resolution effects can also be neglecting making the integral trivial. As discussed in section 2 we instead perform this integration explicitly using a combination of numerical and analytic methods which allow us to maintain arbitrarily high precision at each step involved. There are a number of technical details involved in this procedure which are beyond the scope of this 'overview' of the framework, but the details can be found in [40,41]. We instead briefly sketch an overview of the convolution integral and show its validation.",
            "paragraph_rank": 59,
            "section_rank": 24,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b31",
                    "start": 282,
                    "text": "[31]",
                    "end": 286
                },
                {
                    "type": "bibr",
                    "ref_id": "b40",
                    "start": 820,
                    "text": "[40,",
                    "end": 824
                },
                {
                    "type": "bibr",
                    "ref_id": "b41",
                    "start": 824,
                    "text": "41]",
                    "end": 827
                }
            ]
        },
        {
            "text": "Transforming from CM basis to lepton smearing basis",
            "section_rank": 25
        },
        {
            "section": "Transforming from CM basis to lepton smearing basis",
            "text": "Beginning from eq. (2.1) we first discuss the construction of the background detector level pdf. The construction of the signal will be discussed separately as there is a subtle, but important, difference in performing the convolution. Since there are no undetermined parameters in the background the generator and detector-level (un-normalized) differential cross sections are given simply by P B ( X G ) and P B ( X R ) respectively and the convolution integral can be written schematically as,",
            "paragraph_rank": 60,
            "section_rank": 25
        },
        {
            "section": "Transforming from CM basis to lepton smearing basis",
            "text": "The set of variables",
            "paragraph_rank": 61,
            "section_rank": 25
        },
        {
            "section": "Transforming from CM basis to lepton smearing basis",
            "text": ", \u2126) exhausts the twelve degrees of freedom (note that p T has 2 components and \u2126 contains 5 angles) available to the four (massless) final state leptons. The differential volume element is given by",
            "paragraph_rank": 62,
            "section_rank": 25
        },
        {
            "section": "Transforming from CM basis to lepton smearing basis",
            "text": "To perform this convolution with the transfer function we must first transform to the basis in which the detector smearing of the lepton momenta is parameterized. This requires transforming from the basis of the twelve center of mass variables defined in section 3.1 to the three momentum basis for the four final state leptons. In this basis the lepton three momenta p i can be decomposed in terms of the component of the lepton momentum parallel to the direction (p i|| ) of motion and the two components perpendicular to the direction of motion ( p i\u22a5 ) (which are zero at generator level). We then make the assumption that detector smearing will only affect parallel components p i|| while the perpendicular components p i\u22a5 are left invariant. Note that this assumption is equivalent to assuming angular resolution effects due to detector smearing can be neglected, which is an excellent approximation for the LHC detectors [52][53][54]. In the (p i|| , p i\u22a5 ) basis only the transfer function associated with p i|| is non-trivial while the one associated with the perpendicular components can be represented simply as a delta function for each perpendicular direction, thus allowing for trivial integration over the eight p i\u22a5 variables.",
            "paragraph_rank": 63,
            "section_rank": 25,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b53",
                    "start": 928,
                    "text": "[52]",
                    "end": 932
                },
                {
                    "type": "bibr",
                    "ref_id": "b54",
                    "start": 932,
                    "text": "[53]",
                    "end": 936
                },
                {
                    "type": "bibr",
                    "start": 936,
                    "text": "[54]",
                    "end": 940
                }
            ]
        },
        {
            "section": "Transforming from CM basis to lepton smearing basis",
            "text": "With these assumptions the integral in eq. (4.1) can then be represented as follows,",
            "paragraph_rank": 64,
            "section_rank": 25
        },
        {
            "section": "Transforming from CM basis to lepton smearing basis",
            "text": "2)",
            "paragraph_rank": 65,
            "section_rank": 25
        },
        {
            "text": "JHEP01(2015)125",
            "section_rank": 26
        },
        {
            "section": "JHEP01(2015)125",
            "text": "where we have defined the lepton momenta 'smearing factors'",
            "paragraph_rank": 66,
            "section_rank": 26
        },
        {
            "section": "JHEP01(2015)125",
            "text": "We have also defined |J B | which is the 12 \u00d7 12 Jacobian which parametrizes the (nonlinear) transformation that takes us from the center of mass basis to the lepton smearing basis. The construction of this Jacobian is highly non trivial and requires a combination of analytic and numerical techniques which are beyond the scope of this overview, but the relevant details can be found in [40,41].",
            "paragraph_rank": 67,
            "section_rank": 26,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b40",
                    "start": 388,
                    "text": "[40,",
                    "end": 392
                },
                {
                    "type": "bibr",
                    "ref_id": "b41",
                    "start": 392,
                    "text": "41]",
                    "end": 395
                }
            ]
        },
        {
            "section": "JHEP01(2015)125",
            "text": "We thus see in eq. (42) that what started out as a twelve dimensional integral has been reduced to a much more manageable integration over four variables. The details and validation of this four dimensional integration, which is done using a recursive numerical integration technique [43] can also be found in [40,41].",
            "paragraph_rank": 68,
            "section_rank": 26,
            "ref_spans": [
                {
                    "ref_id": "formula_8",
                    "start": 19,
                    "text": "(4",
                    "end": 21
                },
                {
                    "type": "bibr",
                    "ref_id": "b43",
                    "start": 284,
                    "text": "[43]",
                    "end": 288
                },
                {
                    "type": "bibr",
                    "ref_id": "b40",
                    "start": 310,
                    "text": "[40,",
                    "end": 314
                },
                {
                    "type": "bibr",
                    "ref_id": "b41",
                    "start": 314,
                    "text": "41]",
                    "end": 317
                }
            ]
        },
        {
            "section": "JHEP01(2015)125",
            "text": "To construct the detector level signal differential cross section (again un-normalized), which is now a function of the effective couplings A, we follow the same procedure as for the background starting from,",
            "paragraph_rank": 69,
            "section_rank": 26
        },
        {
            "section": "JHEP01(2015)125",
            "text": "We again use the assumptions which allow us to perform the trivial integration over the eight p i\u22a5 variables, but instead transform to the following integration basis",
            "paragraph_rank": 70,
            "section_rank": 26
        },
        {
            "section": "JHEP01(2015)125",
            "text": "We now also use the fact that, as mentioned below eq. (3.3), the\u015d spectrum for the signal is \u221d \u03b4(\u015d G \u2212 m 2 h ) (where m h is the generated Higgs mass), enabling us to perform the integration over d\u015d G as well. Thus, we have for the final signal detector level differential cross section,",
            "paragraph_rank": 71,
            "section_rank": 26
        },
        {
            "section": "JHEP01(2015)125",
            "text": "where again |J S | represent the 12 \u00d7 12 Jacobian (which is different from |J B |) taking us from the CM basis to the lepton smearing basis. By using a delta function to model the width of the resonance, there is one less dimension to integrate over as compared to the background case. While this makes it easier computationally in one respect, an additional complication arises since we have to integrate along a trajectory in whic\u0125 s G is kept constant. This places an additional constraint when performing the",
            "paragraph_rank": 72,
            "section_rank": 26
        },
        {
            "section": "JHEP01(2015)125",
            "text": "integration which further complicates matters and must be properly taken into account. Explicit details of this integration and its validation along with the derivation of the signal Jacobian |J S | in eq. (4.6) are given in [40,41].",
            "paragraph_rank": 73,
            "section_rank": 26,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b40",
                    "start": 225,
                    "text": "[40,",
                    "end": 229
                },
                {
                    "type": "bibr",
                    "ref_id": "b41",
                    "start": 229,
                    "text": "41]",
                    "end": 232
                }
            ]
        },
        {
            "text": "Comments on transfer function",
            "section_rank": 27
        },
        {
            "section": "Comments on transfer function",
            "text": "Detector response effects including effects from selection inefficiency may be parameterized into transfer functions in the following way,  The response function S, parameterizes the probability for a lepton with actual momentum p G i to be reconstructed with momentum p R i , while \u03b4 is the Dirac delta function in the perpendicular components, and is the selection efficiency. With typical lepton selection criteria employed by the LHC experiments [53,54], it is a good approximation that each lepton is independent. Thus, the full transfer function for the event may be written as:",
            "paragraph_rank": 74,
            "section_rank": 27,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b54",
                    "start": 450,
                    "text": "[53,",
                    "end": 454
                },
                {
                    "type": "bibr",
                    "start": 454,
                    "text": "54]",
                    "end": 457
                }
            ]
        },
        {
            "section": "Comments on transfer function",
            "text": "We treat T ( c | P G ) as a function of c which takes the generator level momenta P G as input. The only effect of imperfect momentum measurement on the production spectra is to provide a small smearing of the p T spectrum for the four lepton system. We can mitigate the effects of the smearing by averaging over the production spectra when performing parameter extractions. Further details on the construction and implementation of the transfer function can be found in [38,39,41].",
            "paragraph_rank": 75,
            "section_rank": 27,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b38",
                    "start": 471,
                    "text": "[38,",
                    "end": 475
                },
                {
                    "type": "bibr",
                    "ref_id": "b39",
                    "start": 475,
                    "text": "39,",
                    "end": 478
                },
                {
                    "type": "bibr",
                    "ref_id": "b41",
                    "start": 478,
                    "text": "41]",
                    "end": 481
                }
            ]
        },
        {
            "text": "Validation of convolution integral",
            "section_rank": 28
        },
        {
            "section": "Validation of convolution integral",
            "text": "As validation of the convolution integral we first show in figures 2-5 projections for signal and background. We compare in these plots the distributions for a Madgraph sample which has had detector smearing and acceptance effects applied to it versus projections generated from our detector level differential cross sections obtained after the convolution described above.",
            "paragraph_rank": 76,
            "section_rank": 28
        },
        {
            "section": "Validation of convolution integral",
            "text": "We have obtained the signal and background production spectrum for the (\u015d, p T , Y, \u03c6) variables from POWHEG and boosted the Madgraph events and those from our projections accordingly. We have used the interpolation procedure described in section 3.   production spectra for the signal and background differential cross sections and combined them with the analytic expressions for the h \u2192 4 and qq \u2192 4 processes. For the signal we show the tree level Standard Model point where A ZZ 1 = 2 and all other couplings are set to zero. For both signal and background we show only the 2e2\u00b5 final state, but results for 4e (or 4\u00b5) are found in [38,39,41].",
            "paragraph_rank": 77,
            "section_rank": 28,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b38",
                    "start": 636,
                    "text": "[38,",
                    "end": 640
                },
                {
                    "type": "bibr",
                    "ref_id": "b39",
                    "start": 640,
                    "text": "39,",
                    "end": 643
                },
                {
                    "type": "bibr",
                    "ref_id": "b41",
                    "start": 643,
                    "text": "41]",
                    "end": 646
                }
            ]
        },
        {
            "section": "Validation of convolution integral",
            "text": "A further validation beyond these projections however is to look at the likelihoods (the differential cross section evaluated for a set of observables) for both the signal and background which contain the full correlations between the different variables. We show these in figure 6 for a CMS-like phase space and a very large number of events. To obtain these likelihoods we have evaluated our detector-level differential cross section with the Madgraph sample which has had detector smearing and acceptance effects applied and plotted it on top of the result of evaluating our detector-level differential cross section with events generated from the expression itself. We find the agreement between the two results to be very good. Further details are found in the accompanying documents [40,41].",
            "paragraph_rank": 78,
            "section_rank": 28,
            "ref_spans": [
                {
                    "type": "figure",
                    "start": 273,
                    "text": "figure 6",
                    "end": 281
                },
                {
                    "type": "bibr",
                    "ref_id": "b40",
                    "start": 789,
                    "text": "[40,",
                    "end": 793
                },
                {
                    "type": "bibr",
                    "ref_id": "b41",
                    "start": 793,
                    "text": "41]",
                    "end": 796
                }
            ]
        },
        {
            "section": "Validation of convolution integral",
            "text": "These plots should not be taken as validation of the complete detector-level differential cross sections which must be validated with full simulation and data. They are meant only to show the validation of the convolution procedure as well as the construction of the generator-level differential cross sections including the analytic computations. Complete validations of the full detector level likelihoods including the various production and background effects can be found in [38,39,41].  Figure 6. Validation of the convolution integrals described in eq. (4.1) and eq. (4.4). In blue we show the 'boosted' Madgraph sample with acceptance cuts and detector smearing applied while in red we show projections from our differential cross sections after the convolution integration for the tree level SM signal and background likelihood.",
            "paragraph_rank": 79,
            "section_rank": 28,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b38",
                    "start": 480,
                    "text": "[38,",
                    "end": 484
                },
                {
                    "type": "bibr",
                    "ref_id": "b39",
                    "start": 484,
                    "text": "39,",
                    "end": 487
                },
                {
                    "type": "bibr",
                    "ref_id": "b41",
                    "start": 487,
                    "text": "41]",
                    "end": 490
                },
                {
                    "type": "figure",
                    "start": 493,
                    "text": "Figure 6",
                    "end": 501
                }
            ]
        },
        {
            "text": "JHEP01(2015)125",
            "section_rank": 29
        },
        {
            "text": "Construction of likelihoods and parameter extraction",
            "section_rank": 30
        },
        {
            "section": "Construction of likelihoods and parameter extraction",
            "text": "With the detector level differential cross sections obtained in eq. (4.2) and eq. (4.6) in hand we can then go on to construct the full likelihood for a particular dataset. Before doing so, we must properly normalize the background and signal differential cross sections by performing the full integration over all twelve reconstructed X variables where from now on we drop the superscript R since we only deal with detector level observables in what follows. In this section we present a schematic overview of the normalization procedure. We also at this stage briefly discuss averaging over the production variables (Y, p T , \u03c6) and the implementation of systematic uncertainties through the use of nuisance parameters in the likelihood functions. Further details can be found in [40,41].",
            "paragraph_rank": 80,
            "section_rank": 30,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b40",
                    "start": 782,
                    "text": "[40,",
                    "end": 786
                },
                {
                    "type": "bibr",
                    "ref_id": "b41",
                    "start": 786,
                    "text": "41]",
                    "end": 789
                }
            ]
        },
        {
            "text": "JHEP01(2015)125",
            "section_rank": 31
        },
        {
            "text": "Normalization of background and signal",
            "section_rank": 32
        },
        {
            "section": "Normalization of background and signal",
            "text": "One can reduce the effects of production uncertainties by averaging over the detector level production variables (Y, p T , \u03c6). This is straightforwardly done for the background differential cross sections by the following 4-dimensional integration,",
            "paragraph_rank": 81,
            "section_rank": 32
        },
        {
            "section": "Normalization of background and signal",
            "text": "An overall volume factor is not shown because for the purpose of likelihood maximization this constant factor is not relevant. What matters is that the relative normalization between all components in the likelihood is done consistently. With this differential cross sections in terms of the eight center of mass decay observables we can obtain the overall normalization via a Monte Carlo integration procedure described in [40,41],",
            "paragraph_rank": 82,
            "section_rank": 32,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b40",
                    "start": 424,
                    "text": "[40,",
                    "end": 428
                },
                {
                    "type": "bibr",
                    "ref_id": "b41",
                    "start": 428,
                    "text": "41]",
                    "end": 431
                }
            ]
        },
        {
            "section": "Normalization of background and signal",
            "text": "which gives our final normalized background pdf as,",
            "paragraph_rank": 83,
            "section_rank": 32
        },
        {
            "section": "Normalization of background and signal",
            "text": "We have calculated the qq \u2192 4 expression as a sum of the separate individual contributions [19,35] making it possible to easily perform the integration on each smaller piece to obtain each normalization and then simply sum over them to obtain the overall normalization. Similarly for the signal we have for the averaging over (Y, p T , \u03c6) variables,",
            "paragraph_rank": 84,
            "section_rank": 32,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b18",
                    "start": 91,
                    "text": "[19,",
                    "end": 95
                },
                {
                    "type": "bibr",
                    "ref_id": "b35",
                    "start": 95,
                    "text": "35]",
                    "end": 98
                }
            ]
        },
        {
            "section": "Normalization of background and signal",
            "text": "To obtain the overall normalization in the signal case we first note that it is a function of the underlying parameters A defined in eq. (3.1)). However, from the calculation of the parton level differential cross section presented in [19,35] or from considering eq. (3.1) it is clear (assuming constant effective couplings) that P S (\u015d, M 1 , M 2 , \u2126| A) is a sum over terms each of which is proportional to A i n A j * m . Thus we can write,",
            "paragraph_rank": 85,
            "section_rank": 32,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b18",
                    "start": 235,
                    "text": "[19,",
                    "end": 239
                },
                {
                    "type": "bibr",
                    "ref_id": "b35",
                    "start": 239,
                    "text": "35]",
                    "end": 242
                }
            ]
        },
        {
            "section": "Normalization of background and signal",
            "text": "where",
            "paragraph_rank": 86,
            "section_rank": 32
        },
        {
            "section": "Normalization of background and signal",
            "text": ", \u2126) ij nm represents the individual differential cross sections with the couplings factored out. The separate normalizations for each term can now easily be obtained via,",
            "paragraph_rank": 87,
            "section_rank": 32
        },
        {
            "section": "Normalization of background and signal",
            "text": "from which we can now obtain the total overall normalization for the signal pdf as,",
            "paragraph_rank": 88,
            "section_rank": 32
        },
        {
            "text": "JHEP01(2015)125",
            "section_rank": 33
        },
        {
            "section": "JHEP01(2015)125",
            "text": "This likelihood can also be combined with an appropriate poisson weighting factor to account for the probability of observing a given number of events [38,39,41]. In the case of multiple final states (for example 4e, 4\u00b5 and 2e2\u00b5), we build the likelihood function and implement the appropriate systematic uncertainties for each one separately. We now briefly discuss the implementation of the systematic uncertainties.",
            "paragraph_rank": 89,
            "section_rank": 33,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b38",
                    "start": 151,
                    "text": "[38,",
                    "end": 155
                },
                {
                    "type": "bibr",
                    "ref_id": "b39",
                    "start": 155,
                    "text": "39,",
                    "end": 158
                },
                {
                    "type": "bibr",
                    "ref_id": "b41",
                    "start": 158,
                    "text": "41]",
                    "end": 161
                }
            ]
        },
        {
            "text": "Including systematic uncertainties",
            "section_rank": 34
        },
        {
            "section": "Including systematic uncertainties",
            "text": "Systematic uncertainties must be accounted for given our imperfect knowledge of various aspects of the analysis procedure. The lepton momentum resolution, the size of the backgrounds, and the exact production spectra are some important examples. For each of these systematic uncertainties we can associate an undetermined parameter which parametrizes our ignorance of the corresponding effect. Since we are not directly interested in these parameters, but only use them to estimate our systematic uncertainties, they are deemed nuisance parameters and are subsequently profiled over [38,39]. This is done by generating alternative pdfs using different values for the nuisance parameter of interest. To give one important example, we generate pdfs with narrower or wider lepton response functions to parameterize our knowledge of the lepton momentum resolution. If we define the nominal pdf to be P 0 (O) and the alternative as P 1 (O), one can parameterize the dependence of the likelihood on a nuisance parameter n by interpolating between the nominal and the alternative pdfs as follows:",
            "paragraph_rank": 90,
            "section_rank": 34,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b38",
                    "start": 583,
                    "text": "[38,",
                    "end": 587
                },
                {
                    "type": "bibr",
                    "ref_id": "b39",
                    "start": 587,
                    "text": "39]",
                    "end": 590
                }
            ]
        },
        {
            "section": "Including systematic uncertainties",
            "text": "It is instructive to observe that, for all values of n, the normalization of the total pdf stays the same. Given the asymmetric nature of many systematic uncertainties, it is more appropriate to generate many \"check-points\" along the axis of n and to do piece-wise interpolation without the need of worrying about the normalization. Non-central values of n are a priori disfavored, therefore one can impose a prior on top of the interpolated likelihood: P(O|n) = P(O|n)G(n), (5.13) where G(n) is typically a Gaussian centered at the central value of n. In the case of multiple systematic uncertainties, one can replace n by a vector of nuisance parameters n, and the prior G(n) by G( n). In general G( n) is a multivariate Gaussian-like function with primary axes which are some combination of different nuisance parameter directions. However one can carefully define the nuisance parameters such that correlations between them are negligible. In this limit G( n) can be written as the product of many Gaussian-like functions. This procedure for including systematic uncertainties has been implemented in a recent CMS analysis utilizing our framework [38,39] and further details can be found in [41].",
            "paragraph_rank": 91,
            "section_rank": 34,
            "ref_spans": [
                {
                    "type": "bibr",
                    "start": 475,
                    "text": "(5.13)",
                    "end": 481
                },
                {
                    "type": "bibr",
                    "ref_id": "b38",
                    "start": 1151,
                    "text": "[38,",
                    "end": 1155
                },
                {
                    "type": "bibr",
                    "ref_id": "b39",
                    "start": 1155,
                    "text": "39]",
                    "end": 1158
                },
                {
                    "type": "bibr",
                    "ref_id": "b41",
                    "start": 1195,
                    "text": "[41]",
                    "end": 1199
                }
            ]
        },
        {
            "text": "Comments on parameter extraction",
            "section_rank": 35
        },
        {
            "section": "Comments on parameter extraction",
            "text": "As discussed in [13,17,32] the advantage of analytic approaches is that the likelihood can be maximized for a large set of parameters in the most optimal way without losing information. Our framework allows for the 'analytic' nature of these approaches to be maintained",
            "paragraph_rank": 92,
            "section_rank": 35,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b12",
                    "start": 16,
                    "text": "[13,",
                    "end": 20
                },
                {
                    "type": "bibr",
                    "ref_id": "b16",
                    "start": 20,
                    "text": "17,",
                    "end": 23
                },
                {
                    "type": "bibr",
                    "ref_id": "b32",
                    "start": 23,
                    "text": "32]",
                    "end": 26
                }
            ]
        },
        {
            "text": "JHEP01(2015)125",
            "section_rank": 36
        },
        {
            "section": "JHEP01(2015)125",
            "text": "at detector level giving us the ability to perform fast and accurate multi-parameter fits for lagrangian parameters directly from the data. This is possible once the convolution in eq. (2.1) is performed and after normalization of the signal and background pdfs allowing us to obtain the full detector level likelihood L( A) for a particular dataset. With the likelihood in hand a maximization procedure to find the global maximum can be performed to obtain the value of the parameters for which the likelihood is maximized. For this task we have incorporated the well established MINUIT [61] function minimization/maximization code into our framework. We find excellent rates of convergence and a high degree of stability in locating the global maximum of the likelihood as well as accurate extraction of the parameters as demonstrated in [38][39][40][41] where more details can be found. One important feature of the procedure is that the computationally intensive component of evaluating the likelihood only needs to be done for the events in the final dataset used in the fit for a given experiment. Therefore the computationally expensive pieces can be calculated on the computing grid prior to the analysis of the data, and the fit for parameter extraction itself is then completed within a few seconds. This allows for a great deal of flexibility, including testing alternative parameterizations, when fitting the undetermined parameters. Many examples of the types of parameter extractions which can be done within our framework, both at generator and at detector level, can be found in [35,[37][38][39]41].",
            "paragraph_rank": 93,
            "section_rank": 36,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b61",
                    "start": 588,
                    "text": "[61]",
                    "end": 592
                },
                {
                    "type": "bibr",
                    "ref_id": "b38",
                    "start": 840,
                    "text": "[38]",
                    "end": 844
                },
                {
                    "type": "bibr",
                    "ref_id": "b39",
                    "start": 844,
                    "text": "[39]",
                    "end": 848
                },
                {
                    "type": "bibr",
                    "ref_id": "b40",
                    "start": 848,
                    "text": "[40]",
                    "end": 852
                },
                {
                    "type": "bibr",
                    "ref_id": "b41",
                    "start": 852,
                    "text": "[41]",
                    "end": 856
                },
                {
                    "type": "bibr",
                    "ref_id": "b35",
                    "start": 1595,
                    "text": "[35,",
                    "end": 1599
                },
                {
                    "type": "bibr",
                    "ref_id": "b37",
                    "start": 1599,
                    "text": "[37]",
                    "end": 1603
                },
                {
                    "type": "bibr",
                    "ref_id": "b38",
                    "start": 1603,
                    "text": "[38]",
                    "end": 1607
                },
                {
                    "type": "bibr",
                    "ref_id": "b39",
                    "start": 1607,
                    "text": "[39]",
                    "end": 1611
                },
                {
                    "type": "bibr",
                    "ref_id": "b41",
                    "start": 1611,
                    "text": "41]",
                    "end": 1614
                }
            ]
        },
        {
            "text": "Summary and conclusions",
            "section_rank": 37
        },
        {
            "section": "Summary and conclusions",
            "text": "In this study we build upon an earlier study [35] to construct a comprehensive analysis framework aimed at extracting as much information as possible from the Higgs golden channel. Our framework is based on a maximum likelihood method constructed from analytic expressions of the fully differential cross sections for the h \u2192 4 decay as well as the dominant irreducible qq \u2192 4 background which were computed in [19,35]. As our main result, we have constructed the full 12-dimensional detector level likelihood utilizing all observables available in the golden channel. This allows us to perform parameter extraction of the various possible Higgs couplings, including general CP odd/even admixtures and any possible phases.",
            "paragraph_rank": 94,
            "section_rank": 37,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b35",
                    "start": 45,
                    "text": "[35]",
                    "end": 49
                },
                {
                    "type": "bibr",
                    "ref_id": "b18",
                    "start": 411,
                    "text": "[19,",
                    "end": 415
                },
                {
                    "type": "bibr",
                    "ref_id": "b35",
                    "start": 415,
                    "text": "35]",
                    "end": 418
                }
            ]
        },
        {
            "section": "Summary and conclusions",
            "text": "The detector-level likelihood is obtained by the explicit convolution of a transfer function, encapsulating the relevant detector effects, with the generator-level probability density formed out of the signal and background differential cross sections. After performing this 12-dimensional convolution integral and its normalization we obtain a probability density function from which we construct an un-binned detector-level likelihood which is a continuous function of the effective couplings.",
            "paragraph_rank": 95,
            "section_rank": 37
        },
        {
            "section": "Summary and conclusions",
            "text": "In summary we have given broad overview of a framework optimized for extracting Higgs couplings in the golden channel. We have sketched how the convolution is performed and shown various validations as well as discussed the principles and theoretical basis of the framework. Many of the technical details as well as results using our framework can be found in [19,35,37,38,40,41]. This framework has already proved useful in a recent CMS analysis [38,39] and can be used in the future in a variety of ways to study Higgs couplings in the golden channel using data obtained at the LHC and other future colliders.",
            "paragraph_rank": 96,
            "section_rank": 37,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b18",
                    "start": 360,
                    "text": "[19,",
                    "end": 364
                },
                {
                    "type": "bibr",
                    "ref_id": "b35",
                    "start": 364,
                    "text": "35,",
                    "end": 367
                },
                {
                    "type": "bibr",
                    "ref_id": "b37",
                    "start": 367,
                    "text": "37,",
                    "end": 370
                },
                {
                    "type": "bibr",
                    "ref_id": "b38",
                    "start": 370,
                    "text": "38,",
                    "end": 373
                },
                {
                    "type": "bibr",
                    "ref_id": "b40",
                    "start": 373,
                    "text": "40,",
                    "end": 376
                },
                {
                    "type": "bibr",
                    "ref_id": "b41",
                    "start": 376,
                    "text": "41]",
                    "end": 379
                },
                {
                    "type": "bibr",
                    "ref_id": "b38",
                    "start": 447,
                    "text": "[38,",
                    "end": 451
                },
                {
                    "type": "bibr",
                    "ref_id": "b39",
                    "start": 451,
                    "text": "39]",
                    "end": 454
                }
            ]
        },
        {
            "text": "7 )",
            "section_rank": 38
        },
        {
            "text": "Figure 2 .",
            "section_rank": 39
        },
        {
            "section": "Figure 2 .",
            "text": "Figure 2. Projections of the Y , | p T |, \u221a\u015d \u2261 M 4 and cos \u0398 (see section 3.1 for definitions) spectra showing validation of the convolution described in eq. (4.1) for the background. In blue we show the 'boosted' Madgraph sample with acceptance cuts and detector smearing applied while in red we show projections from our differential cross section after the convolution integration.",
            "paragraph_rank": 98,
            "section_rank": 39
        },
        {
            "text": "Figure 3 .",
            "section_rank": 40
        },
        {
            "section": "Figure 3 .",
            "text": "Figure 3. Projections of the M 1 , M 2 , cos \u03b8 1 , cos \u03b8 2 , \u03a6 and \u03a6 1 (see section 3.1 for definitions) spectra showing validation of the convolution described in eq. (4.1) for the background. In blue we show the 'boosted' Madgraph sample with acceptance cuts and detector smearing applied while in red we show projections from our differential cross section after the convolution integration.",
            "paragraph_rank": 99,
            "section_rank": 40
        },
        {
            "text": "Figure 4 .Figure 5 .",
            "section_rank": 41
        },
        {
            "section": "Figure 4 .Figure 5 .",
            "text": "Figure 4. Projections of the Y , | p T |, \u221a\u015d \u2261 M 4 and cos \u0398 (see section 3.1 for definitions) spectra showing validation of the convolution described in eq. (4.4) for the tree level SM signal. In blue we show the 'boosted' Madgraph sample with acceptance cuts and detector smearing applied while in red we show projections from our differential cross section after the convolution integration.",
            "paragraph_rank": 100,
            "section_rank": 41
        },
        {
            "section": "Figure 4 .Figure 5 .",
            "text": "Though we will not discuss it explicitly here, we are also able to extend our framework to the h \u2192 \u03b3\u03b3 and h \u2192 2 \u03b3 channels.",
            "paragraph_rank": 101,
            "section_rank": 41
        },
        {
            "section": "Figure 4 .Figure 5 .",
            "text": "This lagrangian has been implemented[55] into the FeynRules/Madgraph[56,57] framework for validation purposes.",
            "paragraph_rank": 102,
            "section_rank": 41,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b55",
                    "start": 36,
                    "text": "[55]",
                    "end": 40
                },
                {
                    "type": "bibr",
                    "ref_id": "b56",
                    "start": 68,
                    "text": "[56,",
                    "end": 72
                },
                {
                    "type": "bibr",
                    "ref_id": "b57",
                    "start": 72,
                    "text": "57]",
                    "end": 75
                }
            ]
        },
        {
            "text": "Acknowledgments",
            "section_rank": 43
        },
        {
            "section": "Acknowledgments",
            "text": "The authors are grateful to Artur Apresyan, Michalis Bachtis, Adam   Open Access. This article is distributed under the terms of the Creative Commons Attribution License (CC-BY 4.0), which permits any use, distribution and reproduction in any medium, provided the original author(s) and source are credited.",
            "paragraph_rank": 103,
            "section_rank": 43
        },
        {
            "text": "JHEP01(2015)125",
            "section_rank": 45
        },
        {
            "section": "JHEP01(2015)125",
            "text": "This gives finally for the normalized signal pdf,",
            "paragraph_rank": 104,
            "section_rank": 45
        },
        {
            "section": "JHEP01(2015)125",
            "text": ". (5.8) Since each N ij nm is computed, one does not need to compute the normalization each time a new hypothesis for A is constructed. The procedure outlined here also works on more general polynomial functions of the parameters A which one finds after expanding potentially momentum-dependent form factors in powers of momenta. See [40] for this more general discussion.",
            "paragraph_rank": 105,
            "section_rank": 45,
            "ref_spans": [
                {
                    "type": "bibr",
                    "start": 2,
                    "text": "(5.8)",
                    "end": 7
                },
                {
                    "type": "bibr",
                    "ref_id": "b40",
                    "start": 334,
                    "text": "[40]",
                    "end": 338
                }
            ]
        },
        {
            "section": "JHEP01(2015)125",
            "text": "Note also that if we take the Higgs mass as a fixed input and only fit for ratios of parameters and not their overall normalization, we do not need the absolute normalization of the differential cross sections. It thus suffices to have the relative normalization between the different components correct when performing the maximization. This fact greatly reduces the computational complexity. Instead of propagating the full normalization and aligning units correctly so that when one integrates over all 8 dimensions unity is obtained, it is sufficient to do a Monte Carlo integration using a fixed sample size in a consistent and sufficiently large range. The meaning of the log likelihood difference remains unchanged with this construction. Further details of the normalization procedure for both signal and background are found in [40,41].",
            "paragraph_rank": 106,
            "section_rank": 45,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b40",
                    "start": 837,
                    "text": "[40,",
                    "end": 841
                },
                {
                    "type": "bibr",
                    "ref_id": "b41",
                    "start": 841,
                    "text": "41]",
                    "end": 844
                }
            ]
        },
        {
            "text": "Signal plus background pdf and final likelihood",
            "section_rank": 46
        },
        {
            "section": "Signal plus background pdf and final likelihood",
            "text": "With eq. (5.3) and eq. (5.8) in hand we can now build the signal plus background pdf from which the total likelihood will be constructed. The signal plus background pdf can be written as,",
            "paragraph_rank": 107,
            "section_rank": 46
        },
        {
            "section": "Signal plus background pdf and final likelihood",
            "text": "where O \u2261 (\u015d, M 1 , M 2 , \u2126) is our final set of observables to be used in the construction of the likelihood and F i B is the background fraction for a particular component, each of which must also be extracted.",
            "paragraph_rank": 108,
            "section_rank": 46
        },
        {
            "section": "Signal plus background pdf and final likelihood",
            "text": "The sum over background components is given by,",
            "paragraph_rank": 109,
            "section_rank": 46
        },
        {
            "section": "Signal plus background pdf and final likelihood",
            "text": "where P qq B (O) is the dominant qq \u2192 4 component and is obtained via the convolution integral in eq. (2.1). The sub-dominant gg \u2192 4 and Z + X components, given by P gg B (O) and P Z+X B (O) respectively, must be obtained via the linearly interpolated 'look-up' tables from large Monte Carlo samples as discussed in section 3.6.",
            "paragraph_rank": 110,
            "section_rank": 46
        },
        {
            "section": "Signal plus background pdf and final likelihood",
            "text": "We can now write the likelihood of obtaining a particular dataset containing N events as, ",
            "paragraph_rank": 111,
            "section_rank": 46
        }
    ]
}