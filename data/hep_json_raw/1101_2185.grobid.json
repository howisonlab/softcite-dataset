{
    "level": "paragraph",
    "abstract": [
        {
            "text": "Measurements of luminosity obtained using the ATLAS detector during early running of the Large Hadron Collider (LHC) at \u221a s = 7 TeV are presented. The luminosity is independently determined using several detectors and multiple algorithms, each having different acceptances, systematic uncertainties and sensitivity to background. The ratios of the luminosities obtained from these methods are monitored as a function of time and of \u03bc, the average number of inelastic interactions per bunch crossing. Residual time-and \u03bc-dependence between the methods is less than 2% for 0 < \u03bc < 2.5. Absolute luminosity calibrations, performed using beam separation scans, have a common systematic uncertainty of \u00b111%, dominated by the measurement of the LHC beam currents. After calibration, the luminosities obtained from the different methods differ by at most \u00b12%. The visible cross sections measured using the beam scans are compared to predictions obtained with the PYTHIA and PHOJET event generators and the ATLAS detector simulation.",
            "paragraph_rank": 2,
            "section_rank": 1
        }
    ],
    "body_text": [
        {
            "text": "Introduction and overview",
            "section_rank": 2
        },
        {
            "section": "Introduction and overview",
            "text": "A major goal of the ATLAS [1] physics program for 2010 is the measurement of cross sections for Standard Model processes. Accurate determination of the luminosity is an essential ingredient of this program. This article describes the first results on luminosity determination, including an assessment of the systematic uncertainties, for data taken at the LHC [2] in proton-proton collisions at a center-of-mass energy \u221a s = 7 TeV. It is organized as follows. The ATLAS strategy for measuring and calibrating the luminosity is outlined below and is followed in Sect. 2 by a brief description of the subdetectors used for luminosity determination. Each of these detectors is associated with one e-mail: atlas.secretariat@cern.ch or more luminosity algorithms, described in Sect. 3. The absolute calibration of these algorithms using beam-separation scans forms the subject of Sect. 4. The internal consistency of the luminosity measurements is assessed in Sect. 5. Finally, the scan-based calibrations are compared in Sect. 6 to those predicted using the PYTHIA [3] and PHOJET [4] event generators coupled to a full GEANT4 [5] simulation of the ATLAS detector response [6]. Conclusions are summarized in Sect. 7.",
            "paragraph_rank": 3,
            "section_rank": 2,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b0",
                    "start": 26,
                    "text": "[1]",
                    "end": 29
                },
                {
                    "type": "bibr",
                    "ref_id": "b1",
                    "start": 360,
                    "text": "[2]",
                    "end": 363
                },
                {
                    "type": "bibr",
                    "ref_id": "b2",
                    "start": 1061,
                    "text": "[3]",
                    "end": 1064
                },
                {
                    "type": "bibr",
                    "ref_id": "b3",
                    "start": 1076,
                    "text": "[4]",
                    "end": 1079
                },
                {
                    "type": "bibr",
                    "start": 1122,
                    "text": "[5]",
                    "end": 1125
                },
                {
                    "type": "bibr",
                    "ref_id": "b5",
                    "start": 1168,
                    "text": "[6]",
                    "end": 1171
                },
                {
                    "type": "bibr",
                    "ref_id": "b6",
                    "start": 1209,
                    "text": "7",
                    "end": 1210
                }
            ]
        },
        {
            "section": "Introduction and overview",
            "text": "The luminosity of a pp collider can be expressed as",
            "paragraph_rank": 4,
            "section_rank": 2
        },
        {
            "section": "Introduction and overview",
            "text": "where R inel is the rate of inelastic collisions and \u03c3 inel is the pp inelastic cross section. If a collider operates at a revolution frequency f r and n b bunches cross at the interaction point, this expression can be rewritten as",
            "paragraph_rank": 5,
            "section_rank": 2
        },
        {
            "section": "Introduction and overview",
            "text": "where \u03bc is the average number of inelastic interactions per bunch crossing (BC). Thus, the instantaneous luminosity can be determined using any method that measures the ratio \u03bc/\u03c3 inel . A fundamental ingredient of the ATLAS strategy to assess and control the systematic uncertainties affecting the absolute luminosity determination is to compare the measurements of several luminosity detectors, most of which use more than one counting technique. These multiple detectors and algorithms are characterized by significantly different acceptance, response to pile-up (multiple pp interactions within the same bunch crossing), and sensitivity to instrumental effects and to beam-induced backgrounds. The level of consistency across the various methods, over the full range of single-bunch luminosities and beam conditions, provides valuable cross-checks as well as an estimate of the detector-related systematic uncertainties.",
            "paragraph_rank": 6,
            "section_rank": 2
        },
        {
            "section": "Introduction and overview",
            "text": "Techniques for luminosity determination can be classified as follows:",
            "paragraph_rank": 7,
            "section_rank": 2
        },
        {
            "section": "Introduction and overview",
            "text": "-Event Counting: here one determines the fraction of bunch crossings during which a specified detector registers an \"event\" satisfying a given selection requirement. For instance, a bunch crossing can be said to contain an \"event\" if at least one pp interaction in that crossing induces at least one observed hit in the detector being considered. -Hit Counting: here one counts the number of hits (for example, electronic channels or energy clusters above a specified threshold) per bunch crossing in a given detector. -Particle Counting: here one determines the distribution of the number of particles per beam crossing (or its mean) inferred from reconstructed quantities (e.g. tracks), from pulse-height distributions or from other observables that reflect the instantaneous particle flux traversing the detector (e.g. the total ionization current drawn by a liquidargon calorimeter sector).",
            "paragraph_rank": 8,
            "section_rank": 2
        },
        {
            "section": "Introduction and overview",
            "text": "At present, ATLAS relies only on event-counting methods for the determination of the absolute luminosity. Equation (2) can be rewritten as:",
            "paragraph_rank": 9,
            "section_rank": 2
        },
        {
            "section": "Introduction and overview",
            "text": "where \u03b5 is the efficiency for one inelastic pp collision to satisfy the event-selection criteria, and \u03bc vis \u2261 \u03b5\u03bc is the average number of visible inelastic interactions per BC (i.e. the mean number of pp collisions per BC that pass that \"event\" selection). The visible cross section \u03c3 vis \u2261 \u03b5\u03c3 inel is the calibration constant that relates the measurable quantity \u03bc vis to the luminosity L. Both \u03b5 and \u03c3 vis depend on the pseudorapidity distribution and particle composition of the collision products, and are therefore different for each luminosity detector and algorithm. In the limit \u03bc vis 1, the average number of visible inelastic interactions per BC is given by the intuitive expression",
            "paragraph_rank": 10,
            "section_rank": 2
        },
        {
            "section": "Introduction and overview",
            "text": "where N is the number of events passing the selection criteria that are observed during a given time interval, and N BC is the number of bunch crossings in that same interval. When \u03bc increases, the probability that two or more pp interactions occur in the same bunch crossing is no longer negligible, and \u03bc vis is no longer linearly related to the raw event count N . Instead \u03bc vis must be calculated taking into account Poisson statistics, and in some cases, instrumental or pile-up related effects (Sect. 3.4). Several methods can be used to determine \u03c3 vis . At the Tevatron, luminosity measurements are normalized to the total inelastic pp cross section, with simulated data used to determine the event-or hit-counting efficiencies [7,8]. Unlike the case of the Tevatron, where the pp cross section was determined 1 independently by two experiments, the pp inelastic cross section at 7 TeV has not been measured yet. Extrapolations from lower energy involve significant systematic uncertainties, as does the determination of \u03b5, which depends on the modeling of particle momentum distributions and multiplicity for the full pp inelastic cross section. In the future, the ALFA detector [9] will provide an absolute luminosity calibration at ATLAS through the measurement of elastic pp scattering at small angles in the Coulomb-Nuclear Interference region. In addition, it is possible to normalize cross section measurements to electroweak processes for which precise NNLO calculations exist, for example W and Z production [10]. Although the cross section for the production of electroweak bosons in pp collisions at \u221a s = 7 TeV has been measured by ATLAS [11] and found to be in agreement with the Standard Model expectation, with experimental and theoretical systematic uncertainties of \u223c7%, we choose not to use these data as a luminosity calibration, since such use would preclude future comparisons with theory. However, in the future, it will be possible to monitor the variation of luminosity with time using W and Z production rates.",
            "paragraph_rank": 11,
            "section_rank": 2,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b6",
                    "start": 736,
                    "text": "[7,",
                    "end": 739
                },
                {
                    "type": "bibr",
                    "ref_id": "b7",
                    "start": 739,
                    "text": "8]",
                    "end": 741
                },
                {
                    "type": "bibr",
                    "ref_id": "b0",
                    "start": 818,
                    "text": "1",
                    "end": 819
                },
                {
                    "type": "bibr",
                    "ref_id": "b9",
                    "start": 1525,
                    "text": "[10]",
                    "end": 1529
                },
                {
                    "type": "bibr",
                    "ref_id": "b10",
                    "start": 1658,
                    "text": "[11]",
                    "end": 1662
                }
            ]
        },
        {
            "section": "Introduction and overview",
            "text": "An alternative is to calibrate the counting techniques using the absolute luminosity L inferred from measured accelerator parameters [12,13]:",
            "paragraph_rank": 12,
            "section_rank": 2,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b11",
                    "start": 133,
                    "text": "[12,",
                    "end": 137
                },
                {
                    "type": "bibr",
                    "ref_id": "b12",
                    "start": 137,
                    "text": "13]",
                    "end": 140
                }
            ]
        },
        {
            "section": "Introduction and overview",
            "text": "x \u03a3 y (5) where n 1 and n 2 are the numbers of particles in the two colliding bunches and \u03a3 x and \u03a3 y characterize the widths of the horizontal and vertical beam profiles. One typically measures \u03a3 x and \u03a3 y using van der Meer (vdM) scans (sometimes also called beam-separation or luminosity scans) [14]. The observed event rate is recorded while scanning the two beams across each other first in the horizontal (x), then in the vertical (y) direction. This measurement yields two bellshaped curves, with the maximum rate at zero separation, from which one extracts the values of \u03a3 x and \u03a3 y (Sect. 4). The luminosity at zero separation can then be computed using (5), and \u03c3 vis extracted from (3) using the measured values of L and \u03bc vis . The vdM technique allows the determination of \u03c3 vis without a priori knowledge of the inelastic pp cross section or of detector efficiencies. Scan results can therefore be used to test the reliability of Monte Carlo event generators and of the ATLAS simulation by comparing the visible cross sections predicted by the Monte Carlo for various detectors and algorithms to those obtained from the scan data. 1 In fact, Tevatron cross sections were measured at \u221a s = 1.8 TeV and extrapolated to \u221a s = 1.96 TeV.",
            "paragraph_rank": 13,
            "section_rank": 2,
            "ref_spans": [
                {
                    "type": "bibr",
                    "start": 6,
                    "text": "(5)",
                    "end": 9
                },
                {
                    "type": "bibr",
                    "start": 298,
                    "text": "[14]",
                    "end": 302
                },
                {
                    "type": "bibr",
                    "start": 663,
                    "text": "(5)",
                    "end": 666
                },
                {
                    "type": "bibr",
                    "ref_id": "b0",
                    "start": 1145,
                    "text": "1",
                    "end": 1146
                }
            ]
        },
        {
            "section": "Introduction and overview",
            "text": "ATLAS uses the vdM method to obtain its absolute luminosity calibration both for online monitoring and for offline analysis. Online, the luminosity at the ATLAS interaction point (IP1) is determined approximately once per second using the counting rates from the detectors and algorithms described in Sects. 2 and 3. The raw event count N is converted to a visible average number of interactions per crossing \u03bc vis as described in Sect. 3.4, and expressed as an absolute luminosity using the visible cross sections \u03c3 vis measured during beam-separation scans. The results of all the methods are displayed in the ATLAS control room, and the luminosity from a single online \"preferred\" algorithm is transmitted to the LHC control room, providing real-time feedback for accelerator tuning.",
            "paragraph_rank": 14,
            "section_rank": 2
        },
        {
            "section": "Introduction and overview",
            "text": "The basic time unit for storing luminosity information for later use is the Luminosity Block (LB). The duration of a LB is approximately two minutes, with begin and end times set by the ATLAS data acquisition system (DAQ). All dataquality information, as well as the luminosity, are stored in a relational database for each LB. The luminosity tables in the offline database allow for storage of multiple methods for luminosity determination and are versioned so that updated calibration constants can be applied. The results of all online luminosity methods are stored, and results from additional offline algorithms are added. This infrastructure enables comparison of the results from different methods as a function of time. After data quality checks have been performed and calibrations have been validated, one algorithm is chosen as the \"preferred\" offline algorithm for physics analysis and stored as such in the database. Luminosity information is stored as delivered luminosity. Corrections for trigger prescales, DAQ deadtime and other sources of data loss are performed on an LB-by-LB basis when the integrated luminosity is calculated.",
            "paragraph_rank": 15,
            "section_rank": 2
        },
        {
            "text": "The ATLAS luminosity detectors",
            "section_rank": 3
        },
        {
            "section": "The ATLAS luminosity detectors",
            "text": "The ATLAS detector is described in detail in Ref. [1]. This section provides a brief description of the subsystems used for luminosity measurements, arranged in order of increasing pseudorapidity. 2 A summary of the relevant characteristics of these detectors is given in Table 1. 2 ATLAS uses a coordinate system where the nominal interaction point is at the center of the detector. The direction of beam 2 (counterclockwise around the LHC ring) defines the z-axis; the x-y plane is transverse to the beam. The positive x-axis is defined as pointing to the center of the ring, and the positive y-axis upwards. Side-A of the detector is on the positive-z side and side-C on the negative-z side. The azimuthal angle \u03c6 is measured around the beam axis. The pseudorapidity \u03b7 is defined as \u03b7 = \u2212 ln(tan \u03b8/2) where \u03b8 is the polar angle from the beam axis. The Inner Detector is used to measure the momentum of charged particles. It consists of three subsystems: a pixel detector, a silicon strip tracker (SCT) and a transition radiation straw tube tracker (TRT). These detectors are located inside a solenoidal magnet that provides a 2 T axial field. The tracking efficiency as a function of transverse momentum (p T ), averaged over all pseudorapidity, rises from \u223c10% at 100 MeV to \u223c86% for p T above a few GeV [15].",
            "paragraph_rank": 16,
            "section_rank": 3,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b0",
                    "start": 50,
                    "text": "[1]",
                    "end": 53
                },
                {
                    "type": "bibr",
                    "ref_id": "b1",
                    "start": 197,
                    "text": "2",
                    "end": 198
                },
                {
                    "type": "table",
                    "ref_id": "tab_0",
                    "start": 272,
                    "text": "Table 1.",
                    "end": 280
                },
                {
                    "type": "bibr",
                    "start": 1308,
                    "text": "[15]",
                    "end": 1312
                }
            ]
        },
        {
            "section": "The ATLAS luminosity detectors",
            "text": "For the initial running period at low instantaneous luminosity (<10 33 cm \u22122 s \u22121 ), ATLAS has been equipped with segmented scintillator counters, the Minimum Bias Trigger Scintillators (MBTS), located at z = \u00b1365 cm from the collision center. The main purpose of the MBTS is to provide a trigger on minimum collision activity during a pp bunch crossing. Light emitted by the scintillators is collected by wavelength-shifting optical fibers and guided to a photomultiplier tube (PMT). The MBTS signals, after being shaped and amplified, are fed into leading-edge discriminators and sent to the central trigger processor (CTP). An MBTS hit is defined as a signal above the discriminator threshold (50 mV).",
            "paragraph_rank": 17,
            "section_rank": 3
        },
        {
            "section": "The ATLAS luminosity detectors",
            "text": "The precise timing (\u223c1 ns) provided by the liquid argon (LAr) calorimeter is used to count events with collisions, therefore providing a measurement of the luminosity. The LAr calorimeter covers the region |\u03b7| < 4.9. It consists of the electromagnetic calorimeter (EM) for |\u03b7| < 3.2, the Hadronic Endcap for 1.5 < |\u03b7| < 3.2 and the Forward Calorimeter (FCal) for 3.1 < |\u03b7| < 4.9. The luminosity analysis is based on energy deposits in the Inner Wheel of the electromagnetic endcap (EMEC) and the first layer of the FCal. The precise timing is used to reject background for the offline measurement of the luminosity.",
            "paragraph_rank": 18,
            "section_rank": 3
        },
        {
            "section": "The ATLAS luminosity detectors",
            "text": "The primary purpose of the Beam Conditions Monitor (BCM) [16] is to monitor beam losses and provide fast feedback to the accelerator operations team. It is an essential ingredient of the detector protection system, providing a fast accelerator abort signal in the event of large beam loss. The BCM consists of two arms of diamond sensors located at z = \u00b1184 cm and r = 5.5 cm and uses programable frontend electronics (FPGAs) to histogram the single-sided and coincidence rates as a function of Bunch Crossing Identifier (BCID). These histograms are read out by the BCM monitoring software and made available to other online applications through the online network. Thus, bunch-by-bunch rates are available and are not subject to DAQ deadtime. The detector's value as a luminosity monitor is further enhanced by its excellent timing (0.7 ns) which allows for rejection of backgrounds from beam-halo.",
            "paragraph_rank": 19,
            "section_rank": 3,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b15",
                    "start": 57,
                    "text": "[16]",
                    "end": 61
                }
            ]
        },
        {
            "section": "The ATLAS luminosity detectors",
            "text": "LUCID is a Cherenkov detector specifically designed for measuring the luminosity in ATLAS. Sixteen optically reflecting aluminum tubes filled with C 4 F 10 gas surround the beampipe on each side of the interaction point. Cerenkov photons created by charged particles in the gas are reflected by the tube walls until they reach PMTs situated at the back end of the tubes. The Cherenkov light created in the gas typically produces 60-70 photoelectrons, while the quartz window adds another 40 photoelectrons to the signal. After amplification, the signals are split three-fold and presented to a set of constant fraction discriminators (CFDs), charge-todigital converters and 32-bit flash ADCs with 80 samplings. If the signal has a pulse height larger than the discriminator threshold (which is equivalent to 15 photoelectrons) a tube is \"hit.\" The hit-pattern produced by all the discriminators is sent to a custom-built electronics card (LUMAT) which contains FPGAs that can be programmed with different luminosity algorithms. LUMAT receives timing signals from the LHC clock used for synchronizing all detectors and counts the number of events or hits passing each luminosity algorithm for each BCID in an orbit. It also records the number of orbits made by the protons in the LHC during the counting interval. At present there are four algorithms implemented in the LUMAT firmware (see Sect. 3.2.3). The data from LUMAT are broadcast to the ATLAS online network and archived for later offline use. In addition, LUMAT provides triggers for the CTP and sends the hit-patterns to the DAQ. The LUCID electronics is decoupled from the DAQ so that it can provide an online luminosity determination even if no global ATLAS run is in progress.",
            "paragraph_rank": 20,
            "section_rank": 3
        },
        {
            "section": "The ATLAS luminosity detectors",
            "text": "The primary purpose of the Zero-Degree Calorimeter (ZDC) is to detect forward neutrons and photons with |\u03b7| > 8.3 in both pp and heavy-ion collisions. The ZDC consists of two arms located at z = \u00b1140 m in slots in the LHC TAN (Target Absorber Neutral) [2], occupying space that would otherwise contain inert copper shielding bars. In its final configuration, each arm consists of calorimeter modules, one electromagnetic (EM) module (about 29 radiation lengths deep) followed by three hadronic modules (each about 1.14 interaction lengths deep). The modules are composed of tungsten with an embedded matrix of quartz rods which are coupled to photo multiplier tubes and read out through CFDs. Until July 2010 only the three hadronic modules were installed to allow running of the LHCf experiment [17], which occupied the location where the EM module currently sits. Taking into account the limiting aperture of the beamline, the effective ZDC acceptance for neutrals corresponds to 1 GeV in p T for a 3.5 TeV neutron or photon. Charged particles are swept out of the ZDC acceptance by the final-triplet quadrupoles; Monte Carlo studies have shown that neutral secondaries contribute a negligible amount to the typical ZDC energy. A hit in the ZDC is defined as an energy deposit above CFD threshold. The ZDC is fully efficient for energies above \u223c400 GeV.",
            "paragraph_rank": 21,
            "section_rank": 3,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b1",
                    "start": 252,
                    "text": "[2]",
                    "end": 255
                },
                {
                    "type": "bibr",
                    "ref_id": "b16",
                    "start": 796,
                    "text": "[17]",
                    "end": 800
                }
            ]
        },
        {
            "text": "Luminosity algorithms",
            "section_rank": 4
        },
        {
            "section": "Luminosity algorithms",
            "text": "The time structure of the LHC beams and its consequences for the luminosity measurement (Sect. 3.1) drive the architecture of the online luminosity infrastructure and algorithms (Sect. 3.2). Some approaches to luminosity determination, however, are only possible offline (Sect. 3.3). In all cases, dealing properly with pile-up dependent effects (Sect. 3.4) is essential to ensure the precision of the luminosity measurements.",
            "paragraph_rank": 22,
            "section_rank": 4
        },
        {
            "text": "Bunch patterns and luminosity backgrounds",
            "section_rank": 5
        },
        {
            "section": "Bunch patterns and luminosity backgrounds",
            "text": "The LHC beam is subdivided into 35640 RF-buckets of which nominally every tenth can contain a bunch. Subtracting abort and injection gaps, up to 2808 of these 3564 \"slots\", which are 25 ns long, can be filled with beam. Each of these possible crossings is labeled by an integer BCID which is stored as part of the ATLAS event record. Figure 1 displays the event rate per BC, as measured by two LUCID algorithms, as a function of BCID and timeaveraged over a run that lasted about 15 hours. For this run, 35 bunch pairs collided in both ATLAS and CMS. These are called \"colliding\" (or \"paired\") BCIDs. Bunches that do not collide at IP1 are labeled \"unpaired.\" Unpaired bunches that undergo no collisions in any of the IPs are called \"isolated.\" The structures observed in this figure are visible in the bunch-by-bunch luminosity distributions of all the detectors discussed in this paper, although with magnitudes affected by different instrumental characteristics and background sensitivities. Comparisons of the event rates in colliding, unpaired, isolated and empty bunch crossings for different event-selection criteria provide information about the origin of the luminosity backgrounds, as well as quantitative estimates of the signal purity for each of these detectors and algorithms.",
            "paragraph_rank": 23,
            "section_rank": 5,
            "ref_spans": [
                {
                    "type": "figure",
                    "start": 334,
                    "text": "Figure 1",
                    "end": 342
                }
            ]
        },
        {
            "section": "Bunch patterns and luminosity backgrounds",
            "text": "Requiring at least one hit on at least one side (this is referred to as an Event_OR algorithm below) reveals a complex time structure (Fig. 1a). The colliding bunches are clearly distinguished, with a rate of about four orders of magnitude above background. They are followed by a long tail Fig. 1 Bunch-by-bunch event rate per bunch crossing in ATLAS run 162882, as recorded by a LUCID algorithm that requires a at least one hit on either LUCID side (Event_OR), or b at least one hit on both LUCID sides (Event_AND) within the same BCID where the rate builds up when the paired BCID's follow each other in close succession, but decays slowly when no collisions occur for a sufficiently long time. This \"afterglow\" is also apparent when analyzing the luminosity response of Event_OR algorithms using the BCM or MBTS, albeit at different levels and with different time constants. Instrumental causes such as reflections in signal cables or afterpulsing in photomultipliers have been excluded by pulsing the LED's (the laser) used to calibrate the LUCID (MBTS) phototubes. The \"afterglow\" level is proportional to the instantaneous luminosity (but depends on the bunch pattern because of the long-decaying tail); it vanishes when beams are out of collision. Requiring a coincidence between the two arms of a luminosity detector suppresses the signal by several orders of magnitude, indicating that the hits are randomly distributed. These observations suggest that this \"afterglow\" is due to photons from nuclear de-excitation, which in turn is induced by the hadronic cascades initiated by pp collision products. This interpretation is supported by FLUKA simulations of very similar observations in the CMS beam-conditions monitor [18]. BCID's from unpaired and isolated bunches appear as small spikes above the afterglow background. These spikes are the result of beam-gas and beam-halo interactions; in some cases, they may also contain a very small fraction of pp collisions between an unpaired bunch in one beam and a satellite-or debunched-proton component in the opposing beam. 3 For the Event_AND algorithm ( Fig. 1b), the coincidence requirement between the A-and C-sides suppresses the afterglow signal by an additional four orders of magnitude, clearly showing that this luminosity background is caused by random signals uncorrelated between the two sides. Unpaired-bunch rates for LUCID_Event_AND lie 4-5 orders of magnitude lower than pp collisions between paired bunches.",
            "paragraph_rank": 24,
            "section_rank": 5,
            "ref_spans": [
                {
                    "type": "figure",
                    "start": 134,
                    "text": "(Fig. 1a)",
                    "end": 143
                },
                {
                    "type": "figure",
                    "start": 291,
                    "text": "Fig. 1",
                    "end": 297
                },
                {
                    "type": "bibr",
                    "start": 1730,
                    "text": "[18]",
                    "end": 1734
                },
                {
                    "type": "bibr",
                    "ref_id": "b2",
                    "start": 2083,
                    "text": "3",
                    "end": 2084
                },
                {
                    "type": "figure",
                    "start": 2115,
                    "text": "Fig. 1b)",
                    "end": 2123
                }
            ]
        },
        {
            "section": "Bunch patterns and luminosity backgrounds",
            "text": "This figure illustrates several important points. First, because only a fraction of the BCID's are filled, an algorithm that selects on colliding BCID's is significantly cleaner than one that is BCID-blind. Second, and provided only colliding BCID's are used, the background is small (LUCID) to moderate (MBTS) for Event_OR algorithms, and negligible for Event_AND. In the Event_OR case, the background contains contributions both from afterglow and from beam-gas and beam-halo interactions: its level thus depends crucially on the time separation between colliding bunches.",
            "paragraph_rank": 25,
            "section_rank": 5
        },
        {
            "text": "Online algorithms",
            "section_rank": 6
        },
        {
            "text": "Online luminosity infrastructure",
            "section_rank": 7
        },
        {
            "section": "Online luminosity infrastructure",
            "text": "Online luminosity monitoring and archiving can be made available even when only the core ATLAS DAQ infrastructure is active; this makes it possible to provide luminosity information for machine tuning independently of the \"busy\" state of the DAQ system and of the hardware status of most subdetectors (except for the CTP and for one or more of the luminosity detectors). In addition, since the online luminosity data are collected in the front-end electronics of each detector (or at the CTP input), there is no need for prescaling, even at the highest luminosities.",
            "paragraph_rank": 26,
            "section_rank": 7
        },
        {
            "section": "Online luminosity infrastructure",
            "text": "The calculation and publication of instantaneous luminosities is performed by an application suite called the Online Luminosity Calculator (OLC). The task of the OLC is to retrieve the raw luminosity information (event or hit counts, number of colliding bunches n b , and number of LHC orbits in the time interval considered) from the online network and to use these data to determine \u03bc and hence the measured luminosity. For each luminosity algorithm, the OLC outputs the instantaneous luminosity, averaged over all colliding BCIDs, at about 1 Hz. These values are displayed on online monitors, stored in the ATLAS online-monitoring archive and shipped to the LHC control room to assist in collision optimization at IP1. In addition, the OLC calculates the luminosity averaged over the current luminosity block (in all cases the luminosity averaged over all colliding BCIDs, and when available the bunch-by-bunch luminosity vector) and stores these in the ATLAS conditions database.",
            "paragraph_rank": 27,
            "section_rank": 7
        },
        {
            "section": "Online luminosity infrastructure",
            "text": "Most methods provide an LB-averaged luminosity measured from colliding bunches only, but for different detectors the requirement is imposed at different stages of the analysis. The BCM readout driver and the LUCID LUMAT module provide bunch-by-bunch raw luminosity information for each LB, as well as the luminosity per LB summed over all colliding BCID's. For these two detectors, the OLC calculates the total (i.e. bunch-integrated) luminosity using an extension of (3) that remains valid even when each bunch pair produces a different luminosity (reflecting a different value of \u03bc) because of different bunch currents and/or emittances:",
            "paragraph_rank": 28,
            "section_rank": 7
        },
        {
            "section": "Online luminosity infrastructure",
            "text": "where the sum is performed over the colliding BCID's. This makes it possible to properly apply the pile-up correction bunch-by-bunch (Sect. 3.4).",
            "paragraph_rank": 29,
            "section_rank": 7
        },
        {
            "section": "Online luminosity infrastructure",
            "text": "For detectors where bunch-by-bunch luminosity is unavailable online, (3) is used, with \u03bc vis computed using the known number of paired BCID's and the raw luminosity information averaged over either the colliding BCID's (this is the case for the MBTS) or all BCID's (the front-end luminosity infrastructure of the ZDC provides no bunch-bybunch capability at this time).",
            "paragraph_rank": 30,
            "section_rank": 7
        },
        {
            "section": "Online luminosity infrastructure",
            "text": "For the MBTS, which lacks appropriate FPGA capabilities in the front end, the selection of colliding bunches is done through the trigger system. The BCID's that correspond to colliding bunches are identified and grouped in a list called the \"physics bunch group,\" which is used to gate the physics triggers. A second set of triggers using unpaired bunches is used offline to estimate beam backgrounds. The MBTS counters provide trigger signals to the CTP, which then uses bunch-group information to create separate triggers for physics and for unpaired bunch groups. The CTP scalers count the number of events that fire each trigger, as well as the number of LHC orbits (needed to compute the rate per bunch crossing). Every 10 s these scalers are read out and published to the online network. Three values are stored for each trigger type: trigger before prescale (TBP), trigger after prescale and trigger after veto (TAV). The TBP counts are calculated directly using inputs to the CTP and are therefore free from any dead time or veto (except when the DAQ is paused), while the TAV corresponds to the rate of accepted events for which a trigger fired. To maximize the statistical power of the measurement and remain unaffected by prescale changes, online luminosity measurements by the MBTS algorithms use the TBP rates.",
            "paragraph_rank": 31,
            "section_rank": 7
        },
        {
            "text": "BCM algorithms",
            "section_rank": 8
        },
        {
            "section": "BCM algorithms",
            "text": "Out of the four sensors on each BCM side, only two are currently used for online luminosity determination. Three online algorithms, implemented in the firmware of the BCM readout driver, report results:",
            "paragraph_rank": 32,
            "section_rank": 8
        },
        {
            "section": "BCM algorithms",
            "text": "-BCM_Event_OR counts the number of events per BC in which at least one hit above threshold occurs on either the A-side, the C-side or both, within a 12.5 ns window centered on the arrival time of particles originating at IP1. -BCM_Event_AND counts the number of events per BC where at least one hit above threshold is observed, within a 12.5 ns-wide coincidence window, both on the A-and the C-side. Because the geometric coverage of the BCM is quite small, the event rate reported by this algorithm during the beam-separation scans was too low to perform a reliable calibration. Therefore this algorithm will not be considered further in this paper. -BCM_Event_XORC counts the number of events per BC where at least one hit above threshold is observed on the C-side, with none observed on the A-side within the same 12.5 ns-wide window. Because converting the event-counting probability measured by this method into an instantaneous luminosity involves more complex combinatorics than for the simpler Event_OR and Event_AND cases, fully exploiting this algorithm requires more extensive studies. These lie beyond the scope of the present paper.",
            "paragraph_rank": 33,
            "section_rank": 8
        },
        {
            "text": "LUCID algorithms",
            "section_rank": 9
        },
        {
            "section": "LUCID algorithms",
            "text": "Four algorithms are currently implemented in the LUMAT card:",
            "paragraph_rank": 34,
            "section_rank": 9
        },
        {
            "section": "LUCID algorithms",
            "text": "-LUCID_Zero_OR counts the number of events per BC where at least one of the two detector sides reports no hits within one BCID, or where neither side contains any hit in one BCID. -LUCID_Zero_AND counts the number of events per BC where no hit is found within one BCID on either detector side. -LUCID_Hit_OR reports the mean number of hits per BC.",
            "paragraph_rank": 35,
            "section_rank": 9
        },
        {
            "section": "LUCID algorithms",
            "text": "In this algorithm, hits are counted for any event where there is at least one hit in any one of the 16 tubes in either detector side in one BCID. -LUCID_Hit_AND reports the mean number of hits per BC, with the additional requirement that the event contain at least one hit on each of the two detector sides in one BCID.",
            "paragraph_rank": 36,
            "section_rank": 9
        },
        {
            "section": "LUCID algorithms",
            "text": "The LUCID event-counting algorithms simply subtract the number of empty events reported by the zero-counting algorithms above from the total number of bunch crossings:",
            "paragraph_rank": 37,
            "section_rank": 9
        },
        {
            "section": "LUCID algorithms",
            "text": "-LUCID_Event_AND reports the number of events with at least one hit on each detector side (N LUCID_Event_AND = N BC \u2212 N LUCID_Zero_OR ). -LUCID_Event_OR reports the number of events for which the sum of the hits on both detector sides is at least one",
            "paragraph_rank": 38,
            "section_rank": 9
        },
        {
            "section": "LUCID algorithms",
            "text": "Converting measured hit-counting probabilities into instantaneous luminosity does not lend itself to analytic models of the type used for event counting and requires detailed Monte Carlo modeling that depends on the knowledge of both the detector response and the particle spectrum in pp collisions. This modeling introduces additional systematic uncertainties and to be used reliably requires more extensive studies that lie beyond the scope of the present paper.",
            "paragraph_rank": 39,
            "section_rank": 9
        },
        {
            "text": "MBTS algorithms",
            "section_rank": 10
        },
        {
            "section": "MBTS algorithms",
            "text": "Raw online luminosity information is supplied by the following two CTP scalers:",
            "paragraph_rank": 40,
            "section_rank": 10
        },
        {
            "section": "MBTS algorithms",
            "text": "-MBTS_Event_OR counts the number of events per BC where at least one hit above threshold is observed on either the A-side or the C-side, or both; -MBTS_Event_AND counts the number of events per BC where at least one hit above threshold is observed both on the A-and the C-side.",
            "paragraph_rank": 41,
            "section_rank": 10
        },
        {
            "text": "ZDC algorithms",
            "section_rank": 11
        },
        {
            "section": "ZDC algorithms",
            "text": "Online luminosity information is supplied by dedicated ZDC scalers that count pulses produced by constant-fraction discriminators connected to the analog sum of ZDC photomultiplier signals on each side separately:",
            "paragraph_rank": 42,
            "section_rank": 11
        },
        {
            "section": "ZDC algorithms",
            "text": "-ZDC_A reports the event rate where at least one hit above threshold is observed on the A-side, irrespective of whether a hit is simultaneously observed on the C-side. -ZDC_C reports the event rate where at least one hit above threshold is observed on the C-side, irrespective of whether a hit is simultaneously observed on the A-side. -ZDC_Event_AND reports the event rate where at least one hit above threshold is observed in coincidence on the Aand C-sides. This algorithm is still under study and is not considered further in this paper.",
            "paragraph_rank": 43,
            "section_rank": 11
        },
        {
            "section": "ZDC algorithms",
            "text": "The data described here were taken before the ZDC electronic gains and timings were fully equalized. Hence the corresponding visible cross sections for the A-and C-side differ by a few per cent.",
            "paragraph_rank": 44,
            "section_rank": 11
        },
        {
            "text": "Offline algorithms",
            "section_rank": 12
        },
        {
            "section": "Offline algorithms",
            "text": "Some luminosity algorithms require detailed information that is not easily accessible online. These algorithms use data collected with a minimum bias trigger (e.g. one of the MBTS triggers) and typically include tighter requirements to further reduce backgrounds. Because such analyses can only be performed on events that are recorded by the DAQ system, they are statistically less powerful than the online algorithms. However, since the MBTS rates per BCID are not available online, offline algorithms are important for these detectors for runs where the currents are very different from one bunch to the next. In addition, these methods use event selection criteria that are very similar to final physics analyses.",
            "paragraph_rank": 45,
            "section_rank": 12
        },
        {
            "section": "Offline algorithms",
            "text": "Verification that the luminosities obtained from the offline methods agree well with those obtained from the online techniques through the full range of relevant \u03bc provides an important cross-check of systematic uncertainties. As with the online measurements, the LB-averaged instantaneous luminosities are stored in the ATLAS conditions database.",
            "paragraph_rank": 46,
            "section_rank": 12
        },
        {
            "text": "MBTS timing algorithm",
            "section_rank": 13
        },
        {
            "section": "MBTS timing algorithm",
            "text": "The background rate for events passing the MBTS_Event_AND trigger is a factor of about 1000 below the signal. As a result, online luminosity measurements from that trigger can be reliably calculated without performing a background subtraction. However, the signal-to-background ratio is reduced when the two beams are displaced relative to each other (since the signal decreases but the beam-induced backgrounds remain constant). At the largest beam separations used during the vdM scans, the background rate approaches 10% of the signal. While these backgrounds are included in the fit model used to determine the online MBTS luminosity calibration (see Sect. 4.3), it is useful to cross-check these calibrations by reanalysing the data with a tighter offline selection. The offline time resolution of the MBTS is \u223c3 ns and the distance between the A-and C-sides corresponds to a time difference of 23 ns for particles moving at the speed of light. Imposing a requirement that the difference in time measured for signals from the two sides be less than 10 ns reduces the background rate in the MBTS_Event_AND triggered events to a negligible level (<10 \u22124 ) even at the largest beam displacements used in the scans, while maintaining good signal efficiency. This algorithm is called MBTS_Timing. In those instances where different bunches have substantially different luminosities, MBTS_Timing can be used to properly account for the pile-up dependent corrections.",
            "paragraph_rank": 47,
            "section_rank": 13
        },
        {
            "text": "Liquid argon algorithm",
            "section_rank": 14
        },
        {
            "section": "Liquid argon algorithm",
            "text": "The timing cut used in MBTS_Timing is only applicable to coincidence triggers, where hits are seen both on the A-and C-sides. It is possible to cross-check the online calibration of the single-sided MBTS_Event_OR trigger, where the signalto-background ratios are lower, by imposing timing requirements on a different detector. The LAr_Timing algorithm uses the liquid argon endcap calorimeters for this purpose. Events are required to pass the MBTS_Event_OR trigger and to have significant in-time energy deposits in both EM calorimeter endcaps. The analysis considers the energy deposits in the EMEC Inner Wheels and the first layer of the FCal, corresponding to the pseudorapidity range 2.5 < |\u03b7| < 4.9. Cells are required to have an energy 5\u03c3 above the noise level and to have E > 250 MeV in the EMEC or E > 1200 MeV in the FCal. Two cells are required to pass the selection on each of the A-and C-side. The time on the A-side (C-side) is then defined as the average time of all the cells on the A-side (Cside) that pass the above requirements. The times obtained from the A-side and C-side are then required to agree to better than \u00b15 ns (the distance between the A-and C-sides corresponds to a time difference of 30 ns for particles moving at the speed of light).",
            "paragraph_rank": 48,
            "section_rank": 14
        },
        {
            "text": "Track-based algorithms",
            "section_rank": 15
        },
        {
            "section": "Track-based algorithms",
            "text": "Luminosity measurements have also been performed offline by counting the rate of events with one or more reconstructed tracks in the MBTS_Event_OR sample. Here, rather than imposing a timing cut, the sample is selected by requiring that one or more charged particle tracks be reconstructed in the inner detector. Two variants of this analysis have been implemented that differ only in the details of the track selection.",
            "paragraph_rank": 49,
            "section_rank": 15
        },
        {
            "section": "Track-based algorithms",
            "text": "The first method, referred to here as primary-vertex event counting (PrimVtx) has larger acceptance. The track selection and vertex reconstruction requirements are identical to those used for the study of charged particle multiplicities at \u221a s = 7 TeV [15]. Here, a reconstructed primary vertex is required that is formed from at least two tracks, each with p T > 100 MeV. Furthermore, the tracks are required to fulfill the following quality requirements: transverse impact parameter |d 0 | < 4 mm with respect to the luminous centroid, errors on the transverse and longitudinal impact parameters \u03c3 (d 0 ) < 5 mm and \u03c3 (z 0 ) < 10 mm, at least 4 hits in the SCT, and at least 6 hits in Pixel and SCT.",
            "paragraph_rank": 50,
            "section_rank": 15,
            "ref_spans": [
                {
                    "type": "bibr",
                    "start": 252,
                    "text": "[15]",
                    "end": 256
                }
            ]
        },
        {
            "section": "Track-based algorithms",
            "text": "The second analysis, referred to here as charged-particle event counting (ChPart), is designed to allow the comparison of results from ALICE, ATLAS and CMS. It therefore uses fiducial and p T requirements that are accessible to all three experiments. The method counts the rate of events that have at least one track with transverse momentum p T > 0.5 GeV and pseudorapidity |\u03b7| < 0.8. The track selection and acceptance corrections are identical (with the exception of the |\u03b7| < 0.8 requirement) to those in Ref. [19]. The main criteria are an MBTS_Event_OR trigger, a reconstructed primary vertex with at least three tracks with p T > 150 MeV, and at least one track with p T > 500 MeV, |\u03b7| < 0.8 and at least 6 SCT hits and one Pixel hit. Data are corrected for the trigger efficiency, the efficiency of the vertex requirement and the tracking efficiency, all of which depend on p T and \u03b7.",
            "paragraph_rank": 51,
            "section_rank": 15,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b18",
                    "start": 514,
                    "text": "[19]",
                    "end": 518
                }
            ]
        },
        {
            "text": "Converting counting rates to absolute luminosity",
            "section_rank": 16
        },
        {
            "section": "Converting counting rates to absolute luminosity",
            "text": "The value of \u03bc vis i used to determine the bunch luminosity L i in BCID i is obtained from the raw number of counts N i and the number of bunch crossings N BC , using an algorithmdependent expression and assuming that:",
            "paragraph_rank": 52,
            "section_rank": 16
        },
        {
            "section": "Converting counting rates to absolute luminosity",
            "text": "-the number of pp-interactions occurring in any bunch crossing obeys a Poisson distribution. This assumption drives the combinatorial formalism presented in Sects. 3.4.1 and 3.4.2 below. -the efficiency to detect a single inelastic pp interaction is constant, in the sense that it does not change when several interactions occur in the same bunch crossing. This is tantamount to assuming that the efficiency \u03b5 n for detecting one event associated with n interactions occurring in the same crossing is given by",
            "paragraph_rank": 53,
            "section_rank": 16
        },
        {
            "section": "Converting counting rates to absolute luminosity",
            "text": "where \u03b5 1 is the detection efficiency corresponding to a single inelastic interaction in a bunch crossing (the same definition applies to the efficiencies \u03b5 OR , \u03b5 A , \u03b5 C and \u03b5 AND defined below). This assumption will be validated in Sect. 3.4.3.",
            "paragraph_rank": 54,
            "section_rank": 16
        },
        {
            "section": "Converting counting rates to absolute luminosity",
            "text": "The bunch luminosity is then given directly and without additional assumptions by",
            "paragraph_rank": 55,
            "section_rank": 16
        },
        {
            "section": "Converting counting rates to absolute luminosity",
            "text": "using the value of \u03c3 vis measured during beam-separation scans for the algorithm considered. However, providing a value for \u03bc \u2261 \u03bc vis /\u03b5 = \u03bc vis \u03c3 inel /\u03c3 vis requires an assumption on the as yet unmeasured total inelastic cross section at \u221a s = 7 TeV. 4 ",
            "paragraph_rank": 56,
            "section_rank": 16,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b3",
                    "start": 253,
                    "text": "4",
                    "end": 254
                }
            ]
        },
        {
            "text": "Inclusive-OR algorithms",
            "section_rank": 17
        },
        {
            "section": "Inclusive-OR algorithms",
            "text": "In the Event_OR case, the logic is straightforward. Since the Poisson probability for observing zero events in a given bunch crossing is P 0 (\u03bc vis ) = e \u2212\u03bc vis = e \u2212\u03bc\u03b5 OR , the probability of observing at least one event is",
            "paragraph_rank": 57,
            "section_rank": 17
        },
        {
            "section": "Inclusive-OR algorithms",
            "text": "Here the raw event count N OR is the number of bunch crossings, during a given time, in which at least one pp interaction satisfies the event-selection criteria of the OR algorithm under consideration, and N BC is the total number of bunch crossings during the same interval. Equation (9) reduces to the intuitive result P Event_OR (\u03bc vis ) \u2248 \u03bc vis when \u03bc vis 1. Solving for \u03bc vis in terms of the event-counting rate yields:",
            "paragraph_rank": 58,
            "section_rank": 17
        },
        {
            "text": "Coincidence algorithms",
            "section_rank": 18
        },
        {
            "section": "Coincidence algorithms",
            "text": "For the Event_AND case, the relationship between \u03bc vis and N is more complicated. Instead of depending on a single efficiency, the event-counting probability must be written in terms of \u03b5 A , \u03b5 C and \u03b5 AND , the efficiencies for observing an event with, respectively, at least one hit on the A-side, at least one hit on the C-side and at least one hit on both sides simultaneously. These efficiencies are related to the Event_OR efficiency by \u03b5 OR = \u03b5 A + \u03b5 C \u2212 \u03b5 AND . The probability P Event_AND (\u03bc) of there being at least one hit on both sides is one minus the probability P Zero_OR 0 of there being no hit on at least one side. The latter, in turn, equals the probability that there be no hit on at least side A (P 0A = e \u2212\u03bc\u03b5 A ), plus the probability that there be no hit on at least side C (P 0C = e \u2212\u03bc\u03b5 C ), minus the probability that there be no hit on either side (P 0 = e \u2212\u03bc\u03b5 OR ):",
            "paragraph_rank": 59,
            "section_rank": 18
        },
        {
            "section": "Coincidence algorithms",
            "text": "This equation cannot be inverted analytically. The most appropriate functional form depends on the values of \u03b5 A , \u03b5 C and \u03b5 AND . For cases such as LUCID_Event_AND and BCM_Event_AND, the above equation can be simplified under the assumption that \u03b5 A \u2248 \u03b5 C . The efficiencies \u03b5 AND and \u03b5 OR are defined by, respectively, \u03b5 AND \u2261 \u03c3 AND vis /\u03c3 inel and \u03b5 OR \u2261 \u03c3 OR vis /\u03c3 inel ; the average number of visible inelastic interactions per BC is computed as \u03bc vis \u2261 \u03b5 AND \u03bc. Equation 11 ",
            "paragraph_rank": 60,
            "section_rank": 18,
            "ref_spans": [
                {
                    "ref_id": "formula_11",
                    "start": 478,
                    "text": "11",
                    "end": 480
                }
            ]
        },
        {
            "section": "Coincidence algorithms",
            "text": "The value of \u03bc vis is then obtained by solving (12) numerically using the values of \u03c3 OR vis and \u03c3 AND vis extracted from beam separation scans. The validity of this technique will be quantified in Sect. 5.",
            "paragraph_rank": 61,
            "section_rank": 18
        },
        {
            "section": "Coincidence algorithms",
            "text": "If the efficiency is high and \u03b5 AND \u2248 \u03b5 A \u2248 \u03b5 C , as is the case for MBTS_Event_AND, (11) can be approximated by",
            "paragraph_rank": 62,
            "section_rank": 18
        },
        {
            "section": "Coincidence algorithms",
            "text": "The \u03bc-dependence of the probability function P Event_AND is controlled by the relative magnitudes of \u03b5 A , \u03b5 C and \u03b5 AND (or of the corresponding measured visible cross sections). This is in contrast to the Event_OR case, where the efficiency \u03b5 OR factors out of (10).",
            "paragraph_rank": 63,
            "section_rank": 18
        },
        {
            "text": "Pile-up-related instrumental effects",
            "section_rank": 19
        },
        {
            "section": "Pile-up-related instrumental effects",
            "text": "The \u03bc-dependence of the probability functions P Event_OR and P Event_AND is displayed in Fig. 2. All algorithms saturate at high \u03bc, reflecting the fact that as the pile-up increases, the probability of observing at least one event per bunch crossing approaches one. Any event-counting luminosity algorithm will therefore lose precision, and ultimately become unusable, as the LHC luminosity per bunch increases far beyond present levels. The tolerable pile-up level is detector-and algorithm-dependent: the higher the efficiency (\u03b5 OR MBTS > \u03b5 AND MBTS > \u03b5 OR LUCID > \u03b5 AND LUCID ), the earlier the onset of this saturation. The accuracy of the event-counting formalism can be verified using simulated data. Figure 2 (bottom) shows that the parameterizations of Sects. 3.4.1 and 3.4.2 deviate from the full simulation by \u00b12% at most: possible instrumental effects not accounted for by the combinatorial formalism are predicted to have negligible impact for the bunch luminosities achieved in the 2010 LHC run (0 < \u03bc < 5).",
            "paragraph_rank": 64,
            "section_rank": 19,
            "ref_spans": [
                {
                    "type": "figure",
                    "ref_id": "fig_0",
                    "start": 89,
                    "text": "Fig. 2",
                    "end": 95
                },
                {
                    "type": "figure",
                    "ref_id": "fig_0",
                    "start": 708,
                    "text": "Figure 2 (bottom)",
                    "end": 725
                }
            ]
        },
        {
            "section": "Pile-up-related instrumental effects",
            "text": "It should be stressed, however, that the agreement between the Poisson formalism and the full simulation depends critically on the validity of the assumption, summarized by (7), that the efficiency for detecting an inelastic pp interaction is independent of the number of interactions that occur in each crossing. This requires, for instance, that the threshold for registering a hit in a phototube (nominally 15 photoelectrons for LUCID) be low enough compared to the average single-particle response. This condition is satisfied by the simulation shown in Fig. 2. Repeating this simulation with the LUCID threshold raised to 50 photoelectrons yields systematic discrepancies as large as 7% between the computed and simulated probability functions for the LU-CID Event_AND algorithm. When the threshold is too high, a particle from a single pp interaction occasionally fails to fire the discriminator. However, if two such particles from different pp interactions in the same bunch crossing traverse the same tube, they may produce enough light to register a hit. This effect is called migration.",
            "paragraph_rank": 65,
            "section_rank": 19,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b6",
                    "start": 173,
                    "text": "(7)",
                    "end": 176
                },
                {
                    "type": "figure",
                    "ref_id": "fig_0",
                    "start": 558,
                    "text": "Fig. 2",
                    "end": 564
                }
            ]
        },
        {
            "text": "Absolute calibration using beam-separation scans",
            "section_rank": 20
        },
        {
            "section": "Absolute calibration using beam-separation scans",
            "text": "The primary calibration of all luminosity algorithms is derived from data collected during van der Meer scans. The principle (Sect. 4.1) is to measure simultaneously the collision rate at zero beam separation and the corresponding absolute luminosity inferred from the charge of the colliding proton bunches and from the horizontal and vertical convolved beam sizes [13]. Three sets of beam scans have been carried out in ATLAS, as detailed in Sect. 4.2. These were performed in both the horizontal and the vertical directions in order to reconstruct the transverse convolved beam profile. During each scan, the collision rates measured by the luminosity detectors were recorded while the beams were moved stepwise with respect to each other in the transverse plane.",
            "paragraph_rank": 66,
            "section_rank": 20,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b12",
                    "start": 366,
                    "text": "[13]",
                    "end": 370
                }
            ]
        },
        {
            "text": "Absolute luminosity from beam parameters",
            "section_rank": 21
        },
        {
            "section": "Absolute luminosity from beam parameters",
            "text": "In terms of colliding-beam parameters, the luminosity L is defined (for beams that collide with zero crossing angle) as",
            "paragraph_rank": 67,
            "section_rank": 21
        },
        {
            "section": "Absolute luminosity from beam parameters",
            "text": "where n b is the number of colliding bunches, f r is the machine revolution frequency (11245.5 Hz for LHC), n 1 2is the number of particles per bunch in beam 1 (2) and \u03c1 1(2) (x, y) is the normalized particle density in the transverse (x-y) plane of beam 1 (2) at the IP. Under the general assumption that the particle densities can be factorized into independent horizontal and vertical components, (\u03c1(x, y) = \u03c1(x)\u03c1(y)), (14) can be rewritten as",
            "paragraph_rank": 68,
            "section_rank": 21,
            "ref_spans": [
                {
                    "start": 112,
                    "text": "2",
                    "end": 113
                },
                {
                    "type": "bibr",
                    "start": 422,
                    "text": "(14)",
                    "end": 426
                }
            ]
        },
        {
            "section": "Absolute luminosity from beam parameters",
            "text": "where",
            "paragraph_rank": 69,
            "section_rank": 21
        },
        {
            "section": "Absolute luminosity from beam parameters",
            "text": "is the beam overlap integral in the x direction (with an analogous definition in the y direction). In the method proposed by van der Meer [14] the overlap integral (for example in the x direction) can be calculated as:",
            "paragraph_rank": 70,
            "section_rank": 21,
            "ref_spans": [
                {
                    "type": "bibr",
                    "start": 138,
                    "text": "[14]",
                    "end": 142
                }
            ]
        },
        {
            "section": "Absolute luminosity from beam parameters",
            "text": "where R x (\u03b4) is the luminosity (or equivalently \u03bc vis )-at this stage in arbitrary units-measured during a horizontal scan at the time the two beams are separated by the distance \u03b4 and \u03b4 = 0 represents the case of zero beam separation. \u03a3 x is defined by the equation:",
            "paragraph_rank": 71,
            "section_rank": 21
        },
        {
            "section": "Absolute luminosity from beam parameters",
            "text": "In the case where the luminosity curve R x (\u03b4) is Gaussian, \u03a3 x coincides with the standard deviation of that distribution. By using the last two equations, (15) can be rewritten as",
            "paragraph_rank": 72,
            "section_rank": 21
        },
        {
            "section": "Absolute luminosity from beam parameters",
            "text": "x \u03a3 y (18) which is a general formula to extract luminosity from machine parameters by performing a beam separation scan. Equation 18is quite general; \u03a3 x and \u03a3 y only depend on the area under the luminosity curve.",
            "paragraph_rank": 73,
            "section_rank": 21,
            "ref_spans": [
                {
                    "type": "bibr",
                    "start": 6,
                    "text": "(18)",
                    "end": 10
                },
                {
                    "start": 131,
                    "text": "18",
                    "end": 133
                }
            ]
        },
        {
            "text": "Luminosity-scan data sets",
            "section_rank": 22
        },
        {
            "section": "Luminosity-scan data sets",
            "text": "Three van der Meer scans have been performed at the AT-LAS interaction point ( Table 2). The procedure [12,20] ran as follows. After centering the beams on each other at the IP in both the horizontal and the vertical plane using miniscans, a full luminosity-calibration scan was carried out in the horizontal plane, spanning a range of \u00b16\u03c3 b in horizontal beam-separation (where \u03c3 b is the nominal transverse size of either beam at the IP). A full luminosity-calibration scan was then carried out in the vertical plane, again spanning a range of \u00b16\u03c3 b in relative beam separation. The mini-scans used to first center the beams on each other in the transverse plane were done by activating closed  5 around the IP that vary the IP positions of both beams by \u00b11\u03c3 b in opposite directions, either horizontally or vertically. The relative positions of the two beams were then adjusted, in each plane, to achieve (at that time) optimum transverse overlap. The full horizontal and vertical scans followed an identical procedure, where the same orbit bumps were used to displace the two beams in opposite directions by \u00b13\u03c3 b , resulting in a total variation of \u00b16\u03c3 b in relative displacement at the IP. In Scan I, the horizontal scan started at zero nominal separation, moved to the maximum separation in the negative direction, stepped back to zero and on to the maximum positive separation, and finally returned to the original settings of the closed-orbit bumps (zero nominal separation). The same procedure was followed for the vertical scan. In Scans II and III, after collision optimization with the transverse mini-scans, a full horizontal scan was taken from negative to positive nominal separation, followed by a hysteresis cycle where the horizontal nominal separation was run to \u22126\u03c3 b , then 0 then +6\u03c3 b , and finally followed by a full horizontal scan in the opposite direction to check for potential hysteresis effects. The same procedure was then repeated in the vertical direction.",
            "paragraph_rank": 74,
            "section_rank": 22,
            "ref_spans": [
                {
                    "type": "table",
                    "ref_id": "tab_2",
                    "start": 79,
                    "text": "Table 2",
                    "end": 86
                },
                {
                    "type": "bibr",
                    "ref_id": "b11",
                    "start": 103,
                    "text": "[12,",
                    "end": 107
                },
                {
                    "type": "bibr",
                    "ref_id": "b19",
                    "start": 107,
                    "text": "20]",
                    "end": 110
                },
                {
                    "type": "bibr",
                    "start": 697,
                    "text": "5",
                    "end": 698
                }
            ]
        },
        {
            "section": "Luminosity-scan data sets",
            "text": "For each scan, at each of 27 steps in relative displacement, the beams were left in a quiescent state for \u223c30 seconds. During this time the (relative) luminosities measured by all active luminosity monitors were recorded as a func- 5 A closed orbit bump is a local distortion of the beam orbit that is implemented using pairs of steering dipoles located on either side of the affected region. In this particular case, these bumps are tuned to translate either beam parallel to itself at the IP, in either the horizontal or the vertical direction. tion of time in a dedicated online-data stream, together with the value of the nominal separation, the beam currents and other relevant accelerator parameters transmitted to ATLAS by the accelerator control system. In addition, the full data acquisition system was operational throughout the scan, using the standard trigger menu, and triggered events were recorded as part of the normal data collection.",
            "paragraph_rank": 75,
            "section_rank": 22,
            "ref_spans": [
                {
                    "type": "bibr",
                    "start": 232,
                    "text": "5",
                    "end": 233
                }
            ]
        },
        {
            "text": "Parametrization and analysis of the beam scan data",
            "section_rank": 23
        },
        {
            "section": "Parametrization and analysis of the beam scan data",
            "text": "Data from all three scans have been analyzed both from the dedicated online-data stream and from the standard ATLAS data stream. Analyses using the standard data stream suffer from reduced statistical precision relative to the dedicated stream, but allow for important cross-checks both of the background rates and of the size and position of the luminous region. In addition, because this stream contains full events, these data can be used to measure the visible cross section corresponding to standard analysis selections that require, for example, timing cuts in the MBTS or the liquid argon Calorimeter or the presence of a reconstructed primary vertex. Measurements performed using these two streams provide a consistent interpretation of the data within the relevant statistical and systematic uncertainties.",
            "paragraph_rank": 76,
            "section_rank": 23
        },
        {
            "section": "Parametrization and analysis of the beam scan data",
            "text": "In all cases, the analyses fit the relative variation of the bunch luminosity as a function of the beam separation to extract \u03a3 x and \u03a3 y (17). These results are then combined with the measured bunch currents to determine the absolute luminosity using (18). Although the pile-up effects remained relatively weak during these scans, the raw rates (P Event_OR , P Event_AND , . . .) are converted 6 into a mean number of interactions per crossing \u03bc vis as described in Sect. 3.4. In addition, to remove sensitivity to the slow decay of the beam currents over the duration of the scan, the data are analyzed as specific rates, obtained by dividing the measured average interaction rate per BC by the product of the bunch currents measured at that scan point:",
            "paragraph_rank": 77,
            "section_rank": 23,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b16",
                    "start": 138,
                    "text": "(17)",
                    "end": 142
                },
                {
                    "type": "bibr",
                    "start": 252,
                    "text": "(18)",
                    "end": 256
                },
                {
                    "type": "bibr",
                    "ref_id": "b5",
                    "start": 395,
                    "text": "6",
                    "end": 396
                }
            ]
        },
        {
            "section": "Parametrization and analysis of the beam scan data",
            "text": "Here (n 1 n 2 ) meas is the product of the numbers of protons in the two colliding bunches during the measurement, (n 1 n 2 ) MAX is its maximum value during the scans, and R meas is the value of \u03bc vis at the current scan point. Beam currents are measured using two complementary LHC systems [21]. The fast bunch-current transformers (FBCT) are AC-coupled, high-bandwidth devices which use gated electronics to perform continuous measurements of individual bunch charges for each beam. The Direct-Current Current Transformers (DCCT) measure the total circulating intensity in each of the two beams irrespective of their underlying time structure. The DCCT's have intrinsically better accuracy, but require averaging over hundreds of seconds to achieve the needed precision. The relative (bunchto-bunch) currents are based on the FBCT measurement. The absolute scale of the bunch intensities n 1 and n 2 is determined by rescaling the total circulating charge measured by the FBCTs to the more accurate DCCT measurements. Detailed discussions of the performance and calibration of these systems are presented in Ref. [22].",
            "paragraph_rank": 78,
            "section_rank": 23,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b20",
                    "start": 292,
                    "text": "[21]",
                    "end": 296
                },
                {
                    "type": "bibr",
                    "start": 1116,
                    "text": "[22]",
                    "end": 1120
                }
            ]
        },
        {
            "section": "Parametrization and analysis of the beam scan data",
            "text": "Fits to the relative luminosity require a choice of parametrization of the shape of the scan curve. For all detectors and algorithms, fits using a single Gaussian or a single Gaussian with a flat background yield unacceptable \u03c7 2 distributions. In all cases, fits to a double Gaussian (with a common mean) plus a flat background result in a \u03c7 2 per degree of freedom close to one. In general, the background rates are consistent with zero for algorithms requiring a coincidence between sides, while small but statistically significant backgrounds are observed for algorithms requiring only a single side. These backgrounds are reduced to less than 0.3% of the luminosity at zero beam separation by using data from the paired bunches only. Offline analyses that require timing or a primary vertex, in addition to being restricted to paired bunches, have very low background. The residual background is subtracted using the rate measured in unpaired bunches; no background term is therefore needed in the fit function for the offline case. Examples of such fits are shown in Fig. 3.",
            "paragraph_rank": 79,
            "section_rank": 23,
            "ref_spans": [
                {
                    "type": "figure",
                    "ref_id": "fig_1",
                    "start": 1073,
                    "text": "Fig. 3",
                    "end": 1079
                }
            ]
        },
        {
            "section": "Parametrization and analysis of the beam scan data",
            "text": "For these fits the specific rate is described by a double Gaussian:",
            "paragraph_rank": 80,
            "section_rank": 23
        },
        {
            "section": "Parametrization and analysis of the beam scan data",
            "text": "Here \u03c3 i and \u03c3 j are the widths of first and second Gaussians respectively, f i is the fraction of the rate in the first Gaussian and x 0 is introduced to allow for the possibility that the beams are not perfectly centered at the time of the scan. The value of \u03a3 x in (18) is calculated as 1",
            "paragraph_rank": 81,
            "section_rank": 23
        },
        {
            "text": "Fit results",
            "section_rank": 24
        },
        {
            "section": "Fit results",
            "text": "Summaries of the relevant fit parameters for the three scans are presented in Tables 7 through 9 in the Appendix. Because the emittance during Scan I was different from that during Scans II and III, the values of \u03a3 x and \u03a3 y are not expected to be the same for the first and the later scans. Furthermore, because the beam currents were lower in Scan I, the peak luminosities for this scan are lower than for the later scans. These tables, as well as Fig. 4, show that the mean position and \u03a3 for a given scan are consistent within statistical uncertainties amongst all algorithms. These data also indicate several potential sources of systematic uncertainty. First, the fitted position of the peak luminosity deviates from zero by as much as 7 \u03bcm, indicating that the beams may not have been properly centered before the start of the scan. Second, in scans II and III, the peak luminosities for the horizontal and vertical scans, as measured with a single algorithm, show a systematic difference of as much as 5% (with a lower rate observed in the vertical scan for all algorithms). This systematic dependence may indicate a level of irreproducibility in the scan setup. The effect of these systematic uncertainties on the luminosity calibration is discussed in Sect. 4.5. Figure 5 (and Table 10 in the Appendix) report the specific luminosity normalized to units of 10 11 protons per bunch",
            "paragraph_rank": 82,
            "section_rank": 24,
            "ref_spans": [
                {
                    "type": "figure",
                    "ref_id": "fig_2",
                    "start": 450,
                    "text": "Fig. 4",
                    "end": 456
                },
                {
                    "type": "figure",
                    "ref_id": "fig_3",
                    "start": 1273,
                    "text": "Figure 5",
                    "end": 1281
                },
                {
                    "type": "table",
                    "ref_id": "tab_0",
                    "start": 1287,
                    "text": "Table 10",
                    "end": 1295
                }
            ]
        },
        {
            "section": "Fit results",
            "text": "The differences between algorithms within each of Scans II and III is consistent within statistics, and the average specific luminosities measured in these two scans agree to better than 0.3%. Calibration of the absolute luminosity from the beam scans uses the following expression for \u03c3 vis : where R MAX and L MAX are, respectively, the value of R sp and the absolute luminosity (inferred from the measured machine parameters) when the beams collide exactly head-on. Since there are two independent measurements, one each for the x and y directions, and each has the same statistical significance, the average of the two measurements is consid-ered as the best estimate of R MAX :",
            "paragraph_rank": 83,
            "section_rank": 24
        },
        {
            "section": "Fit results",
            "text": "The values of \u03c3 vis for each method and each scan are reported in Table 10 in the Appendix. While the results of the  second and third luminosity scans are compatible within statistical uncertainties, those of the first luminosity scan are lower by 2.7% to 4.8% for all online algorithms, but are consistent for the offline track-based algorithms. These differences again indicate possible systematic variations occurring between machine fills and are most likely to be caused by variations in the beam current calibration (see Sect. 4.5).",
            "paragraph_rank": 84,
            "section_rank": 24,
            "ref_spans": [
                {
                    "type": "table",
                    "ref_id": "tab_0",
                    "start": 66,
                    "text": "Table 10",
                    "end": 74
                }
            ]
        },
        {
            "text": "Systematic uncertainties",
            "section_rank": 25
        },
        {
            "section": "Systematic uncertainties",
            "text": "Systematic uncertainties affecting the luminosity and visible cross section measurements arise from the following effects.",
            "paragraph_rank": 85,
            "section_rank": 25
        },
        {
            "text": "Beam intensities",
            "section_rank": 26
        },
        {
            "section": "Beam intensities",
            "text": "A systematic error in the measurement of the absolute bunch charge translates directly into an uncertainty on the luminosity calibration. The accuracy of the bunch intensity measurement depends on that of the DCCT calibration. While laboratory measurements indicate an rms absolute scale uncertainty of better than 1.2%, the DCCT suffers from slow baseline drifts that are beam-, time-and temperature-dependent. These baseline offsets can only be determined with no beam in the LHC. For the fills under consideration, the DCCT baseline was measured before injection, and then again after dumping the beam. The DCCT-baseline determination is subject to magnetic and electronic drifts that translate into an rms uncertainty on the total circulating charge of \u223c1.15 \u00d7 10 9 protons. Conservatively combining the uncertainty on the absolute scale and on the baseline subtraction linearly yields a fractional uncertainty on the total charge n 1(2) in beam 1 (2) of",
            "paragraph_rank": 86,
            "section_rank": 26
        },
        {
            "section": "Beam intensities",
            "text": "Treating the current-scale uncertainty as fully correlated between the two beams results in a total systematic error of \u00b114% on the product of bunch currents for Scan I, and of \u00b18% for each of Scans II and III. Conservatively taking the arithmetic average of the three values yields an overall \u00b110% systematic uncertainty for the running conditions summarized in Table 3. Because the baseline correction dominates the overall bunch-charge uncertainty, and because it drifts on the time scale of a few hours, these uncertainties are largely uncorrelated between the first (scan I) and the second (scans II + III) luminosity-calibration sessions.",
            "paragraph_rank": 87,
            "section_rank": 26,
            "ref_spans": [
                {
                    "type": "table",
                    "ref_id": "tab_3",
                    "start": 363,
                    "text": "Table 3",
                    "end": 370
                }
            ]
        },
        {
            "text": "Length-Scale Calibration",
            "section_rank": 27
        },
        {
            "section": "Length-Scale Calibration",
            "text": "Fits to the beam size depend on knowledge of the relative displacement between the beams at each scan step. Thus, any miscalibration of the beam separation lengthscale will result in a mismeasurement of the luminosity. The desired nominal beam separation during beam scans determines the magnet settings of the closed orbit bumps that generate the beam separation. The only accelerator instrumentation available for calibrating the length-scale of the beam separation is the beam position monitor system. Unfortunately, the short-term stability and reliability of this system are not adequate to perform such a calibration. In contrast, the vertex resolution of the AT-LAS Inner Detector provides a stable and precise method of calibration. These calibrations were done in dedicated scans where both beams were moved in the same direction first by +100 \u03bcm and then by \u2212100 \u03bcm from the nominal beam position, first in the horizontal and then in the vertical direction. The luminous beam centroid was determined using reconstructed primary vertices. In addition, the primary vertex event rate was monitored to ensure that the two beams remained centered with respect to each other. The calibration constants derived for the length-scale were (1.001 \u00b1 0.003) and (1.001 \u00b1 0.004) in the horizontal and vertical directions respectively, indicating that the scale associated with the magnet settings and that obtained from the ATLAS Inner Detector agree to better than 0.5%. The dominant source of uncertainty is the precision with which the two beams could be kept transversely aligned during the length-scale calibration scans. In addition, these scans consisted of only three points and extended to only \u00b1100 \u03bcm; therefore these data do not allow for studies of non-linearities, nor for checks of the calibration at the larger beam displacements used during the luminosity-calibration scans. Finally, if the transverse widths of the two beams happened to be significantly different, the measured displacements of the luminous centroid at each scan point would not exactly reflect the average displacement of the two beams. The combination of these effects results in an estimated systematic uncertainty of 2% on the length-scale calibration, in spite of the high precision of the calibration-scan data.",
            "paragraph_rank": 88,
            "section_rank": 27
        },
        {
            "text": "Imperfect Beam Centering",
            "section_rank": 28
        },
        {
            "section": "Imperfect Beam Centering",
            "text": "If the beams are slightly offset with respect to each other in the scan direction, there is no impact on the results of the luminosity scan. However, a deviation from zero separation in the transverse direction orthogonal to that of the scan reduces the rate observed for all the data points of that scan. The systematic uncertainty associated with imperfect beam centering has been estimated by considering the maximum deviation of the peak position (measured in terms of the nominal beam separation) from the nominal null separation that was calibrated through the re-alignment of the beams at the beginning of that scan. This deviation is translated into an expected decrease in rate and therefore in a systematic uncertainty affecting the measurement of the visible cross section. A systematic uncertainty of 2% is assigned.",
            "paragraph_rank": 89,
            "section_rank": 28
        },
        {
            "text": "Transverse Emittance Growth and Other Sources of Non-reproducibility",
            "section_rank": 29
        },
        {
            "section": "Transverse Emittance Growth and Other Sources of Non-reproducibility",
            "text": "Wire-scanner measurements of the transverse emittances of the LHC beams were performed at regular intervals during the luminosity-scan sessions, yielding measured emittance degradations of roughly 1% to 3% per beam and per plane between the first and the last scan at the ATLAS IP [23]. This emittance growth causes a progressive increase of the transverse beam sizes (and therefore of \u03a3 x and \u03a3 y ), leading to a \u223c2% degradation of the specific luminosity. This luminosity degradation, in turn, should be reflected in a variation over time of the specific rates R MAX",
            "paragraph_rank": 90,
            "section_rank": 29,
            "ref_spans": [
                {
                    "type": "bibr",
                    "start": 281,
                    "text": "[23]",
                    "end": 285
                }
            ]
        },
        {
            "section": "Transverse Emittance Growth and Other Sources of Non-reproducibility",
            "text": "x and R MAX y (24). A first potential bias arises because if the time dependence of \u03a3 x and \u03a3 y during a scan is not taken into account, the emittance growth may effectively distort the luminosity-scan curve. Next, and because the horizontal and vertical scans were separated in time, uncorrected emittance growth may induce inconsistencies in computing the luminosity from accelerator parameters using (23). The emittance growth was estimated independently from the wire-scanner data, and by a technique that relies on the relationship, for Gaussian beams, between \u03a3, the single-beam sizes \u03c3 1 and \u03c3 2 and the transverse luminous size \u03c3 L (which is measured using the spatial distribution of primary vertices) [24]:",
            "paragraph_rank": 91,
            "section_rank": 29,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b23",
                    "start": 14,
                    "text": "(24)",
                    "end": 18
                },
                {
                    "type": "bibr",
                    "start": 403,
                    "text": "(23)",
                    "end": 407
                },
                {
                    "type": "bibr",
                    "ref_id": "b23",
                    "start": 711,
                    "text": "[24]",
                    "end": 715
                }
            ]
        },
        {
            "section": "Transverse Emittance Growth and Other Sources of Non-reproducibility",
            "text": "Here the emittance growth is taken from the measured evolution of the transverse luminous size during the fill. The variations in both \u03a3 and R MAX (which should in principle cancel each other when calculating the visible cross-section) were then predicted from the two emittance-growth estimates, and compared to the luminosity-scan results. While the predicted variation of \u03a3 between consecutive scans is very small (0.3-0.8 \u03bcm) and well reproduced by the data, the time evolution of R MAX displays irregular deviations from the wire-scanner prediction of up to 3%, suggesting that at least one additional source of non-reproducibility is present. Altogether, these estimates suggest that a \u00b13% systematic uncertainty on the luminosity calibration be assigned to emittance growth and unidentified causes of non-reproducibility.",
            "paragraph_rank": 92,
            "section_rank": 29
        },
        {
            "text": "\u03bc-Dependence of the Counting Rate",
            "section_rank": 30
        },
        {
            "section": "\u03bc-Dependence of the Counting Rate",
            "text": "All measurements have been corrected for \u03bc dependent non-linearities. Systematic uncertainties on the predicted counting rate as a function of \u03bc have been studied using Monte Carlo simulations, where the efficiency (or equivalently \u03c3 vis ) have been varied. For \u03bc < 2 the uncertainty is estimated to be <2%, as illustrated in Fig. 2.",
            "paragraph_rank": 93,
            "section_rank": 30,
            "ref_spans": [
                {
                    "type": "figure",
                    "ref_id": "fig_0",
                    "start": 326,
                    "text": "Fig. 2",
                    "end": 332
                }
            ]
        },
        {
            "text": "Choice of Fit Model",
            "section_rank": 31
        },
        {
            "section": "Choice of Fit Model",
            "text": "For all methods, fits of the scan data to the default function (double Gaussian with common mean plus constant background for the online algorithms and double Gaussian for the background-free offline algorithms) have \u03c7 2 per degree of freedom values close to 1.0, indicating that the fits are good. The systematic uncertainty due to this choice of fit function has been estimated by refitting the offline data using a cubic spline as an alternative model. The value of \u03c3 vis changes by approximately 1%.",
            "paragraph_rank": 94,
            "section_rank": 31
        },
        {
            "text": "Transverse coupling at the IP",
            "section_rank": 32
        },
        {
            "section": "Transverse coupling at the IP",
            "text": "The scan formalism described in Sect. 4.1 explicitly supposes that the horizontal and vertical charge-density functions are uncorrelated at the IP. The impact of linear transverse coupling on the validity of this assumption has been studied in detail in Ref. [23]. This analysis shows that (16)- (18) remain fully valid if at the collision point, either at least one of the beams is round, or neither beam is tilted in the x-y plane, or the beams have equal tilts. In the case of unequal horizontal and vertical emittances and/or \u03b2-functions, the maximum error due to a residual tilt of the two beams can be computed using LHC lattice functions measured by resonant excitation and emittance ratios extracted from wire-scanner measurements. The resulting error on the absolute luminosity computed using (18) is found to be negligible (<0.25%).",
            "paragraph_rank": 95,
            "section_rank": 32,
            "ref_spans": [
                {
                    "type": "bibr",
                    "start": 259,
                    "text": "[23]",
                    "end": 263
                },
                {
                    "type": "bibr",
                    "start": 296,
                    "text": "(18)",
                    "end": 300
                }
            ]
        },
        {
            "section": "Transverse coupling at the IP",
            "text": "A summary of the systematic uncertainties is presented in Table 3. The overall uncertainty of 11% is dominated by the measurement of the beam intensities. At least some portion of this uncertainty is common to interactions points 1 (AT-LAS) and 5 (CMS); the size of this correlated uncertainty remains under study. It is possible to test the consistency of the vdM calibrations by comparing the luminosities obtained using different luminosity detectors and/or algorithms. Figure 6 shows the instantaneous luminosities obtained by various algorithms for Run 162882 7 , each normalized using the calibration extracted from its vdM scan data. The absolute luminosities agree to better than 2%; the relative luminosities track each other over time to within the statistical fluctuations. Over most of the 2010 pp run, LUCID_Event_OR was chosen as the preferred offline algorithm because its pile-up correction was well-understood, its statistical power was adequate and backgrounds for this algorithm were low.",
            "paragraph_rank": 96,
            "section_rank": 32,
            "ref_spans": [
                {
                    "type": "table",
                    "ref_id": "tab_3",
                    "start": 58,
                    "text": "Table 3",
                    "end": 65
                },
                {
                    "type": "figure",
                    "ref_id": "fig_4",
                    "start": 473,
                    "text": "Figure 6",
                    "end": 481
                },
                {
                    "type": "bibr",
                    "ref_id": "b6",
                    "start": 565,
                    "text": "7",
                    "end": 566
                }
            ]
        },
        {
            "section": "Transverse coupling at the IP",
            "text": "Comparing the residual \u03bc-dependence (if any) of the measured luminosity across multiple detectors and algorithms probes the consistency of the pile-up correction procedures described in Sect. 3.4. Figure 7 shows, for some of the LUCID and MBTS algorithms, the raw counting rate as a function of the average number of inelastic interactions per BC measured by LUCID_Event_OR using the prescription of Sect. 3.4.1. Non-linearities are apparent (as expected) for the LUCID_Event_AND, LUCID_Event_OR and MBTS_Event_AND algorithms. If the parametrizations of Sect. 3.4 are correct, however, then the ratio of the luminosities determined using the different algorithms should be independent of \u03bc. Figure 8 shows that the values of \u03bc obtained with the LUCID_Event_AND and MBTS_Event_AND algorithms remain within \u00b11% of that measured using the LU-CID_Event_OR algorithm over the range 0 < \u03bc < 2.5. Comparisons of the LUCID_Event_OR and LUCID_Event_AND algorithms demonstrate agreement up to \u03bc = 5, the highest  ",
            "paragraph_rank": 97,
            "section_rank": 32,
            "ref_spans": [
                {
                    "type": "figure",
                    "start": 197,
                    "text": "Figure 7",
                    "end": 205
                },
                {
                    "type": "figure",
                    "ref_id": "fig_6",
                    "start": 691,
                    "text": "Figure 8",
                    "end": 699
                }
            ]
        },
        {
            "text": "Comparison with Monte Carlo generators",
            "section_rank": 33
        },
        {
            "section": "Comparison with Monte Carlo generators",
            "text": "Because the vdM method does not require knowledge of the inelastic cross section nor of the detector acceptance, the values of \u03c3 vis obtained from the beam scans can be used to test the accuracy of the predictions of Monte Carlo event generators. Such predictions suffer from several theoretical uncertainties. First, because the pp inelastic cross section has not been measured at 7 TeV, the generators obtain \u03c3 inel by extrapolating from lower energy. Results of this extrapolation depend on the functional form used. The PYTHIA and PHOJET generators, for example, predict values for \u03c3 inel that differ by 6.6%. Second, the generators must separately model the non-diffractive (ND), single-diffractive (SD) and double-diffractive (DD) components of the cross section. There exists no unique prescription for classifying events as diffractive or non-diffractive and no calculation of the cross sections from first principles. Typical uncertainties associated with such classifications are illustrated in Table 4. The fraction of \u03c3 inel corresponding to ND events is 68% in PYTHIA and 81% in PHOJET, while the DD fractions are 13% and 5% respectively. Finally, there are significant uncertainties on the modeling of the predicted multiplicity-, p T -and \u03b7-distributions for particles produced in soft pp interactions, particularly for the poorly constrained diffractive components. Differences in these distributions will affect the efficiencies for events to pass the selection criteria of a specific luminosity algorithm.",
            "paragraph_rank": 98,
            "section_rank": 33,
            "ref_spans": [
                {
                    "type": "table",
                    "ref_id": "tab_4",
                    "start": 1005,
                    "text": "Table 4",
                    "end": 1012
                }
            ]
        },
        {
            "section": "Comparison with Monte Carlo generators",
            "text": "Within the framework of Monte Carlo generators, \u03c3 vis is calculated using the expression",
            "paragraph_rank": 99,
            "section_rank": 33
        },
        {
            "section": "Comparison with Monte Carlo generators",
            "text": "where \u03b5 process are the efficiencies and \u03c3 process the cross sections for the individual inelastic processes (ND, SD and DD). Table 5 shows the predicted efficiencies for observing ND, SD and DD events using either PYTHIA (with the default ATLAS MC09 tune [25]) or PHOJET, for some of the algorithms described in Sect. 3. In general, the PHO-JET predictions are about 15% to 20% higher than those obtained with PYTHIA. One exception is LUCID_Event_AND which is less sensitive to the diffractive processes: here the two generators agree to within 5% overall. Additional systematic uncertainties on these predictions, associated with the modeling of the detector response in the simulation, are algorithm-and trigger-dependent and vary from 2.2% for MBTS_Event_OR to 6% for LUCID_Event_AND.",
            "paragraph_rank": 100,
            "section_rank": 33,
            "ref_spans": [
                {
                    "type": "table",
                    "start": 126,
                    "text": "Table 5",
                    "end": 133
                }
            ]
        },
        {
            "section": "Comparison with Monte Carlo generators",
            "text": "As noted in Sect. 4.4, there is a systematic difference between the values of \u03c3 vis obtained from the first scan and those based on the second and third scans. In reporting our best estimate of the measured visible cross sections, we chose to average the results of the first scan with the average of the second and third scans. Comparisons of the vdM scan measurements with the Monte Carlo predictions are presented in Table 6 and Fig. 9. For a given event generator, the comparisons exhibit an RMS spread of 4 to 5%; on the average, the PYTHIA (PHOJET) predictions are 15% (33%) higher than the data. Given the 11% systematic uncertainty on the vdM calibration, which is correlated across all algorithms, PYTHIA agrees with the data at the level of 1.5\u03c3 , while PHOJET and the data deviate at the 3\u03c3 level.  Measurements of the LHC luminosity have been performed by ATLAS in proton-proton collisions at \u221a s = 7 TeV using multiple detectors and algorithms. The absolute luminosity calibrations obtained using beam-separation scans suffer from a \u00b111% systematic uncertainty, that is dominated by the uncertainty in the bunch intensities and is therefore highly correlated across all methods. For a given bunch luminosity, i.e. for a fixed value of \u03bc (the average number of inelastic pp interactions per crossing), the absolute luminosities obtained using different detectors and algorithms agree to within \u00b12%. In addition, the luminosities from these methods track each other within better than 2% over the range 0 < \u03bc < 2.5. The visible cross sections obtained from the beam scan calibrations also have a systematic uncertainty of 11% and are lower than those predicted by PYTHIA (PHOJET) by about 15% (33%). missioning and operation of the LHC during this initial high-energy data-taking period as well as the support staff from our institutions without whom ATLAS could not be operated efficiently. We would like, in addition, to extend special thanks to our LHC colleagues H. Burkhardt, M. Ferro-Luzzi, S.M. White, as well as to the LHC beam-instrumentation team, for their crucial contributions to the absolute-luminosity calibration reported in this paper.",
            "paragraph_rank": 101,
            "section_rank": 33,
            "ref_spans": [
                {
                    "type": "table",
                    "start": 420,
                    "text": "Table 6",
                    "end": 427
                },
                {
                    "type": "figure",
                    "ref_id": "fig_7",
                    "start": 432,
                    "text": "Fig. 9",
                    "end": 438
                }
            ]
        },
        {
            "section": "Comparison with Monte Carlo generators",
            "text": "We acknowledge the support of ANPCyT, Argentina; YerPhI, Armenia; ARC, Australia; BMWF, Austria; ANAS, Azerbaijan; SSTC, Belarus; CNPq and FAPESP, Brazil; NSERC, NRC and CFI, Canada The crucial computing support from all WLCG partners is acknowledged gratefully, in particular from CERN and the ATLAS Tier-1 facilities at TRIUMF (Canada), NDGF (Denmark, Norway, Sweden), CC-IN2P3 (France), KIT/GridKA (Germany), INFN-CNAF (Italy), NL-T1 (Netherlands), PIC (Spain), ASGC (Taiwan), RAL (UK) and BNL (USA) and in the Tier-2 facilities worldwide.",
            "paragraph_rank": 102,
            "section_rank": 33
        },
        {
            "section": "Comparison with Monte Carlo generators",
            "text": "Open Access This article is distributed under the terms of the Creative Commons Attribution Noncommercial License which permits any noncommercial use, distribution, and reproduction in any medium, provided the original author(s) and source are credited.",
            "paragraph_rank": 103,
            "section_rank": 33
        },
        {
            "text": "Table 7",
            "section_rank": 34
        },
        {
            "section": "Table 7",
            "text": "Summary of the relevant fit parameters for the Beam Scan I. For offline algorithms, the rates have been corrected for trigger prescales. Because the rates in the BCM were low, the value of \u03a3 used for the BCM was fixed to that obtained from the LUCID_Event_OR.",
            "paragraph_rank": 104,
            "section_rank": 34
        },
        {
            "section": "Table 7",
            "text": "No results are presented for the ZDC, since the constant fraction discriminators used for the ZDC measurements were installed later in the run  ",
            "paragraph_rank": 105,
            "section_rank": 34
        },
        {
            "text": "Fig. 2",
            "section_rank": 35
        },
        {
            "section": "Fig. 2",
            "text": "Fig. 2 Fraction of bunch crossings containing a detected event for LU-CID and MBTS algorithms as a function of \u03bc, the true average number of inelastic pp interactions per BC. The plotted points are the result of a Monte Carlo study performed using the PYTHIA event generator together with a GEANT4 simulation of the ATLAS detector response. The curves reflect the combinatorial formalism of Sects. 3.4.1 and 3.4.2, using as input only the visible cross sections extracted from that same simulation. The bottom inset shows the difference between the full simulation and the parameterization",
            "paragraph_rank": 106,
            "section_rank": 35
        },
        {
            "text": "Fig. 3",
            "section_rank": 36
        },
        {
            "section": "Fig. 3",
            "text": "Fig. 3Results of fits to the second luminosity scan in the x (left) and y (right) direction for the a LUCID_Event_OR, b MBTS_Timing, and c ChPart algorithms. The panels at the bottom of each graph show the difference of the measured rates from the value predicted by the fit, normalized to the statistical uncertainty on the data (\u03c3 )",
            "paragraph_rank": 107,
            "section_rank": 36
        },
        {
            "text": "Fig. 4",
            "section_rank": 37
        },
        {
            "section": "Fig. 4",
            "text": "Fig. 4 Fit results for the values of a \u03a3 x , b \u03a3 y , c x 0 and d y 0 obtained using different luminosity algorithms during Scan II. The dashed vertical line shows the unweighted average of all the algorithms. The shaded bands indicate \u00b10.5% deviations from the mean for (a) and (b)",
            "paragraph_rank": 108,
            "section_rank": 37
        },
        {
            "text": "Fig. 5",
            "section_rank": 38
        },
        {
            "section": "Fig. 5",
            "text": "Fig. 5 Comparison of the specific luminosities obtained using various luminosity algorithms for a Scan II and b Scan III. The dashed lines show the unweighted average of all algorithms; the shaded band indi-",
            "paragraph_rank": 109,
            "section_rank": 38
        },
        {
            "text": "Fig. 6 a",
            "section_rank": 39
        },
        {
            "section": "Fig. 6 a",
            "text": "Fig. 6a ATLAS instantaneous luminosity for Run 162882, as measured using several algorithms. Each curve is independently normalized using the vdM calibration obtained for that algorithm. The inset at the bottom shows the ratio of the luminosity obtained with each algorithm to that obtained with LUCID_Event_OR. The statistical uncertainties for the online algorithms (LUCID_Event_OR, LU-CID_Event_AND and MBTS_Event_AND) are negligible. Statistical uncertainties for the offline algorithms (LAr_Timing and ChPart) are displayed. b Comparison of the integrated luminosity obtained for Run 162882 for each of the algorithms shown above, together with the statistical uncertainties on the measurements. The dotted line shows the weighted mean of all the algorithms. The shaded band indicates a \u00b12% deviation from that mean",
            "paragraph_rank": 110,
            "section_rank": 39
        },
        {
            "text": "Fig.",
            "section_rank": 40
        },
        {
            "section": "Fig.",
            "text": "Fig. Fraction of bunch crossings containing a detected event (N/N BC ) for several algorithms, as a function of \u03bc LUCID_Event_OR",
            "paragraph_rank": 111,
            "section_rank": 40
        },
        {
            "text": "Fig. 8",
            "section_rank": 41
        },
        {
            "section": "Fig. 8",
            "text": "Fig. 8 Fractional deviation of the average value of \u03bc obtained with the MBTS_Event_AND and LUCID_Event_AND algorithms with respect to the LUCID_Event_OR algorithm as a function of \u03bc obtained with LUCID_Event_OR",
            "paragraph_rank": 112,
            "section_rank": 41
        },
        {
            "text": "Fig. 9",
            "section_rank": 42
        },
        {
            "section": "Fig. 9",
            "text": "Fig. 9Comparison of the measured values of \u03c3 vis for several algorithms to the a PYTHIA and b PHOJET predictions. The errors on the points are the systematic uncertainties due to possible inaccuracies in modeling the detector response. The uncertainties for different algo-",
            "paragraph_rank": 113,
            "section_rank": 42
        },
        {
            "text": "; CERN; CONICYT, Chile; CAS, MOST and NSFC, China; COLCIEN-CIAS, Colombia; MSMT CR, MPO CR and VSC CR, Czech Republic; DNRF, DNSRC and Lundbeck Foundation, Denmark; ARTEMIS, European Union; IN2P3-CNRS, CEA-DSM/IRFU, France; GNAS, Georgia; BMBF, DFG, HGF, MPG and AvH Foundation, Germany; GSRT, Greece; ISF, MINERVA, GIF, DIP and Benoziyo Center, Israel; INFN, Italy; MEXT and JSPS, Japan; CNRST, Morocco; FOM and NWO, Netherlands; RCN, Norway; MNiSW, Poland; GRICES and FCT, Portugal; MERYS (MECTS), Romania; MES of Russia and ROSATOM, Russian Federation; JINR; MSTD, Serbia; MSSR, Slovakia; ARRS and MVZT, Slovenia; DST/NRF, South Africa; MICINN, Spain; SRC and Wallenberg Foundation, Sweden; SER, SNSF and Cantons of Bern and Geneva, Switzerland; NSC, Taiwan; TAEK, Turkey; STFC, the Royal Society and Leverhulme Trust, United Kingdom; DOE and NSF, United States of America.",
            "paragraph_rank": 114,
            "section_rank": 43
        },
        {
            "text": "Table 1",
            "section_rank": 44
        },
        {
            "section": "Table 1",
            "text": "Summary of relevant characteristics of the detectors used for luminosity measurements. For the ZDC, the number of readout channels only includes those used by the luminosity algorithms",
            "paragraph_rank": 115,
            "section_rank": 44
        },
        {
            "text": "\u2212 2e \u2212\u03bc(\u03b5 AND +\u03b5 OR )/2 + e \u2212\u03bc\u03b5 OR",
            "paragraph_rank": 116,
            "section_rank": 45
        },
        {
            "text": "Table 2",
            "section_rank": 46
        },
        {
            "section": "Table 2",
            "text": "Summary of the main characteristics of the three beam scans performed at the ATLAS interaction point. The values of luminosity/bunch and \u03bc are given for zero beam separation",
            "paragraph_rank": 117,
            "section_rank": 46
        },
        {
            "text": "Table 3",
            "section_rank": 47
        },
        {
            "text": "Table 4",
            "section_rank": 48
        },
        {
            "section": "Table 4",
            "text": "Predicted inelastic pp cross sections at \u221a s = 7 TeV for PYTHIA and for PHOJET. A small (\u223c1 mb) contribution from double-pomeron processes (\"central diffraction\") is not included in these cross section predictions",
            "paragraph_rank": 119,
            "section_rank": 48
        },
        {
            "text": "Table 5 Table 6",
            "section_rank": 49
        },
        {
            "section": "Table 5 Table 6",
            "text": "Efficiencies at \u221a s = 7 TeV for several of the luminosity methods described in Sect. 3. The predicted visible cross sections \u03c3 vis are obtained using (27), the efficiencies in the present table and the cross sections inTable 4Comparison of the visible cross sections determined from beam scans (\u03c3 vis ) to the predictions of the PYTHIA and PHOJET Monte Carlo generators. The ratio of prediction to measurement is also shown. The errors affecting the measured visible cross sections are statistical only. The errors on the PYTHIA and PHOJET visible cross sections are obtained from the systematic uncertainty associated with modeling the detector response. These uncertainties are fully corre-lated, row by row, between PYTHIA and PHOJET; they are fully correlated between the two LUCID algorithms, and highly correlated for the five MBTS-triggered algorithms (MBTS_AND, MBTS_OR, MBTS_timing_Event, PrimVtx_Event and ChPart_Event). The fully correlated 11% systematic uncertainty on visible cross sections, that arises from the vdM calibration, is not included in the errors listed in this table",
            "paragraph_rank": 120,
            "section_rank": 49
        },
        {
            "text": "Table 8",
            "section_rank": 50
        },
        {
            "section": "Table 8",
            "text": "Summary of the relevant fit parameters for the Beam Scan II. For offline algorithms, the rates have been corrected for trigger prescales. Because the rates in the BCM were low, the value of \u03a3 used for the BCM was fixed to that obtained from the LUCID_Event_OR",
            "paragraph_rank": 121,
            "section_rank": 50
        },
        {
            "text": "Table 8 (Table 9 Table 10",
            "section_rank": 51
        },
        {
            "section": "Table 8 (Table 9 Table 10",
            "text": "Summary of the relevant fit parameters for the Beam Scan III. For offline algorithms, the rates have been corrected for trigger prescales. Because the rates in the BCM were low, the value of \u03a3 used for the BCM was fixed to that obtained from the LUCID_Event_OR Measurements of the visible cross section and peak specific luminosity for all algorithms that have been calibrated using the vdM scan data for each of the three beam scans. The uncertainties reported here are statistical only. The emittance during Scan I was different from that during Scans II and III, so the specific luminosity in that first scan is not expected to be the same. No results for Scan I are presented for the ZDC, since the constant fraction discriminators used for the ZDC measurements were installed later in the run. Because the rates in the BCM were low, the value of \u03a3 used for the BCM was fixed to that",
            "paragraph_rank": 122,
            "section_rank": 51
        },
        {
            "section": "Table 8 (Table 9 Table 10",
            "text": "In proton storage rings, a small fraction of the injected (or stored) beam may fail to be captured into (or may slowly diffuse out of) the intended RF bucket, generating a barely detectable unbunched beam component and/or coalescing into very low-intensity \"satellite\" bunches that are separated from a nominal bunch by up to a few tens of buckets.",
            "paragraph_rank": 123,
            "section_rank": 51
        },
        {
            "section": "Table 8 (Table 9 Table 10",
            "text": "ATLAS uses the PYTHIA value of 71.5 mb.",
            "paragraph_rank": 124,
            "section_rank": 51
        },
        {
            "section": "Table 8 (Table 9 Table 10",
            "text": "For the coincidence algorithms, the procedure is iterative because it requires the a priori knowledge of \u03c3 vis . Monte Carlo estimates were used as the starting point.",
            "paragraph_rank": 125,
            "section_rank": 51
        },
        {
            "section": "Table 8 (Table 9 Table 10",
            "text": "The bunch-by-bunch event rate per crossing for LUCID_Event_OR averaged over the full run is shown inFig. 1.",
            "paragraph_rank": 126,
            "section_rank": 51
        },
        {
            "section": "Table 8 (Table 9 Table 10",
            "text": "Acknowledgements We wish to thank CERN for the efficient com-",
            "paragraph_rank": 127,
            "section_rank": 53
        },
        {
            "text": "Appendix: Fits to beam scan data",
            "section_rank": 55
        },
        {
            "section": "Appendix: Fits to beam scan data",
            "text": "This appendix presents results of the fits to vdM scan data for all scans and all algorithms. ",
            "paragraph_rank": 128,
            "section_rank": 55
        }
    ]
}