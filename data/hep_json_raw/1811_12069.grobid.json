{
    "level": "paragraph",
    "abstract": [
        {
            "text": "Recently machine learning algorithms based on deep layered artificial neural networks (DNNs) have been applied to a wide variety of high energy physics problems such as jet tagging or event classification. We explore a simple but effective preprocessing step which transforms each real-valued observational quantity or input feature into a binary number with a fixed number of digits. Each binary digit represents the quantity or magnitude in different scales. We have shown that this approach improves the performance of DNNs significantly for some specific tasks without any further complication in feature engineering. We apply this multi-scale distributed binary representation to deep learning on b-jet tagging using daughter particles' momenta and vertex information.",
            "paragraph_rank": 1,
            "section_rank": 1
        }
    ],
    "body_text": [
        {
            "text": "Introduction",
            "section_rank": 2
        },
        {
            "section": "Introduction",
            "text": "In the most recent machine learning, deep-layered artificial neural networks (DNNs) have been successfully applied to wide range of physics problems from phase transitions in statistical mechanics [1,2] to quark/gluon jet discrimination in high energy physics [3]. However, since there are no comprehensive rules or principles by which we can select a particular architecture or the input features of the network model for the corresponding tasks, the so called hyper-parameters determining the architecture of the networks and the overall learning processes are examined by pain-stacking trials and errors for a given set of input features. The resulting parameters for the networks are believed to be optimal, but they are too specific to be applied for other similar tasks. So for these similar tasks, trials and errors often need to be repeated.",
            "paragraph_rank": 2,
            "section_rank": 2,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b0",
                    "start": 197,
                    "text": "[1,",
                    "end": 200
                },
                {
                    "type": "bibr",
                    "ref_id": "b1",
                    "start": 200,
                    "text": "2]",
                    "end": 202
                },
                {
                    "type": "bibr",
                    "ref_id": "b2",
                    "start": 260,
                    "text": "[3]",
                    "end": 263
                }
            ]
        },
        {
            "section": "Introduction",
            "text": "However, if we use good input features or representations for DNNs, we can reduce the repetitions, since the good input features help DNNs to learn good internal representations [4,5] with less dependence on its detailed architectures. Our investigation stems from a following simple question: what happens if one uses multiple variables with smaller dynamic range instead of one variable with large dynamic range as an input feature? We have tested this with a simple model. We designed a network to predict the sign of the sum of the spin states on a 3 \u00d7 3 block within a 10 \u00d7 10 lattice of random spins provided with the coordinates of center of the block limited within inner 8 \u00d7 8 lattice as shown in Fig. 1. The network had to find out what the uniformly distributed input variables for coordinates of block center site meant and how to process them to perform the task, as this was not coded in explicitly. In this setting, when we provided the network with a binary representation of the site coordinate, for example (011, 100) instead of (3, 4), the network learned more quickly and its training and test error were less as shown on the Fig. 1b. We can consider that this phenomenon is caused by the increased sparsity resulting from transforming the input values from a large base range to a smaller base (see Fig. 8 for the related discussion), having a distributed representation [4,5] simply implemented with a number of binary digits for each input value in this case.",
            "paragraph_rank": 3,
            "section_rank": 2,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b3",
                    "start": 178,
                    "text": "[4,",
                    "end": 181
                },
                {
                    "type": "bibr",
                    "ref_id": "b4",
                    "start": 181,
                    "text": "5]",
                    "end": 183
                },
                {
                    "type": "figure",
                    "ref_id": "fig_1",
                    "start": 706,
                    "text": "Fig. 1",
                    "end": 712
                },
                {
                    "type": "figure",
                    "ref_id": "fig_1",
                    "start": 1146,
                    "text": "Fig. 1b",
                    "end": 1153
                },
                {
                    "type": "figure",
                    "start": 1320,
                    "text": "Fig. 8",
                    "end": 1326
                },
                {
                    "type": "bibr",
                    "ref_id": "b3",
                    "start": 1392,
                    "text": "[4,",
                    "end": 1395
                },
                {
                    "type": "bibr",
                    "ref_id": "b4",
                    "start": 1395,
                    "text": "5]",
                    "end": 1397
                }
            ]
        },
        {
            "section": "Introduction",
            "text": "Inspired by these findings, we transformed each real-valued feature of jet constituents to a k-number of binary digits as the input features of deep neural networks for b-jet tagging [6] to test whether this simple preprocessing can improve the performance of the networks on a harder problem. We could implement the multi-scale distributed (MSD) representation in various ways, but the representation with k-number of binary digits (MSD 2 k-digits) is the simplest and convenient one. Thus we studied only this binary implementation, which shall be denoted as MSD 2 k-digits or simply MSD k-digits from now on.",
            "paragraph_rank": 4,
            "section_rank": 2,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b5",
                    "start": 183,
                    "text": "[6]",
                    "end": 186
                }
            ]
        },
        {
            "section": "Introduction",
            "text": "In section 2, we will describe briefly data set used in this work, before exploring the detailed preprocessing of jet data and MSD digit representation. Section 3 covers the network architectures and its learning processes, and section 4 presents the results followed by our concluding remarks.  ",
            "paragraph_rank": 5,
            "section_rank": 2
        },
        {
            "text": "Jet Data and MSD k-digit Representation",
            "section_rank": 3
        },
        {
            "section": "Jet Data and MSD k-digit Representation",
            "text": "To test the improvement of the MSD representational algorithm against the typically used real-valued ones on b-jet tagging, we generated tt events in pp collisions at \u221a s = 13 TeV using the next leading order POWHEG [9] event generator matched to PYTHIA8 [7] for hadronization. Jets with transverse momentum p T > 30 GeV and pseudo rapidity |\u03b7| < 2.4 with at least two daughter constituents are selected. We used FastJet [8] with the anti-k T algorithm [11] with a jet radius R = 0.4 for jet finding and clustering. We used Delphes for fast detector simulation [10]. We did not separated samples by p T range of jet for quick and simple test of MSD representation.",
            "paragraph_rank": 6,
            "section_rank": 3,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b8",
                    "start": 216,
                    "text": "[9]",
                    "end": 219
                },
                {
                    "type": "bibr",
                    "ref_id": "b6",
                    "start": 255,
                    "text": "[7]",
                    "end": 258
                },
                {
                    "type": "bibr",
                    "ref_id": "b7",
                    "start": 421,
                    "text": "[8]",
                    "end": 424
                },
                {
                    "type": "bibr",
                    "ref_id": "b10",
                    "start": 453,
                    "text": "[11]",
                    "end": 457
                },
                {
                    "type": "bibr",
                    "ref_id": "b9",
                    "start": 561,
                    "text": "[10]",
                    "end": 565
                }
            ]
        },
        {
            "section": "Jet Data and MSD k-digit Representation",
            "text": "We used only jets initiated from light partons such as up, down, strange quarks and gluon (light-jet) for background. The number of b-jets and light-jets used in this study are the same, each with 70k samples. The total 140k jet samples are divided into a training set (D train ) of 100k samples, with a early validation set (D validation ) of 10k samples for early-stop to be collectively applied to all member networks and test set (D test ) of 30k samples for the final performance evaluation.",
            "paragraph_rank": 7,
            "section_rank": 3
        },
        {
            "section": "Jet Data and MSD k-digit Representation",
            "text": "We used the jet constituent's relative transverse momentum ratio and it's vertex positions as input features (",
            "paragraph_rank": 8,
            "section_rank": 3
        },
        {
            "section": "Jet Data and MSD k-digit Representation",
            "text": "Jets have different number of its constituents or particles. We used a fully connected feed forward networks which take a fixed number of variables for its input, so we truncated the sequence of jet constituents variables up to the 50th constituent. And if the number of jet constituents n is less than 50, we set rest values to zeros. So,",
            "paragraph_rank": 9,
            "section_rank": 3
        },
        {
            "section": "Jet Data and MSD k-digit Representation",
            "text": "where upper index represents the order of jet constituent in some rule, e.g. ordering by p T . Before transforming the sequence of jet constituents variables x to MSD k-digit representation, zero-centering and normalization processes are performed to adjust the dynamic ranges of each component. This data-oriented processed representation will be called real-valued representation and it is defined as follows: and v z before and after the normalization process (1).",
            "paragraph_rank": 10,
            "section_rank": 3
        },
        {
            "section": "Jet Data and MSD k-digit Representation",
            "text": "Transforming the real-valued representation, u, to MSD 2 k-digit representation can be simply performed by converting decimal numbers to binary digits with signed magnitude representation after clipping and rounding process as follows:",
            "paragraph_rank": 11,
            "section_rank": 3
        },
        {
            "section": "Jet Data and MSD k-digit Representation",
            "text": "and then the resulting decimal number z i is represented as a signed binary B i",
            "paragraph_rank": 12,
            "section_rank": 3
        },
        {
            "section": "Jet Data and MSD k-digit Representation",
            "text": "where i = 1, 2, . . . , 200 and b \u2208 {0, 1}. The number of features are increased to 200 \u00d7 k by the transformation. While the optimal interval or resolution may need systematic analysis, we just set it by 20\u03c3/(2 k\u22121 \u2212 1), with \u03c3 = 1 for the real-valued representation. ",
            "paragraph_rank": 13,
            "section_rank": 3
        },
        {
            "text": "Networks Architectures and Learning processes",
            "section_rank": 4
        },
        {
            "section": "Networks Architectures and Learning processes",
            "text": "In this study, we used a fully connected neural network (FCN) for b-jet tagging. The schematic architecture of this network is shown in Fig. 5. We used the rectified linear unit (ReLU) activation function [12] for all units h",
            "paragraph_rank": 14,
            "section_rank": 4,
            "ref_spans": [
                {
                    "type": "figure",
                    "ref_id": "fig_6",
                    "start": 136,
                    "text": "Fig. 5",
                    "end": 142
                },
                {
                    "type": "bibr",
                    "ref_id": "b11",
                    "start": 205,
                    "text": "[12]",
                    "end": 209
                }
            ]
        },
        {
            "section": "Networks Architectures and Learning processes",
            "text": "i 's in each hidden layer indexed by the superscript l and the sof tmax function for two output units o 1 and o 2 . The categorical cross-entropy is used as the cost function for the training.",
            "paragraph_rank": 15,
            "section_rank": 4
        },
        {
            "section": "Networks Architectures and Learning processes",
            "text": "Below are input units for each of the real-valued jet constituent features and its corresponding MSD 16-digit representation.",
            "paragraph_rank": 16,
            "section_rank": 4
        },
        {
            "section": "Networks Architectures and Learning processes",
            "text": "where binary position index m = 0, . . . , 15 and i = 1, . . . , 4 \u00d7 50 for the real-valued input feature. We prepared both \"Deep\" and \"Not so deep\" versions for each input representation to check the stability of our result. where binary position index m = 0, . . . , 15 and i = 1, . . . , 4 \u00d7 50 for the real-valued input feature. We prepared both \"Deep\" and \"Not so deep\" versions for each input representation to check the stability of our result. Neural networks were trained using the Theano Python library [13] on GPUs using the NVIDIA CUDA platform. The connection weights of the networks were initialized with the Xavier Initialization scheme [14]. The Adam [15] algorithm was used to update the weights up to each early-stop epoch specified in Table. 1, using a batch size of 1000 which corresponds to 100 iterations per epoch in the training stage.",
            "paragraph_rank": 17,
            "section_rank": 4,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b12",
                    "start": 513,
                    "text": "[13]",
                    "end": 517
                },
                {
                    "type": "bibr",
                    "ref_id": "b13",
                    "start": 652,
                    "text": "[14]",
                    "end": 656
                },
                {
                    "type": "bibr",
                    "ref_id": "b14",
                    "start": 667,
                    "text": "[15]",
                    "end": 671
                },
                {
                    "type": "table",
                    "start": 754,
                    "text": "Table.",
                    "end": 760
                }
            ]
        },
        {
            "section": "Networks Architectures and Learning processes",
            "text": "For regularization, we used the simple ensemble voting method [16] in which the network ensemble was composed of networks trained with just different random seeds for their initial connection weights and a random sequence of mini-batches up to epoch t like this, N (t) = {f (I; \u03b8 1 (t)), . . . , f (I; \u03b8 q (t)), . . . , f (I; \u03b8 S (t))},  in try-out phase with ensemble size = 20 along iterations. Two curves showing higher error rates at early epochs are for the real-valued representation on each \"Not so deep\" and \"Deep\" plots, while the lower two curves for the MSD representation. Curves on training error are lower than those of validation for each representation. and the ensemble vote was defined as where f is the network model, \u03b8 q (t)'s represent the connection weights and biases updated up to epoch t, with different initialization and mini-batch sequence D train q (t) = {d q (0), d q (1), . . . , d q (t)} by random seed with index q, and S is size of the ensemble of networks. We also used L 2 penalty [17] and early-stop method together with previously mentioned ensemble method. The early stop was determined by inspecting networks learning progress plot obtained by try-out learning up to 30 epochs ( Fig. 6), to find the epoch at which the mean of validation errors by 20 member networks (try-out ensemble size S try\u2212out = 20) become stagnated, with the ensemble mean error as below at epoch t,",
            "paragraph_rank": 18,
            "section_rank": 4,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b15",
                    "start": 62,
                    "text": "[16]",
                    "end": 66
                },
                {
                    "type": "bibr",
                    "ref_id": "b16",
                    "start": 1017,
                    "text": "[17]",
                    "end": 1021
                },
                {
                    "type": "figure",
                    "ref_id": "fig_7",
                    "start": 1219,
                    "text": "Fig. 6)",
                    "end": 1226
                }
            ]
        },
        {
            "section": "Networks Architectures and Learning processes",
            "text": "where E fq [D] represent prediction error rate by member network f (I; \u03b8 q (t)) of ensemble N for data set D at epoch t. Table. 1 shows the summary of the architectures and parameters on regularization.",
            "paragraph_rank": 19,
            "section_rank": 4,
            "ref_spans": [
                {
                    "type": "table",
                    "start": 121,
                    "text": "Table.",
                    "end": 127
                }
            ]
        },
        {
            "text": "Results",
            "section_rank": 5
        },
        {
            "section": "Results",
            "text": "We obtained performance curves by extracting outputs of the networks, shown in Fig. 7. The MSD method shows significant improvement on both \"Deep\" and \"Not so deep\" networks compared to the real-valued method. On the other hand, only a marginal improvement was observed when one uses two's complement binary representation over the real-valued method. Two's complement representation showed lower performance than the signed MSD representation as demonstrated in Fig. 7b. There are a few comments on the sparsity of our representation. We found that our MSD representation is more sparse than the real-valued representation. This was demonstrated in Fig. 8 where the averages of normalized activation h",
            "paragraph_rank": 20,
            "section_rank": 5,
            "ref_spans": [
                {
                    "type": "figure",
                    "ref_id": "fig_9",
                    "start": 79,
                    "text": "Fig. 7",
                    "end": 85
                },
                {
                    "type": "figure",
                    "ref_id": "fig_9",
                    "start": 463,
                    "text": "Fig. 7b",
                    "end": 470
                },
                {
                    "type": "figure",
                    "start": 650,
                    "text": "Fig. 8",
                    "end": 656
                }
            ]
        },
        {
            "section": "Results",
            "text": "are plotted. In Fig. 8a, the blue line is for the MSD representation in the \"Not so deep\" network. It clearly shows that the number of deactivated units for this MSD representation  is much larger than that for the real-valued representation (See the green line). Hence we conclude that our MSD representation is more sparse, which partially explains why it shows a better performance in this machine learning. Fig. 8b is for the \"Deep\" networks and basically shows the same trends.  Figure 8: Effect on sparsity: the averages of normalized activation h (1)  ",
            "paragraph_rank": 21,
            "section_rank": 5,
            "ref_spans": [
                {
                    "type": "figure",
                    "start": 16,
                    "text": "Fig. 8a",
                    "end": 23
                },
                {
                    "type": "figure",
                    "start": 411,
                    "text": "Fig. 8b",
                    "end": 418
                },
                {
                    "type": "figure",
                    "start": 484,
                    "text": "Figure 8",
                    "end": 492
                },
                {
                    "type": "bibr",
                    "ref_id": "b0",
                    "start": 554,
                    "text": "(1)",
                    "end": 557
                }
            ]
        },
        {
            "text": "Conclusions",
            "section_rank": 6
        },
        {
            "section": "Conclusions",
            "text": "We demonstrated that a simple transformation from each real-valued input feature to MSD digit representation can lead to a large improvement in the fully connected networks for b-jet tagging without any additional domain specific feature engineering.",
            "paragraph_rank": 22,
            "section_rank": 6
        },
        {
            "section": "Conclusions",
            "text": "In typical network optimization, one has to examine broad range of hyper-parameters, such as network depth, number of units per layer and regularization parameters. We have shown that our results from two groups of networks, \"Deep\" and \"Not so deep\", are not sensitive to such parameters.",
            "paragraph_rank": 23,
            "section_rank": 6
        },
        {
            "section": "Conclusions",
            "text": "Compared with a typical binary transformation, our MSD conversion described in Eqs. (2) and (3) from the real-valued feature to signed binary digits reflects the multi-scale property of the original data. For example, typical two's complement representation converts decimal digits (\u22123, \u22122, \u22121, 0, 1, 2, 3) to (101, 110, 111, 000, 001, 010, 011), while our signed binary representation converts to (111, 110, 101, 000, 001, 010, 011). Our choice not only strengthens the multi-scale property but also makes the result informationtheoretically effective. For instance, this is because (111), two's complement of \u22121, acti-vates excessive input units compared with the signed representation of (101).",
            "paragraph_rank": 24,
            "section_rank": 6
        },
        {
            "section": "Conclusions",
            "text": "In this note, we limit our study of the MSD digit representation to the problem of b-jet tagging. However our method can be straightforwardly applied to many other areas of deep learning problems. Further investigation is required in this direction.",
            "paragraph_rank": 25,
            "section_rank": 6
        },
        {
            "text": "Figure 1 :",
            "section_rank": 7
        },
        {
            "section": "Figure 1 :",
            "text": "Figure 1: Multi-scale representation of site position distributed in 6 binary digits: the left most digits represent the region in largest scale.",
            "paragraph_rank": 26,
            "section_rank": 7
        },
        {
            "text": "Figure 2 :",
            "section_rank": 8
        },
        {
            "section": "Figure 2 :",
            "text": "Figure 2: Mean and standard deviation (std) plots for jet constituentsr P T , v x ,v z along constituentindex, before (upper two rows) and after (lower two rows) normalization.",
            "paragraph_rank": 27,
            "section_rank": 8
        },
        {
            "text": "Fig. 2 Figure 3 :",
            "section_rank": 9
        },
        {
            "section": "Fig. 2 Figure 3 :",
            "text": "Fig. 2shows distributions of ratio r p T of constituent's p const.T",
            "paragraph_rank": 28,
            "section_rank": 9
        },
        {
            "text": "Fig. 3",
            "section_rank": 10
        },
        {
            "section": "Fig. 3",
            "text": "shows the mean and standard deviations of the vertex position, v x , for each of the digits in the MSD 2 16-digit representation. The first digit, b 0 , showing the sign bit, followed by b 1 for the first significant figure, to the last digit b 15 , are the so called signed MSD 2 16-digit representation 1 . The horizontal axis represents the jet constituent index. The plots of the mean values of the most significant binary digits are smooth and it starts to be more discontinuous and the digit significance decreases. The reason for such discontinuity is due to the presence of sharp peaks near zero in the distribution of simulated v x , as seen inFig. 4.",
            "paragraph_rank": 29,
            "section_rank": 10
        },
        {
            "text": "Figure 4 :",
            "section_rank": 11
        },
        {
            "section": "Figure 4 :",
            "text": "Figure 4: The distribution of jet constituents' v x in simulated samples: (top) log-scale vertex position significance v x /\u03c3 vx and (bottom) digitized vertex position with interval (20\u03c3 vx )/2 15 near zero. The plots on the left are for the b-jet whereas the right ones for the light-jet.",
            "paragraph_rank": 30,
            "section_rank": 11
        },
        {
            "text": "Figure 5 :",
            "section_rank": 12
        },
        {
            "section": "Figure 5 :",
            "text": "Figure 5: Schematic architecture of fully connected neural network (FCN) used for b-jet tagging with jet constituents variable in real-valued or MSD k-digit representation. Blue lines with arrow connecting units of lower and higher layers represent the direction of information flow in which a signal from one unit in a layer enters into all units in its next layer.",
            "paragraph_rank": 31,
            "section_rank": 12
        },
        {
            "text": "Figure 6 :",
            "section_rank": 13
        },
        {
            "section": "Figure 6 :",
            "text": "Figure 6: Learning progress (mean of training and validation errors by member networks of ensemble",
            "paragraph_rank": 32,
            "section_rank": 13
        },
        {
            "text": "Figure 7 :",
            "section_rank": 14
        },
        {
            "section": "Figure 7 :",
            "text": "Figure 7: Performance curves obtained from the output of ensemble vote: the horizontal axis represents b-jet efficiency and the vertical axis represents probability with which the network ensemble identify incorrectly light-jet as b-jet. (a) Plot for comparison of MSD 16-digit and the real-valued representation. The curves split into two groups, upper two lines for the real-valued and lower two lines for the MSD 16-digit. (b) Plot for comparison of MSD 16-digit, two's complement binary 16-digit, and the real-valued representations. The performance of the two's complement representation is rather similar to that of the real-valued representation.",
            "paragraph_rank": 33,
            "section_rank": 14
        },
        {
            "text": "test of hidden units in the first layer of \"Not so deep\" (a) and \"Deep\" (b) networks for each real-valued and MSD 16digit representation to the problem of b-jet tagging, where h (1) (i) is the sorted activation from the smallest to the largest as h (1)",
            "paragraph_rank": 34,
            "section_rank": 15
        },
        {
            "text": "Table 1 :",
            "section_rank": 16
        },
        {
            "section": "Table 1 :",
            "text": "Hyper-parameters on regularization and architecture are presented in this table. While the number of input units of network for real-valued representation is 200, the number of input units of network for MSD-16 is 200 \u00d7 16 = 3200.",
            "paragraph_rank": 35,
            "section_rank": 16
        },
        {
            "section": "Table 1 :",
            "text": "Let B a k \u22121 digit binary number. Its signed k-digit representation is defined as follows: For negative B, one has the binary representation of 2 k\u22121 + |B| while, for non negative B, one simply adds an extra 0 in front of B to get k-digit representation. On the other hand, for its two's complement representation below, one has k-digit binary representation of 2 k \u2212 |B| for negative B whereas the definition of k-digit representation remains the same for non negative B.",
            "paragraph_rank": 36,
            "section_rank": 16
        },
        {
            "text": "Acknowledgement",
            "section_rank": 18
        },
        {
            "section": "Acknowledgement",
            "text": "This work was supported in part by NRF Grant 2017R1A2B4003095.",
            "paragraph_rank": 37,
            "section_rank": 18
        }
    ]
}