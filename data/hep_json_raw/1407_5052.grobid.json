{
    "level": "paragraph",
    "abstract": [
        {
            "text": "We describe a method based on the CLs approach to present results in searches of new physics, under the condition that the relevant parameter space is continuous. Our method relies on a class of test statistics developed for non-nested hypotheses testing problems, denoted by \u2206T , which has a Gaussian approximation to its parent distribution when the sample size is large. This leads to a simple procedure of forming exclusion sets for the parameters of interest, which we call the Gaussian CLs method. Our work provides a self-contained mathematical proof for the Gaussian CLs method, that explicitly outlines the required conditions. These conditions are milder than that required by the Wilks' theorem to set confidence intervals (CIs). We illustrate the Gaussian CLs method in an example of searching for a sterile neutrino, where the CLs approach was rarely used before. We also compare data analysis results produced by the Gaussian CLs method and various CI methods to showcase their differences.",
            "paragraph_rank": 1,
            "section_rank": 1
        }
    ],
    "body_text": [
        {
            "text": "I. INTRODUCTION",
            "section_rank": 2
        },
        {
            "section": "I. INTRODUCTION",
            "text": "The Standard Model of particle physics has been extremely successful since its establishment in the mid-1970s. In particular, the Higgs particle discovered at LHC in 2012 [1,2] completed the list of fundamental particles predicted by the minimal Standard Model. On the other hand, there has been experimental evidence that point to new physics beyond the Standard Model: neutrino oscillations indicate non-zero neutrino mass; various gravitational effects indicate the existence of nonbaryonic dark matter; the accelerating expansion of our universe indicates the existence of dark energy; the large observed matter-anti-matter asymmetry in the universe indicates the existence of additional CP violation source beyond that in the quark mixing matrix, etc. Searches for new physics beyond the Standard Model have been and still are at the frontier of high energy particle physics.",
            "paragraph_rank": 2,
            "section_rank": 2,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b0",
                    "start": 171,
                    "text": "[1,",
                    "end": 174
                },
                {
                    "type": "bibr",
                    "ref_id": "b1",
                    "start": 174,
                    "text": "2]",
                    "end": 176
                }
            ]
        },
        {
            "section": "I. INTRODUCTION",
            "text": "Given experiment data, a problem of searching for new physics often turns into a parameter estimation problem, and the findings are presented in the form of constraints on some continuous parameter(s). One example is the search for sterile neutrino suggested by LSND [3], MiniBooNE [4], and reactor antineutrino anomalies [5]. 1 In this case, data collected from an experiment consists of neutrino interaction counts in multiple energy bins, x = (N 1 , \u2022 \u2022 \u2022 , N n ). Data analysis results are generally shown as constraints in the two-dimensional parameter space of (sin 2 2\u03b8, |\u2206m 2 |), where \u03b8 is the mixing angle involving the sterile neutrino, and |\u2206m 2 | is the mass-squared difference of neutrino mass eigenstate beyond three generations.",
            "paragraph_rank": 3,
            "section_rank": 2,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b2",
                    "start": 267,
                    "text": "[3]",
                    "end": 270
                },
                {
                    "type": "bibr",
                    "ref_id": "b3",
                    "start": 282,
                    "text": "[4]",
                    "end": 285
                },
                {
                    "type": "bibr",
                    "ref_id": "b4",
                    "start": 322,
                    "text": "[5]",
                    "end": 325
                },
                {
                    "type": "bibr",
                    "ref_id": "b0",
                    "start": 327,
                    "text": "1",
                    "end": 328
                }
            ]
        },
        {
            "section": "I. INTRODUCTION",
            "text": "One way to set constraints is to form confidence intervals 2 (CI), which contains parameter values that are compatible with the data. Let \u03b2 denote the parameter(s), such as \u03b2 = (sin 2 2\u03b8, |\u2206m 2 |) in the neutrino oscillation problem. A CI can be obtained by inverting a testing procedure. Specifically, the set of all \u03b2 1 such that the hypothesis H 0 : \u03b2 = \u03b2 1 is not rejected at level 1 \u2212 c, forms a level-c CI. A testing procedure is often performed by thresholding a test statistic, which is a user-chosen function that, for any given \u03b2 1 , defines a criterion to order all possible values of x. To test H 0 : \u03b2 = \u03b2 1 , a commonly used type of test statistic takes the form",
            "paragraph_rank": 4,
            "section_rank": 2
        },
        {
            "section": "I. INTRODUCTION",
            "text": "where \u03c7 2 is a function that measures the compatibility between \u03b2 1 and x. One important example of \u03c7 2 is the negative-two-log-likelihood function, and the corresponding \u2206\u03c7 2 is called the likelihood ratio (LR) test statistic.",
            "paragraph_rank": 5,
            "section_rank": 2
        },
        {
            "section": "I. INTRODUCTION",
            "text": "In the field of high energy physics, the unified approach to construct CIs advocated by Feldman and Cousins [6] is indeed based on the likelihood ratio test statistic. A parameter value \u03b2 1 is included in a level-c CI if \u2206\u03c7 2 (\u03b2 1 ; x) is below a threshold t c , such that Prob \u03b21 (\u2206\u03c7 2 (\u03b2 1 ; X) \u2264 t c ) \u2265 c. Here, the subscript \u03b2 1 means that X is a random outcome from a model with true parameter value \u03b2 1 . In general, Monte Carlo (MC) simulation can be used 3 to approximate the parent dis-tribution of \u2206\u03c7 2 . We refer to the corresponding method of constructing CIs as the MC CI method. An example of the MC CI method, tailored for the LR test statistic, can be found in section V.B of Feldman and Cousins [6]. The MC CI method is often computationally intensive. Alternatively, t c can be approximated using a Chi-square distribution, a summary of its usage in particle physics is provided by the Particle Data Group [8]. This method is simple to carry out, but the approximation is only valid under relatively stringent conditions. Specifically, the Chi-square thresholds are justified by the Wilks' theorem [9] for the LR test statistic under regularity conditions C1-C3 in Sec. II, and they are justified for the variations of \u2206\u03c7 2 listed in Sec. IV A under similar conditions [10,11]. We conveniently refer to any method that constructs approximate CIs based on Chi-square thresholds as a Wilks' CI method.",
            "paragraph_rank": 6,
            "section_rank": 2,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b5",
                    "start": 108,
                    "text": "[6]",
                    "end": 111
                },
                {
                    "type": "bibr",
                    "ref_id": "b2",
                    "start": 464,
                    "text": "3",
                    "end": 465
                },
                {
                    "type": "bibr",
                    "ref_id": "b1",
                    "start": 512,
                    "text": "2",
                    "end": 513
                },
                {
                    "type": "bibr",
                    "ref_id": "b5",
                    "start": 713,
                    "text": "[6]",
                    "end": 716
                },
                {
                    "type": "bibr",
                    "ref_id": "b7",
                    "start": 925,
                    "text": "[8]",
                    "end": 928
                },
                {
                    "type": "bibr",
                    "ref_id": "b8",
                    "start": 1117,
                    "text": "[9]",
                    "end": 1120
                },
                {
                    "type": "bibr",
                    "ref_id": "b9",
                    "start": 1288,
                    "text": "[10,",
                    "end": 1292
                },
                {
                    "type": "bibr",
                    "ref_id": "b10",
                    "start": 1292,
                    "text": "11]",
                    "end": 1295
                }
            ]
        },
        {
            "section": "I. INTRODUCTION",
            "text": "In theory, forming CIs using test statistics of the form \u2206\u03c7 2 (\u03b2; X) is desirable, because it leads to a unified approach in setting limits in the absence of new physics signals and in estimating parameters after the discovery of new physics [6]. However, in the problem of searching for sterile neutrinos, the computationally expensive MC CI method is usually necessary to obtain valid thresholds t c for the \u2206\u03c7 2 statistic, making the application difficult.",
            "paragraph_rank": 7,
            "section_rank": 2,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b5",
                    "start": 242,
                    "text": "[6]",
                    "end": 245
                }
            ]
        },
        {
            "section": "I. INTRODUCTION",
            "text": "Compared to \u2206\u03c7 2 , the following test statistic, \u2206T , has a parent distribution that is easy to approximate under mild conditions. ",
            "paragraph_rank": 8,
            "section_rank": 2
        },
        {
            "section": "I. INTRODUCTION",
            "text": "An example of \u2206T is the negative-two-log-likelihood ratio statistic for H 0 and H 1 . Given observed data x and a fixed \u03b2 ref , all \u03b2 1 values that result in Prob \u03b21 (\u2206T (\u03b2 ref , \u03b2 1 ; X) \u2264 t \u2032 c ) \u2265 c constitute a level-c CI. It is proven in Sec. III B that under fairly mild conditions, one can approximate t \u2032 c using quantiles of a Gaussian distribution. Specifically, we show that when the data size is large, the distribution of \u2206T (X), where X represents potential data from a model that satisfies either one of the two hypotheses, say H, can be approximated by the Gaussian distribution with mean \u2206T H and standard deviation 2",
            "paragraph_rank": 9,
            "section_rank": 2
        },
        {
            "text": "\u221a",
            "section_rank": 3
        },
        {
            "section": "\u221a",
            "text": "|\u2206T H |. Here, \u2206T H is defined to be \u2206T (x Asimov H ) as in Eq. (19), where x Asimov H is the Asimov data set [12] as introduced in Sec. III.",
            "paragraph_rank": 10,
            "section_rank": 3,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b18",
                    "start": 64,
                    "text": "(19)",
                    "end": 68
                },
                {
                    "type": "bibr",
                    "ref_id": "b11",
                    "start": 110,
                    "text": "[12]",
                    "end": 114
                }
            ]
        },
        {
            "section": "\u221a",
            "text": "However, CIs constructed from \u2206T can exclude \u03b2 1 values that are not much less compatible with the data than \u03b2 ref is, which we demonstrate in Sec. V D. To avoid counter-intuitive results based on \u2206T , we take the CL s approach of setting exclusion sets [13][14][15] as an alternative to the CI approach. We refer to the simple procedure of setting exclusion sets based on the \u2206T statistic using a Gaussian approximation as the Gaussian CL s method.",
            "paragraph_rank": 11,
            "section_rank": 3,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b12",
                    "start": 254,
                    "text": "[13]",
                    "end": 258
                },
                {
                    "type": "bibr",
                    "ref_id": "b13",
                    "start": 258,
                    "text": "[14]",
                    "end": 262
                },
                {
                    "type": "bibr",
                    "ref_id": "b14",
                    "start": 262,
                    "text": "[15]",
                    "end": 266
                }
            ]
        },
        {
            "section": "\u221a",
            "text": "Note that an exclusion set imposes a different kind of constraint than that of (the complement of) a CI. An exclusion set aims at identifying parameter values that fit the data much worse than the reference model. Consequently, the CL s approach is more reluctant than the CI approach to exclude models where the experiment has little sensitivity. An example comparing the two can be found in Sec. V D.",
            "paragraph_rank": 12,
            "section_rank": 3
        },
        {
            "section": "\u221a",
            "text": "The main contribution of this paper is to provide a mathematical proof for a Gaussian approximation to the distribution of \u2206T . This result justifies the Gaussian CL s method, which requires a computational load similar to that of the Wilks' CI method, and the former is valid in situations where the latter is not. Results similar to ours can be found in Ref. [12] in the context of searching new particles, and in Ref. [16,17] in the context of neutrino mass hierarchy determinations. The self-contained proof provided in this paper makes it easier to fully articulate the required conditions, which were missing in the previous work. Also, we make a more general and realistic assumption in accordance with the physics problem of interest than that of Ref. [12] and the paper by Wald [10] cited therein. For details, see assumptions [A0] and [A1] in Sec. III B 2.",
            "paragraph_rank": 13,
            "section_rank": 3,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b11",
                    "start": 361,
                    "text": "[12]",
                    "end": 365
                },
                {
                    "type": "bibr",
                    "ref_id": "b15",
                    "start": 421,
                    "text": "[16,",
                    "end": 425
                },
                {
                    "type": "bibr",
                    "ref_id": "b16",
                    "start": 425,
                    "text": "17]",
                    "end": 428
                },
                {
                    "type": "bibr",
                    "ref_id": "b11",
                    "start": 760,
                    "text": "[12]",
                    "end": 764
                },
                {
                    "type": "bibr",
                    "ref_id": "b9",
                    "start": 787,
                    "text": "[10]",
                    "end": 791
                }
            ]
        },
        {
            "section": "\u221a",
            "text": "Another contribution of this paper is that we compare various methods that take the CI approach or the CL s approach in a problem of searching for neutrino oscillations, where the CL s approach was rarely used before. Based on the comparisons, we advocate the Gaussian CL s as an attractive alternative method to the CI approach in the application of searching for new physics through precision measurements. First, the Gaussian CL s method is inexpensive to carry out and is valid in very general setups. Secondly, researchers often need to combine results from different experiments. When conditions in the Wilks' theorem are not satisfied, it is simple to combine the test statistics from different experiments and form an overall CI using the Wilks' method. Otherwise, expensive MC methods have to be used to form CIs for each experiment, and there is no rigorous way to combine these results together other than to rerun a more expensive MC for the combined data. In contrast, we explain in Sec. VI that experimental results can be easily combined using the Gaussian CL s method, and is valid under mild conditions. This paper is organized as follows. In Sec. II, we briefly review the CI approach that utilizes a class of statistics, \u2206\u03c7 2 . We look at both the Wilks' CI method and the MC CI method, and discuss their advantages and limitations. In Sec. III, we describe an alternative class of statistics, \u2206T . In Sec. IV, we describe the CL s approach based on the \u2206T statistic, and outline a simple procedure to carry it out using the Gaussian approximation. In Sec. V, using an example of the search for a sterile neutrino, we check the validity of the approximation in the Gaussian CL s method, and compare different methods of forming constraints in the parameter space. Finally, we present discussions and summaries in Sec. VI and Sec. VII, respectively.",
            "paragraph_rank": 14,
            "section_rank": 3
        },
        {
            "text": "II. THE CONFIDENCE INTERVAL APPROACH BASED ON THE \u2206\u03c7 2 STATISTIC",
            "section_rank": 4
        },
        {
            "section": "II. THE CONFIDENCE INTERVAL APPROACH BASED ON THE \u2206\u03c7 2 STATISTIC",
            "text": "In this section, we briefly review the traditional method of setting CIs in the context of neutrino oscillations. We consider a neutrino energy spectrum that consists of n energy bins, and assume that the mean number of counts in each bin is a function of the vector of parameters of main interest, \u03b2 = (sin 2 2\u03b8, |\u2206m 2 |), and a vector of nuisance parameters (such as the overall normalization), \u03b7. Let \u0398, M , and H denote the parameter space of sin 2 2\u03b8, |\u2206m 2 |, and \u03b7, respectively. There are two further physical constraints: sin 2 2\u03b8 \u2265 0 and |\u2206m 2 | \u2265 0. Then for the i-th bin, \u03bb i (sin 2 2\u03b8, |\u2206m 2 |, \u03b7) and N i represent the mean and the observed counts of neutrino induced interactions, respectively. When \u03bb i is large enough, the distribution of N i can be well approximated by a Gaussian distribution with mean \u03bb i and standard deviation \u221a \u03bb i . Given any specific guess of the value of the parameters (sin 2 2\u03b8, |\u2206m 2 |, \u03b7), once the data x = {N i , i = 1, . . . , n} are observed, one can calculate the deviation of the data from the mean values \u03bb i to measure the compatibility of the hypothesized parameter values to x. Commonly used deviations include negative-two-log-likelihood ratio, Pearson chi-square and Neyman chi-square. Further, when certain knowledge concerning the nuisance parameter \u03b7 (e.g. knowledge of detecting efficiency and neutrino flux) is available, it can be reflected in the definition of the deviation. For example, to modify the Pearson Chisquare, denoted by \u03c7 2 P , when previous experiments suggest an estimate of \u03b7 to be \u03b7 0 with standard deviation \u03c3 \u03b7 , one can define the following deviation function:",
            "paragraph_rank": 15,
            "section_rank": 4
        },
        {
            "section": "II. THE CONFIDENCE INTERVAL APPROACH BASED ON THE \u2206\u03c7 2 STATISTIC",
            "text": "Below, we use the notation arg min w h(w) to denote the value of w that minimizes any given function h, and the standard set-builder notation {h(w) : restriction w} to denote a set that is made up of all the points h(w) such that w satisfies the restriction to the right of the colon. Let",
            "paragraph_rank": 16,
            "section_rank": 4
        },
        {
            "section": "II. THE CONFIDENCE INTERVAL APPROACH BASED ON THE \u2206\u03c7 2 STATISTIC",
            "text": "that is, the value of ( sin 2 2\u03b8, |\u2206m 2 |, \u03b7 ) \u2208 \u0398 \u00d7 M \u00d7 H that best fits the data according to the deviation \u03c7 2 . Also, let",
            "paragraph_rank": 17,
            "section_rank": 4
        },
        {
            "section": "II. THE CONFIDENCE INTERVAL APPROACH BASED ON THE \u2206\u03c7 2 STATISTIC",
            "text": "And for any given (sin 2 2\u03b8, |\u2206m",
            "paragraph_rank": 18,
            "section_rank": 4
        },
        {
            "section": "II. THE CONFIDENCE INTERVAL APPROACH BASED ON THE \u2206\u03c7 2 STATISTIC",
            "text": "Then we can define a test statistic that reflects how much worse (sin 2 2\u03b8, |\u2206m 2 |) is than that of the best fit, namely,",
            "paragraph_rank": 19,
            "section_rank": 4
        },
        {
            "section": "II. THE CONFIDENCE INTERVAL APPROACH BASED ON THE \u2206\u03c7 2 STATISTIC",
            "text": "The corresponding CI with confidence level c is defined to be",
            "paragraph_rank": 20,
            "section_rank": 4
        },
        {
            "section": "II. THE CONFIDENCE INTERVAL APPROACH BASED ON THE \u2206\u03c7 2 STATISTIC",
            "text": "The term t c represents the threshold value such that,",
            "paragraph_rank": 21,
            "section_rank": 4
        },
        {
            "section": "II. THE CONFIDENCE INTERVAL APPROACH BASED ON THE \u2206\u03c7 2 STATISTIC",
            "text": "The key in constructing a CI is to specify t c correctly for a given confidence level c.",
            "paragraph_rank": 22,
            "section_rank": 4
        },
        {
            "section": "II. THE CONFIDENCE INTERVAL APPROACH BASED ON THE \u2206\u03c7 2 STATISTIC",
            "text": "Most commonly examined confidence levels use c = 68.3% (1\u03c3), 95.5% (2\u03c3), 99.7% (3\u03c3), which are often linked to threshold values t c = 2.31, 5.99, 11.8, respectively [8]. Note that these three values are the 68.3%, 95.5% and 99.7% quantiles of the Chi-square distribution with two degrees of freedom, respectively. The reason why these threshold values are used is that, CI c is indeed constructed upon screening the entire param-eter space by inspecting one point at a time, denoted by (sin 2 2\u03b8 1 , |\u2206m 2 1 |), and testing the pair of hypotheses, H 0 : (sin 2 2\u03b8, |\u2206m 2 |) = (sin 2 2\u03b8 1 , |\u2206m 2 1 |) versus H 1 : otherwise. To test the above hypotheses using the Chisquare statistic \u2206\u03c7 2 in Eq. (7), the full parameter space for (sin 2 2\u03b8, |\u2206m 2 |, \u03b7) is \u0398 \u00d7 M \u00d7 H, and the null hypothesis space is {(sin 2 2\u03b8 1 , |\u2206m 2 1 |)} \u00d7 H. According to the Wilks' theorem [9], if certain regularity conditions hold, mainly C1. the full parameter space \u0398 \u00d7 M \u00d7 H is a continuous space, and the the model likelihood function is a smooth function (for example three times differen-tiable) in the parameters, C2. the full parameter space contains an open neighborhood around the true value (sin 2 2\u03b8 1 , |\u2206m 2 1 |, \u03b7 1 ), and C3. the data size N i is large for each i = 1, . . . , n, then the statistic \u2206\u03c7 2 (sin 2 2\u03b8 1 , |\u2206m 2 1 |; X) follows approximately a Chi-square distribution when X is data generated from H 0 . Further, the degree of freedom of this Chi-square distribution equals the difference between the dimension of the full parameter space and that of the null hypothesis space, namely 2, in the current case. This procedure of constructing CIs and its extensions have been successfully applied in many studies in order to constrain various parameters in the field of neutrino physics (e.g. Ref. [18]).",
            "paragraph_rank": 23,
            "section_rank": 4,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b7",
                    "start": 165,
                    "text": "[8]",
                    "end": 168
                },
                {
                    "type": "bibr",
                    "ref_id": "b1",
                    "start": 504,
                    "text": "2",
                    "end": 505
                },
                {
                    "type": "bibr",
                    "ref_id": "b1",
                    "start": 594,
                    "text": "2",
                    "end": 595
                },
                {
                    "type": "bibr",
                    "ref_id": "b1",
                    "start": 819,
                    "text": "2",
                    "end": 820
                },
                {
                    "type": "bibr",
                    "ref_id": "b8",
                    "start": 864,
                    "text": "[9]",
                    "end": 867
                },
                {
                    "type": "bibr",
                    "ref_id": "b1",
                    "start": 1314,
                    "text": "2",
                    "end": 1315
                },
                {
                    "type": "bibr",
                    "ref_id": "b17",
                    "start": 1799,
                    "text": "[18]",
                    "end": 1803
                }
            ]
        },
        {
            "section": "II. THE CONFIDENCE INTERVAL APPROACH BASED ON THE \u2206\u03c7 2 STATISTIC",
            "text": "Although the above Wilks' CI method has been widely used in analyzing experimental data, it does not always produce CIs that have correct coverage. Its limitations have been addressed by, for example, Feldman and Cousins [6]. One example is the searches for neutrino oscillations in the disappearance mode. The oscillation probability with (sin 2 2\u03b8, |\u2206m 2 |) in a 2-flavor framework is written as:",
            "paragraph_rank": 24,
            "section_rank": 4,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b5",
                    "start": 221,
                    "text": "[6]",
                    "end": 224
                }
            ]
        },
        {
            "section": "II. THE CONFIDENCE INTERVAL APPROACH BASED ON THE \u2206\u03c7 2 STATISTIC",
            "text": "where L and E \u03bd i are the distance neutrino travels and the neutrino energy at the i-th bin, respectively. Then the mean bin counts",
            "paragraph_rank": 25,
            "section_rank": 4
        },
        {
            "section": "II. THE CONFIDENCE INTERVAL APPROACH BASED ON THE \u2206\u03c7 2 STATISTIC",
            "text": ", where a i and b i are coefficients that depend on the vector of nuisance parameters \u03b7, and m represents the amount of accumulated data (e.g. the elapsed time for data collection).",
            "paragraph_rank": 26,
            "section_rank": 4
        },
        {
            "section": "II. THE CONFIDENCE INTERVAL APPROACH BASED ON THE \u2206\u03c7 2 STATISTIC",
            "text": "The reason why the Wilks' CI method fails for the above neutrino oscillations example is the following. A key middle step in the proof of the Wilks' theorem is that conditions C1\u22123 together ensure that the estimator of (sin 2 2\u03b8, |\u2206m 2 |) based on minimizing \u03c7 2 , has a distribution close to a Gaussian distribution. This suggests two cases. (1) When testing a hypothesis H of the form: sin 2 2\u03b8 = 0 for any value of |\u2206m 2 |, C2 is violated, hence the Wilks' theorem does not apply no matter how large the data size is. (2) When testing hypotheses of all other forms, C1 and C2 are both satisfied, hence as the sample size grows to infinity, the distribution of \u2206\u03c7 2 will eventually converge to a Chi-square distribution. However, for instance if the true sin 2 2\u03b8 is close to 0, then there could be a non-ignorable probability that we observe a data set that results in sin 2 2\u03b8 min = 0. This clearly prevents the distribution of (sin 2 2\u03b8 min , |\u2206m 2 min |) from being closely approximated by a Gaussian distribution. Indeed, the closer sin 2 2\u03b8 0 is to 0, the larger the data size is needed to overcome the above phenomena.",
            "paragraph_rank": 27,
            "section_rank": 4,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b1",
                    "start": 952,
                    "text": "2",
                    "end": 953
                }
            ]
        },
        {
            "section": "II. THE CONFIDENCE INTERVAL APPROACH BASED ON THE \u2206\u03c7 2 STATISTIC",
            "text": "The latter point can also be understood intuitively. The parameter space of sin 2 2\u03b8 vs. |\u2206m 2 |, as is usually displayed in Fig. 1a, is uniform. But the effective parameter space of (sin 2 \u03b8, |\u2206m 2 |), in which the distance between any two points is measured by \u03c7 2 defined in Eq. 3, is no longer uniform (Fig. 1b). Due to the functional form of the oscillation formula, the effective parameter space becomes more compact at smaller sin 2 2\u03b8, as the differences between spectra with different values of |\u2206m 2 | become smaller. Therefore, more data is needed to reach the large data limit required by the Wilks' theorem in order to maintain the open neighborhood around the true parameter values (regularity condition C2). For example, the true sin 2 2\u03b8 = 0 hypothesis does not have an open neighborhood, as sin 2 2\u03b8 < 0 is not allowed. It is therefore impossible to reach the large data limit. Even for non-zero but small true value of sin 2 2\u03b8, the required data size could be well beyond the experimental reach.",
            "paragraph_rank": 28,
            "section_rank": 4,
            "ref_spans": [
                {
                    "type": "figure",
                    "ref_id": "fig_1",
                    "start": 125,
                    "text": "Fig. 1a",
                    "end": 132
                },
                {
                    "start": 282,
                    "text": "3",
                    "end": 283
                },
                {
                    "type": "figure",
                    "ref_id": "fig_1",
                    "start": 306,
                    "text": "(Fig. 1b)",
                    "end": 315
                }
            ]
        },
        {
            "section": "II. THE CONFIDENCE INTERVAL APPROACH BASED ON THE \u2206\u03c7 2 STATISTIC",
            "text": "When these regularity conditions are not satisfied, there are instances when the parent distribution of \u2206\u03c7 2 can have simple approximations that are not necessarily Chi-square. See, for e.g. Ref. [12,Sec. 3], where the parameter \u03b2 has dimension 1. For more general cases, one needs the MC method to set CIs. Below, we review how to produce a valid 1-\u03c3 (68%) CI of (sin 2 2\u03b8, |\u2206m 2 |) using MC, which can be easily generalized to build CIs of any level.",
            "paragraph_rank": 29,
            "section_rank": 4,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b11",
                    "start": 196,
                    "text": "[12,",
                    "end": 200
                },
                {
                    "type": "bibr",
                    "start": 200,
                    "text": "Sec. 3]",
                    "end": 207
                }
            ]
        },
        {
            "section": "II. THE CONFIDENCE INTERVAL APPROACH BASED ON THE \u2206\u03c7 2 STATISTIC",
            "text": "Having observed data ",
            "paragraph_rank": 30,
            "section_rank": 4
        },
        {
            "section": "II. THE CONFIDENCE INTERVAL APPROACH BASED ON THE \u2206\u03c7 2 STATISTIC",
            "text": "n } is generated from the model with true parameter value (sin 2 2\u03b8, |\u2206m 2 |).",
            "paragraph_rank": 31,
            "section_rank": 4
        },
        {
            "section": "II. THE CONFIDENCE INTERVAL APPROACH BASED ON THE \u2206\u03c7 2 STATISTIC",
            "text": "Here, the nuisance parameters can be either randomly generated according to the common hybrid Bayesian/Frequentist approach [19] or fixed at the best-fit values from data according to the full Frequentist approach [7,20].",
            "paragraph_rank": 32,
            "section_rank": 4,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b18",
                    "start": 124,
                    "text": "[19]",
                    "end": 128
                },
                {
                    "type": "bibr",
                    "ref_id": "b6",
                    "start": 214,
                    "text": "[7,",
                    "end": 217
                },
                {
                    "type": "bibr",
                    "ref_id": "b19",
                    "start": 217,
                    "text": "20]",
                    "end": 220
                }
            ]
        },
        {
            "section": "II. THE CONFIDENCE INTERVAL APPROACH BASED ON THE \u2206\u03c7 2 STATISTIC",
            "text": "For j = 1, . . . , T , calculate \u2206\u03c7 2 (sin 2 2\u03b8, |\u2206m 2 |; x (j) ). This produces an empirical distribution of the statistic \u2206\u03c7 2 .",
            "paragraph_rank": 33,
            "section_rank": 4,
            "ref_spans": [
                {
                    "type": "bibr",
                    "start": 60,
                    "text": "(j)",
                    "end": 63
                }
            ]
        },
        {
            "text": "Calculate the percentage of MC samples such that",
            "section_rank": 5
        },
        {
            "section": "Calculate the percentage of MC samples such that",
            "text": "is included in the 1-\u03c3 CI if and only if the percentage is smaller than 68%.",
            "paragraph_rank": 34,
            "section_rank": 5
        },
        {
            "section": "Calculate the percentage of MC samples such that",
            "text": "The key of the above procedure is to generate an empirical distribution of \u2206\u03c7 2 , which is not necessarily close to a Chi-square distribution. Unlike the Wilks' CI method, the MC CI method guarantees the validity of the resulting CIs when the MC sample size is large. However, the procedure can be very time-consuming when the dimension of the vector of unknown parameters is high and/or when a fine grid of the parameter space needs to be examined. In addition, the number of MC samples needed to produce an empirical distribution that leads to an accurate enough CI increases quickly as the required confidence level increases. The procedure can become prohibitively expensive if the minimization process used to find (sin 2 2\u03b8 min , |\u2206m 2 min |) is slow due to the existence of many nuisance parameters or other technical difficulties.",
            "paragraph_rank": 35,
            "section_rank": 5,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b1",
                    "start": 740,
                    "text": "2",
                    "end": 741
                }
            ]
        },
        {
            "section": "Calculate the percentage of MC samples such that",
            "text": "Furthermore, there is no simple recipe to strictly combine the CIs generated with the MC CI method from different experiments to form an overall CI. To see this, consider an example where several experiments are carried out to probe the parameter space of (sin 2 2\u03b8, |\u2206m 2 |). For any space point (sin 2 2\u03b8, |\u2206m 2 |), the \u2206\u03c7 2 statistic of the jth experiment is given by \u03c7 2 (sin 2 2\u03b8, |\u2206m 2 min | (j) ). Note that the minimum-value parameter space point, (sin 2 2\u03b8 min , |\u2206m 2 min |), based on different experiments are typically different. Once the experiments are combined, a strict implementation of the MC CI method requires to know the global minimumvalue parameter space point, which is in general unattainable. Indeed, one has to redo MC simulations for the combined data, which is expensive in computation since minimization has to be done for each MC sample.",
            "paragraph_rank": 36,
            "section_rank": 5,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b1",
                    "start": 390,
                    "text": "2",
                    "end": 391
                },
                {
                    "type": "bibr",
                    "ref_id": "b1",
                    "start": 476,
                    "text": "2",
                    "end": 477
                }
            ]
        },
        {
            "section": "Calculate the percentage of MC samples such that",
            "text": "In the next section, we introduce a different test statistic from \u2206\u03c7 2 , which allows for a simple approximation to its distribution under mild conditions. Using this new test statistic helps circumvent the computational problems mentioned above.",
            "paragraph_rank": 37,
            "section_rank": 5
        },
        {
            "text": "III. THE \u2206T STATISTIC",
            "section_rank": 6
        },
        {
            "text": "A. Non-nested hypotheses testing",
            "section_rank": 7
        },
        {
            "section": "A. Non-nested hypotheses testing",
            "text": "Recall that we used \u03b2 and \u03b7 to denote the parameter of interest and the nuisance parameter respec-tively. The corresponding model has mean bin counts",
            "paragraph_rank": 38,
            "section_rank": 7
        },
        {
            "section": "A. Non-nested hypotheses testing",
            "text": "Let B denote the parameter space for \u03b2. In this section, we consider pairs of nonnested hypotheses H 0 : \u03b2 = \u03b2 0 and H 1 : \u03b2 = \u03b2 1 , one pair at a time, for any \u03b2 0 \u0338 = \u03b2 1 \u2208 B. For convenience and clarity, we update some of our notations and refer to the nuisance parameter under H 0 and H 1 as \u03b7 and \u03b6 respectively, and they can be of different dimensions. Also, we refer to the mean bin counts associated with \u03b2 0 and \u03b2 1 as \u00b5 and \u03bd respectively, that is, the mean count of the ith bin is",
            "paragraph_rank": 39,
            "section_rank": 7
        },
        {
            "section": "A. Non-nested hypotheses testing",
            "text": "We now introduce a test statistic, denoted by \u2206T (\u03b2 0 , \u03b2 1 ; x), or simply \u2206T (x), for testing H 0 versus H 1 . More than one version of the definition of \u2206T will be listed below.",
            "paragraph_rank": 40,
            "section_rank": 7
        },
        {
            "section": "A. Non-nested hypotheses testing",
            "text": "We start with either the Poisson or the Normal distribution to model the data x, and use the general notation L(x, \u03bb) to denote the corresponding likelihood, where \u03bb equals to \u00b5(\u03b7) under H 0 , and \u03bd(\u03b6) under H 1 , respectively. Following the practice of Ref. [21, sec. 2], we convert 4 the likelihood functions under H 0 and H 1 into T H0 (\u03b7; x) and T H1 (\u03b7; x) respectively. Let",
            "paragraph_rank": 41,
            "section_rank": 7,
            "ref_spans": [
                {
                    "type": "bibr",
                    "start": 259,
                    "text": "[21, sec. 2]",
                    "end": 271
                }
            ]
        },
        {
            "section": "A. Non-nested hypotheses testing",
            "text": "and define",
            "paragraph_rank": 42,
            "section_rank": 7
        },
        {
            "section": "A. Non-nested hypotheses testing",
            "text": ", and (11)",
            "paragraph_rank": 43,
            "section_rank": 7
        },
        {
            "section": "A. Non-nested hypotheses testing",
            "text": "both of which can be interpreted as likelihood ratios. Take the Poisson model for example, we have",
            "paragraph_rank": 44,
            "section_rank": 7
        },
        {
            "section": "A. Non-nested hypotheses testing",
            "text": "and",
            "paragraph_rank": 45,
            "section_rank": 7
        },
        {
            "section": "A. Non-nested hypotheses testing",
            "text": "Then, looking at the definition of T H0 for instance, we have",
            "paragraph_rank": 46,
            "section_rank": 7
        },
        {
            "section": "A. Non-nested hypotheses testing",
            "text": "In practice, when there are prior experiments carried out to study the nuisance parameters, an additional term that reflects deviation from this prior knowledge is added to the definition of T H0 (\u03b7; x). We denote this term by \u03c7 2 penalty (\u03b7), an example of which is the term",
            "paragraph_rank": 47,
            "section_rank": 7
        },
        {
            "section": "A. Non-nested hypotheses testing",
            "text": "in Eq. 3. And when the data size is large, terms of smaller order are sometimes omitted from the definition of T H0 (\u03b7; x). There are at least four common variations for T H0 (\u03b7; x) used in practice:",
            "paragraph_rank": 48,
            "section_rank": 7,
            "ref_spans": [
                {
                    "start": 7,
                    "text": "3",
                    "end": 8
                }
            ]
        },
        {
            "section": "A. Non-nested hypotheses testing",
            "text": "Here, Eq. 15 . We can define four versions of T H1 (\u03b6; x) similarly. En route to form the test statistic \u2206T , T H0 (\u03b7; x) and T H1 (\u03b6; x) are further minimized over all nuisance parameters, to obtain T min H0 (x) = min \u03b7 T H0 (\u03b7; x) and T min H1 (x) = min \u03b6 T H1 (\u03b6; x), respectively. Finally, we define the test statistic",
            "paragraph_rank": 49,
            "section_rank": 7,
            "ref_spans": [
                {
                    "ref_id": "formula_24",
                    "start": 10,
                    "text": "15",
                    "end": 12
                }
            ]
        },
        {
            "section": "A. Non-nested hypotheses testing",
            "text": "Note that \u2206T (x) has the interpretation of being a loglikelihood ratio test statistic (or certain variations of it, depending on which version of the definition of T H0 and 5 As summarized in Ref. [21], all the above estimators had a set of properties which the authors considered optimal. They called them \"best asymptotically normal\" (BAN) estimators. The versions of test statistics based directly on likelihood functions are considered superior due to their faster convergence to the limiting chi-square distributions.",
            "paragraph_rank": 50,
            "section_rank": 7,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b4",
                    "start": 173,
                    "text": "5",
                    "end": 174
                },
                {
                    "type": "bibr",
                    "ref_id": "b20",
                    "start": 197,
                    "text": "[21]",
                    "end": 201
                }
            ]
        },
        {
            "section": "A. Non-nested hypotheses testing",
            "text": "T H1 are used) between the two hypotheses. It is easy to see that a positive \u2206T (x) would favor H 0 , and a negative \u2206T (x) would favor H 1 . In addition, the absolute size of \u2206T (x) reflects how much one hypothesis is favored over the other. 6 Remark. We emphasize that \u2206T (x) is a different type of test statistic than \u2206\u03c7 2 (x) in Eq. 7. Specifically, \u2206T (x) involves the best fit under the restrictions H 0 and H 1 , respectively, while \u2206\u03c7 2 (x) involves the best fit under the restrictions H 0 and over the full parameter space, respectively. The way \u2206T is defined is key to why there is a Gaussian approximation that works under very general setups, even in the cases where simple approximations for the conventional \u2206\u03c7 2 statistic fails. Nevertheless, we should note that, when the computing is affordable, forming CIs using \u2206\u03c7 2 is more desirable because it leads to a unified approach for setting limits in absence of new physics signals and in estimating parameters after the discovery of new physics [6].",
            "paragraph_rank": 51,
            "section_rank": 7,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b5",
                    "start": 243,
                    "text": "6",
                    "end": 244
                },
                {
                    "ref_id": "formula_6",
                    "start": 337,
                    "text": "7",
                    "end": 338
                },
                {
                    "type": "bibr",
                    "ref_id": "b5",
                    "start": 1010,
                    "text": "[6]",
                    "end": 1013
                }
            ]
        },
        {
            "section": "A. Non-nested hypotheses testing",
            "text": "Next, we introduce the concept of the Asimov data set [12]. Let x Asimov",
            "paragraph_rank": 52,
            "section_rank": 7,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b11",
                    "start": 54,
                    "text": "[12]",
                    "end": 58
                }
            ]
        },
        {
            "text": "H0",
            "section_rank": 8
        },
        {
            "section": "H0",
            "text": "denote the Asimov data set under H 0 , which is, loosely speaking, the mean counts corresponding to the true model in H 0 without any statistical fluctuation nor variations of systematics (change in nuisance parameters from their true value). In mathematical symbols, x Asimov",
            "paragraph_rank": 53,
            "section_rank": 8
        },
        {
            "text": "H0",
            "section_rank": 9
        },
        {
            "section": "H0",
            "text": "= \u00b5(\u03b7 0 ), where \u03b7 0 stands for the true value of the nuisance parameter. In practice, we do not know \u03b7 0 , so it is commonly approximated by an existing nominal value of the nuisance parameter (such as the term \u03b7 0 in Eq. 3).",
            "paragraph_rank": 54,
            "section_rank": 9,
            "ref_spans": [
                {
                    "start": 223,
                    "text": "3",
                    "end": 224
                }
            ]
        },
        {
            "section": "H0",
            "text": "Finally, we define a term that will help describe the distribution of the test statistic \u2206T under H 0 . Assuming that H 0 is the correct hypothesis, define",
            "paragraph_rank": 55,
            "section_rank": 9
        },
        {
            "section": "H0",
            "text": "where the last step holds because T min H0 (x Asimov",
            "paragraph_rank": 56,
            "section_rank": 9
        },
        {
            "text": "H0",
            "section_rank": 10
        },
        {
            "section": "H0",
            "text": ") = 0 by the definition of T min H0 and x Asimov",
            "paragraph_rank": 57,
            "section_rank": 10
        },
        {
            "text": "H0",
            "section_rank": 11
        },
        {
            "section": "H0",
            "text": ":= \u00b5(\u03b7 0 ). Analogously, let x Asimov",
            "paragraph_rank": 58,
            "section_rank": 11
        },
        {
            "text": "H1",
            "section_rank": 12
        },
        {
            "section": "H1",
            "text": "= \u03bd(\u03b6 0 ) denote the Asimov data set under H 1 , where we can approximate \u03b6 0 by an existing nominal value. Then the following term will help describe the distribution of the test statistic \u2206T , had H 1 been the correct hypothesis:",
            "paragraph_rank": 59,
            "section_rank": 12
        },
        {
            "text": "H1",
            "section_rank": 13
        },
        {
            "section": "H1",
            "text": ").",
            "paragraph_rank": 60,
            "section_rank": 13
        },
        {
            "text": "B. A Gaussian Approximation to the Distribution of \u2206T (X) with Large Data Size",
            "section_rank": 14
        },
        {
            "section": "B. A Gaussian Approximation to the Distribution of \u2206T (X) with Large Data Size",
            "text": "In this section, we show that by omitting terms of relatively small orders, the distribution of \u2206T (X) under hypothesis H follows approximately a Gaussian distribution with mean \u2206T H and standard deviation 2",
            "paragraph_rank": 61,
            "section_rank": 14
        },
        {
            "text": "\u221a",
            "section_rank": 15
        },
        {
            "section": "\u221a",
            "text": "|\u2206T H |, where H could be either H 0 or H 1 .",
            "paragraph_rank": 62,
            "section_rank": 15
        },
        {
            "text": "Description of the mathematical problem and notations",
            "section_rank": 16
        },
        {
            "section": "Description of the mathematical problem and notations",
            "text": "Recall that we defined four versions of (T H0 (\u03b7; x), T H1 (\u03b6; x)) that yield four different definitions of the test statistic \u2206T (X). In this section, we focus on studying \u2206T (X) based on Eq. (17), namely the Pearson Chi-square statistic. For clarity, we call it D(X) from here on. The main part of Sec. III B 2 will be devoted to develop a Gaussian approximation for the distribution of D(X) under H 0 . And in the remarks in the end of Sec. III B 2, we show that under H 0 , the differences between the other three versions of \u2206T (X) to D(X) are insignificant under fairly general conditions, so the approximate distribution derived for D(X) can also be used for all the different versions of \u2206T (X). Note that due to the symmetry between H 0 and H 1 , the aforementioned result also applies to D(X) and its variations under H 1 .",
            "paragraph_rank": 63,
            "section_rank": 16
        },
        {
            "section": "Description of the mathematical problem and notations",
            "text": "The mathematical problem concerning D(X) is the following. Let",
            "paragraph_rank": 64,
            "section_rank": 16
        },
        {
            "section": "Description of the mathematical problem and notations",
            "text": "and let\u03b7",
            "paragraph_rank": 65,
            "section_rank": 16
        },
        {
            "section": "Description of the mathematical problem and notations",
            "text": "Then the definition of D(X) is",
            "paragraph_rank": 66,
            "section_rank": 16
        },
        {
            "section": "Description of the mathematical problem and notations",
            "text": "Note that",
            "paragraph_rank": 67,
            "section_rank": 16
        },
        {
            "section": "Description of the mathematical problem and notations",
            "text": "that is based on Eq. 17. Our goal is to obtain an approximation of the distribution of D(X) under H 0 , when the data size is large. Hence a specific quantity, say m, is needed to reflect the magnitude of the data, in order that we can describe how other quantities in the model change along with it. For example, m could be the duration of the experiment or the total number of events. For the ease of description, let m represent the duration of the experiment in this section. Then p = X m stands for the per unit time observed counts in a potential experiment, and it would remain stable (instead of tending to infinity or zero) as m grows. So we say p is of order O p (1) (with respect to m) 7 .",
            "paragraph_rank": 68,
            "section_rank": 16,
            "ref_spans": [
                {
                    "ref_id": "formula_26",
                    "start": 21,
                    "text": "17",
                    "end": 23
                },
                {
                    "type": "bibr",
                    "ref_id": "b6",
                    "start": 697,
                    "text": "7",
                    "end": 698
                }
            ]
        },
        {
            "section": "Description of the mathematical problem and notations",
            "text": "In order to describe the modeling of counts rigorously, we introduce a set of notations, a summary of which is provided in Table I. Recall that when H 0 is the correct hypothesis, we employed \u00b5(\u03b7) to denote the mean bin counts for models under this hypothesis, where \u03b7 is the vector of unknown nuisance parameters of dimension q. Denote the true value of \u03b7 by \u03b7 0 , that is, \u00b5 0 = \u00b5(\u03b7 0 ) is the true mean counts of the observation such that",
            "paragraph_rank": 69,
            "section_rank": 16,
            "ref_spans": [
                {
                    "type": "table",
                    "start": 123,
                    "text": "Table I",
                    "end": 130
                }
            ]
        },
        {
            "section": "Description of the mathematical problem and notations",
            "text": "When the data size is large, a very good approximation to the model above is given by",
            "paragraph_rank": 70,
            "section_rank": 16
        },
        {
            "section": "Description of the mathematical problem and notations",
            "text": "Further, let \u03c0 := \u00b5/m denote the per unit time mean counts. To help explain these notations, take the example from Sec. IV B for instance, if H 0 : (sin 2 2\u03b8, |\u2206m",
            "paragraph_rank": 71,
            "section_rank": 16
        },
        {
            "section": "Description of the mathematical problem and notations",
            "text": "The terms a i and b i are functions of order O(1), and are determined by the con-figuration of the experiment. For example, a i can represent the detector efficiency, neutrino flux from reactor, target mass, etc., b i can represent the backgrounds. Also,",
            "paragraph_rank": 72,
            "section_rank": 16
        },
        {
            "section": "Description of the mathematical problem and notations",
            "text": "represents the survival probability in a disappearance model.",
            "paragraph_rank": 73,
            "section_rank": 16
        },
        {
            "section": "Description of the mathematical problem and notations",
            "text": "Meanwhile, a competing framework, namely the collection of models that satisfy H 1 , specifies the mean counts incorrectly as \u03bd(\u03b6), where \u03b6 is the unknown nuisance parameter of dimension q * . Also, define the per unit time mean counts under H 1 by \u03c4 = \u03bd/m. When H 0 is the correct hypothesis and that the true model is \u00b5 0 , there exists a unique \u03b6 0 , such that\u03b6 approaches \u03b6 0 as m \u2192 \u221e. We will show in Appendix A that \u03b6 0 has the interpretation that it corresponds to the model \u03bd(\u03b6) among all that belong to the alternative framework that is the closest to the true model \u00b5 0 in terms of the deviation",
            "paragraph_rank": 74,
            "section_rank": 16
        },
        {
            "section": "Description of the mathematical problem and notations",
            "text": ". Denote \u03bd 0 = \u03bd(\u03b6 0 ).",
            "paragraph_rank": 75,
            "section_rank": 16
        },
        {
            "text": "Approximating the distribution of the test statistic D(X)",
            "section_rank": 17
        },
        {
            "section": "Approximating the distribution of the test statistic D(X)",
            "text": "In this section, we always assume that H 0 is the correct hypothesis, under which we study the distribution of D(X) defined in Eq. (24). For convenience, we will suppress the dependence on X in the notation, and write D = \u03c7 2 H1 (\u03b6) \u2212 \u03c7 2 H0 (\u03b7). On one hand, it's well known that the distribution of \u03c7 2 H0 (\u03b7) approaches the Chi-square distribution with degree of freedom (n \u2212 q) as m increases. On the other hand, the limiting distribution of \u03c7 2 H1 (\u03b6) as m increases does not always exist. Indeed, the behavior of \u03c7 2 H1 (\u03b6) for large m is dependent on how far apart the mean counts of the best model under the alternative theoretical frameworks are from that of the true model. Denote the difference of per unit mean counts between the two models by \u03b4 = \u03c0 0 \u2212 \u03c4 0 . First, we state a classical assumption made in many statistical literatures (such as Ref. [10] and Ref. [12]) in order to obtain the limiting distribution of test statistics analogous to \u03c7 2 H1 (\u03b6), that is, the different versions of T min H1 (X): ",
            "paragraph_rank": 76,
            "section_rank": 17,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b23",
                    "start": 131,
                    "text": "(24)",
                    "end": 135
                },
                {
                    "type": "bibr",
                    "ref_id": "b9",
                    "start": 862,
                    "text": "[10]",
                    "end": 866
                },
                {
                    "type": "bibr",
                    "ref_id": "b11",
                    "start": 876,
                    "text": "[12]",
                    "end": 880
                }
            ]
        },
        {
            "section": "Approximating the distribution of the test statistic D(X)",
            "text": "2 ) terms are neglected. In contrast to [A1], we consider the following assumption, which is more general and realistic for the physics problem at hand:",
            "paragraph_rank": 77,
            "section_rank": 17
        },
        {
            "section": "Approximating the distribution of the test statistic D(X)",
            "text": "In words, [A0] assumes that the difference in mean bin counts between the best model under the wrong hypothesis and the true model increases at the same rate as the data size m, or slower.  probability. Take the likelihood ratio test statistic mentioned above for example, the non-centrality parameter in the previous approximation grows to infinity as m increases. Further, the differences between the different versions of \u03c7 2 H1 (\u03b6) usually do not converge to 0 as m increases.",
            "paragraph_rank": 78,
            "section_rank": 17
        },
        {
            "section": "Approximating the distribution of the test statistic D(X)",
            "text": "Although the limiting distribution does not necessarily exist under assumption [A0], it is still possible to approximate the distribution of \u03c7 2 H1 (\u03b6) at a finite, but large enough m. We make such an attempt, but this certainly requires a different derivation than the existing proofs that assume [A1]. In our derivation, we keep track of the terms that have higher order than constants when the data size m grows. Our proof follows the lines of that of Ref. [23,Chap. 16], but with significant modifications. Write",
            "paragraph_rank": 79,
            "section_rank": 17,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b22",
                    "start": 460,
                    "text": "[23,",
                    "end": 464
                },
                {
                    "type": "bibr",
                    "start": 464,
                    "text": "Chap. 16]",
                    "end": 473
                }
            ]
        },
        {
            "section": "Approximating the distribution of the test statistic D(X)",
            "text": "Here",
            "paragraph_rank": 80,
            "section_rank": 17
        },
        {
            "section": "Approximating the distribution of the test statistic D(X)",
            "text": ", and ",
            "paragraph_rank": 81,
            "section_rank": 17
        },
        {
            "section": "Approximating the distribution of the test statistic D(X)",
            "text": "where the three terms in the above expression are of order",
            "paragraph_rank": 82,
            "section_rank": 17
        },
        {
            "section": "Approximating the distribution of the test statistic D(X)",
            "text": "where",
            "paragraph_rank": 83,
            "section_rank": 17
        },
        {
            "section": "Approximating the distribution of the test statistic D(X)",
            "text": ", and the two terms in the above expression are of order O p (1) and O p (m \u2212 1 2 ) respectively. Therefore",
            "paragraph_rank": 84,
            "section_rank": 17,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b0",
                    "start": 61,
                    "text": "(1)",
                    "end": 64
                }
            ]
        },
        {
            "section": "Approximating the distribution of the test statistic D(X)",
            "text": "According to Eq. (40) of Lemma 1, the term in the closed bracket above reduces to 0. Hence",
            "paragraph_rank": 85,
            "section_rank": 17
        },
        {
            "section": "Approximating the distribution of the test statistic D(X)",
            "text": "Denote the first term of D by",
            "paragraph_rank": 86,
            "section_rank": 17
        },
        {
            "section": "Approximating the distribution of the test statistic D(X)",
            "text": "where the second to last equality follows from Appendix A.  ",
            "paragraph_rank": 87,
            "section_rank": 17
        },
        {
            "section": "Approximating the distribution of the test statistic D(X)",
            "text": "Remarks and Implications of Eq. 261. For the common physics problem that we are interested in, additional simplification can be made to the approximating distribution, N(D, 4D + 4ms). Specifically, in searching for new physics through precision measurements, the mean bin counts of the true model and that of the best model under the alternative hypothesis are relatively close to each other, that is,",
            "paragraph_rank": 88,
            "section_rank": 17,
            "ref_spans": [
                {
                    "ref_id": "formula_54",
                    "start": 32,
                    "text": "26",
                    "end": 34
                }
            ]
        },
        {
            "section": "Approximating the distribution of the test statistic D(X)",
            "text": "In such situations, one can ignore the ms term in Eq. (26), because ms = \u2211",
            "paragraph_rank": 89,
            "section_rank": 17
        },
        {
            "section": "Approximating the distribution of the test statistic D(X)",
            "text": "2. We claimed in Sec. IV A that, at large data limit, the three versions of \u2206T (X) = T min H1 (X) \u2212 T min H0 (X) based on the definition of T H0 (and the corresponding T H1 ) in Eq. 15, (16), and Eq. (18), each have negligible difference from the D(X). We validate this claim as follows.",
            "paragraph_rank": 90,
            "section_rank": 17,
            "ref_spans": [
                {
                    "ref_id": "formula_24",
                    "start": 182,
                    "text": "15",
                    "end": 184
                },
                {
                    "type": "bibr",
                    "ref_id": "b15",
                    "start": 186,
                    "text": "(16)",
                    "end": 190
                },
                {
                    "type": "bibr",
                    "ref_id": "b17",
                    "start": 200,
                    "text": "(18)",
                    "end": 204
                }
            ]
        },
        {
            "section": "Approximating the distribution of the test statistic D(X)",
            "text": "For the moment, we drop the penalty term \u03c7 2 penalty (\u03b7) from Eq. (15)- (18) for simplicity. And we will address the issue of the penalty term in the next remark.",
            "paragraph_rank": 91,
            "section_rank": 17,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b17",
                    "start": 72,
                    "text": "(18)",
                    "end": 76
                }
            ]
        },
        {
            "section": "Approximating the distribution of the test statistic D(X)",
            "text": "First, for Eq. (15), we have,",
            "paragraph_rank": 92,
            "section_rank": 17
        },
        {
            "section": "Approximating the distribution of the test statistic D(X)",
            "text": "The last step was obtained through expanding log",
            "paragraph_rank": 93,
            "section_rank": 17
        },
        {
            "section": "Approximating the distribution of the test statistic D(X)",
            "text": "2 )). Next, for Eq. (16), we have,",
            "paragraph_rank": 94,
            "section_rank": 17
        },
        {
            "section": "Approximating the distribution of the test statistic D(X)",
            "text": "Finally, for Eq. 18, we have,",
            "paragraph_rank": 95,
            "section_rank": 17,
            "ref_spans": [
                {
                    "ref_id": "formula_27",
                    "start": 17,
                    "text": "18",
                    "end": 19
                }
            ]
        },
        {
            "section": "Approximating the distribution of the test statistic D(X)",
            "text": "The differences between each version of T H0 (X) and \u2211",
            "paragraph_rank": 96,
            "section_rank": 17
        },
        {
            "section": "Approximating the distribution of the test statistic D(X)",
            "text": "are negligible. Next we examine the differences between each version of T H1 (X) and \u2211",
            "paragraph_rank": 97,
            "section_rank": 17
        },
        {
            "section": "Approximating the distribution of the test statistic D(X)",
            "text": ". We will only consider situations where condition Eq. (27) hold. If so, the term",
            "paragraph_rank": 98,
            "section_rank": 17,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b26",
                    "start": 55,
                    "text": "(27)",
                    "end": 59
                }
            ]
        },
        {
            "section": "Approximating the distribution of the test statistic D(X)",
            "text": ") << 1, which will help validate the following three approximations. First, for Eq. 15, we have,",
            "paragraph_rank": 99,
            "section_rank": 17,
            "ref_spans": [
                {
                    "ref_id": "formula_24",
                    "start": 84,
                    "text": "15",
                    "end": 86
                }
            ]
        },
        {
            "section": "Approximating the distribution of the test statistic D(X)",
            "text": "Next, for Eq. (16), we have,",
            "paragraph_rank": 100,
            "section_rank": 17
        },
        {
            "section": "Approximating the distribution of the test statistic D(X)",
            "text": ".",
            "paragraph_rank": 101,
            "section_rank": 17
        },
        {
            "section": "Approximating the distribution of the test statistic D(X)",
            "text": "Finally, for Eq. (18), we have,",
            "paragraph_rank": 102,
            "section_rank": 17
        },
        {
            "section": "Approximating the distribution of the test statistic D(X)",
            "text": "In situations where Eq. (27) is satisfied, the differences between each version of T H1 (X) and \u2211 ",
            "paragraph_rank": 103,
            "section_rank": 17
        },
        {
            "section": "Approximating the distribution of the test statistic D(X)",
            "text": "3. We emphasize that Eq. (24) is a specific form of T in Eq. (15), (16), (17), and Eq. (18). The penalty term in T represents the constraint of systematic uncertainties, and is commonly obtained by dedicated measurements. When one includes the dedicated measurements as part of Chi-square definition, one naturally recovers Eq. (24). Therefore, our proof in Sec. III B is also valid for test statistics with the format of T in Eq. (15), (16), (17), and Eq. (18).",
            "paragraph_rank": 104,
            "section_rank": 17,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b15",
                    "start": 67,
                    "text": "(16)",
                    "end": 71
                },
                {
                    "type": "bibr",
                    "ref_id": "b16",
                    "start": 73,
                    "text": "(17)",
                    "end": 77
                },
                {
                    "type": "bibr",
                    "ref_id": "b17",
                    "start": 87,
                    "text": "(18)",
                    "end": 91
                },
                {
                    "type": "bibr",
                    "ref_id": "b23",
                    "start": 328,
                    "text": "(24)",
                    "end": 332
                },
                {
                    "type": "bibr",
                    "ref_id": "b15",
                    "start": 437,
                    "text": "(16)",
                    "end": 441
                },
                {
                    "type": "bibr",
                    "ref_id": "b16",
                    "start": 443,
                    "text": "(17)",
                    "end": 447
                }
            ]
        },
        {
            "section": "Approximating the distribution of the test statistic D(X)",
            "text": "4. We comment on the large data limit, which is required to reach the final conclusion (Eq. (28)) and to show the equivalence of Eq. 15, (16), (17), and Eq. (18). For a single bin, Ni\u2212\u00b5i \u00b5i is negligible if N i is greater than about 100. For multiple bins, the contributions from each bin will likely cancel and the condition can be relaxed in practice to that the total number of events, \u2211 i N i , is greater than about 100.",
            "paragraph_rank": 105,
            "section_rank": 17,
            "ref_spans": [
                {
                    "ref_id": "formula_24",
                    "start": 133,
                    "text": "15",
                    "end": 135
                },
                {
                    "type": "bibr",
                    "ref_id": "b15",
                    "start": 137,
                    "text": "(16)",
                    "end": 141
                },
                {
                    "type": "bibr",
                    "ref_id": "b16",
                    "start": 143,
                    "text": "(17)",
                    "end": 147
                },
                {
                    "type": "bibr",
                    "ref_id": "b17",
                    "start": 157,
                    "text": "(18)",
                    "end": 161
                }
            ]
        },
        {
            "section": "Approximating the distribution of the test statistic D(X)",
            "text": "In summary of this section, we showed the following result. Assume the following set of conditions hold: ",
            "paragraph_rank": 106,
            "section_rank": 17
        },
        {
            "section": "Approximating the distribution of the test statistic D(X)",
            "text": "Then a simple approximation for the distribution of \u2206T (X) under H j , for either j = 0 or 1, is the Gaussian distribution with mean \u2206T Hj and standard deviations 2 \u221a",
            "paragraph_rank": 107,
            "section_rank": 17
        },
        {
            "section": "Approximating the distribution of the test statistic D(X)",
            "text": "|\u2206T Hj |. Based on the Gaussian approximation, the CL s value is easily calculated with \u2206T (x), \u2206T H0 , and \u2206T H1 . In case any of the above conditions (CD1-CD3) breaks down, the distribution of \u2206T (X) is not necessarily well approximated by the Gaussian distribution, and should instead be estimated through Monte Carlo simulations.",
            "paragraph_rank": 108,
            "section_rank": 17
        },
        {
            "text": "IV. THE CLs APPROACH BASED ON THE \u2206T STATISTIC",
            "section_rank": 18
        },
        {
            "section": "IV. THE CLs APPROACH BASED ON THE \u2206T STATISTIC",
            "text": "The \u2206T (x) statistic described in the previous section can be used to form both CIs and CL s , and they differ in how the associated p-values are utilized. Note that both procedures are easy to carry out because of the simple Gaussian approximation for the distribution of \u2206T (x). We will introduce the CL s approach with the \u2206T (x) statistic below. The principle of forming CIs with \u2206T (x) is the same as that with \u2206\u03c7 2 .",
            "paragraph_rank": 109,
            "section_rank": 18
        },
        {
            "text": "FIG. 2. (color online)",
            "section_rank": 19
        },
        {
            "section": "FIG. 2. (color online)",
            "text": "Illustration of the CLs approach with log-likelihood ratio \u2206T . In order to be consistent with the convention in Ref. [24], we plot the densities of \u2212\u2206T instead. See text for more discussions.",
            "paragraph_rank": 110,
            "section_rank": 19,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b23",
                    "start": 118,
                    "text": "[24]",
                    "end": 122
                }
            ]
        },
        {
            "text": "A. The CLs Approach Based on the \u2206T Statistic",
            "section_rank": 20
        },
        {
            "section": "A. The CLs Approach Based on the \u2206T Statistic",
            "text": "The CL s approach [13][14][15] is a popular approach to present searches for new physics beyond the Standard Model. Recent examples of using this approach in neutrino physics can be found in Ref. [25,26]. Examples of using this approach in LHC super particle search can be found at Ref. [27,28]. We emphasize that, the CL s approach is a different way to present statistical results than the traditional approach of setting confidence intervals (CI). The traditional CI approach is appropriate in treating established signals [15]. Whereas the CL s approach is appropriate in setting exclusion limits, such that models with parameter values beyond the limits are much worse than the Standard Model in fitting the observed data. In this section, we briefly review the principle of the CL s approach in a two-hypotheses testing problem. Fig. 2 is a heuristic illustration of the distribution of the log-likelihood ratio \u2206T (X), where X stands for data from a potential repeat of the experiment. The black (red) curve stands for the density function of the expected distribution of \u2206T (X) under the assumption that the null (alternative) hypothesis is true. The green line represents \u2206T (x) calculated from the observed data x. A positive (negative) \u2206T (x) would favor H 0 (H 1 ) over H 1 (H 0 ). The CL s value is then defined as:",
            "paragraph_rank": 111,
            "section_rank": 20,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b12",
                    "start": 18,
                    "text": "[13]",
                    "end": 22
                },
                {
                    "type": "bibr",
                    "ref_id": "b13",
                    "start": 22,
                    "text": "[14]",
                    "end": 26
                },
                {
                    "type": "bibr",
                    "ref_id": "b14",
                    "start": 26,
                    "text": "[15]",
                    "end": 30
                },
                {
                    "type": "bibr",
                    "ref_id": "b24",
                    "start": 196,
                    "text": "[25,",
                    "end": 200
                },
                {
                    "type": "bibr",
                    "ref_id": "b25",
                    "start": 200,
                    "text": "26]",
                    "end": 203
                },
                {
                    "type": "bibr",
                    "ref_id": "b26",
                    "start": 287,
                    "text": "[27,",
                    "end": 291
                },
                {
                    "type": "bibr",
                    "ref_id": "b27",
                    "start": 291,
                    "text": "28]",
                    "end": 294
                },
                {
                    "type": "bibr",
                    "ref_id": "b14",
                    "start": 526,
                    "text": "[15]",
                    "end": 530
                },
                {
                    "type": "figure",
                    "ref_id": "fig_7",
                    "start": 835,
                    "text": "Fig. 2",
                    "end": 841
                }
            ]
        },
        {
            "section": "A. The CLs Approach Based on the \u2206T Statistic",
            "text": "where 1 \u2212 p 1 (1 \u2212 p 0 ) is the probability that a potential repeat of the experiment will yield a \u2206T (X) value larger than \u2206T (x) when the alternate (null) hypothesis is true. Hence, the definition of CL s in Eq. (30) suggests that a CL s value close to zero would favor H 0 against H 1 . On the other hand, as illustrated in Fig. 3 space is typically defined as the set of parameter values of new physics that corresponds to CL s value smaller than \u03b1 = 0.05 [24], while other threshold values of the CL s can be used as well.",
            "paragraph_rank": 112,
            "section_rank": 20,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b30",
                    "start": 214,
                    "text": "(30)",
                    "end": 218
                },
                {
                    "type": "figure",
                    "start": 327,
                    "text": "Fig. 3",
                    "end": 333
                },
                {
                    "type": "bibr",
                    "ref_id": "b23",
                    "start": 460,
                    "text": "[24]",
                    "end": 464
                }
            ]
        },
        {
            "section": "A. The CLs Approach Based on the \u2206T Statistic",
            "text": "Note that the CL s value is never smaller than (1 \u2212 p 1 ), the p-value used in the corresponding CI approach. Hence, had the exclusion contour at \u03b1 been used to set a CI, it would have coverage probability over 1 \u2212 \u03b1. Nevertheless, the CL s value appears to be a more reasonable measure of extremeness than (1\u2212p 1 ), in situations where H 0 and H 1 are very similar (see Fig. 3). For example, assuming the data x is an \"extreme\" measurement with respect to H 1 (i.e. small p 1 ), it will also be disfavored by H 0 (i.e. small p 0 ). If only a single p-value, either p 0 or p 1 , is examined as in the CI approach, then one would draw the inappropriate conclusion of excluding H 0 or H 1 while favoring the other hypothesis. However, since the hypotheses H 0 and H 1 are similar, the data does not carry enough information to differentiate them. The CL s value, which is the ratio between 1 \u2212 p 1 and 1 \u2212 p 0 will protect against such situations.",
            "paragraph_rank": 113,
            "section_rank": 20,
            "ref_spans": [
                {
                    "type": "figure",
                    "start": 371,
                    "text": "Fig. 3",
                    "end": 377
                }
            ]
        },
        {
            "section": "A. The CLs Approach Based on the \u2206T Statistic",
            "text": "In order to obtain the value of p 0 and p 1 required to calculate the CL s , one needs to find the distribution of \u2206T (X) under H 0 and H 1 . While Monte Carlo simulations can provide approximations to the distribution of \u2206T (X), simpler methods, such as Gaussian approximations, are desired to lower the computing burden.",
            "paragraph_rank": 114,
            "section_rank": 20
        },
        {
            "text": "B. Setting Exclusion Sets with the Gaussian CLs Method",
            "section_rank": 21
        },
        {
            "section": "B. Setting Exclusion Sets with the Gaussian CLs Method",
            "text": "In this section, we illustrate the procedure of setting exclusion sets with the Gaussian CL s method for the neutrino oscillation example from Sec. II.",
            "paragraph_rank": 115,
            "section_rank": 21
        },
        {
            "section": "B. Setting Exclusion Sets with the Gaussian CLs Method",
            "text": "Here the parameter of interest is \u03b2 = (sin 2 2\u03b8, |\u2206m 2 |). The mean count for the ith bin is described as",
            "paragraph_rank": 116,
            "section_rank": 21
        },
        {
            "section": "B. Setting Exclusion Sets with the Gaussian CLs Method",
            "text": ", where a i and b i are coefficients that depend on the vector of nuisance parameters \u03b7, and m represents the amount of accumulated data. It is typical to use \u03b2 0 = (0, |\u2206m 2 0 |) as a reference parameter point, where |\u2206m 2 0 | can be any fixed value since it does not enter the model for bin counts when sin 2 2\u03b8 = 0. In this case, the null hypothesis is specified to be H 0 : \u03b2 = \u03b2 0 (i.e. the Standard Model with three light neutrinos). Next, for any \u03b2 1 = (sin 2 2\u03b8 1 , |\u2206m 2 1 |) from the parameter space \u0398 \u00d7 M , specify the alternative hypothesis to be H 1 : \u03b2 = \u03b2 1 , and perform the following procedure: ",
            "paragraph_rank": 117,
            "section_rank": 21,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b1",
                    "start": 478,
                    "text": "2",
                    "end": 479
                }
            ]
        },
        {
            "section": "B. Setting Exclusion Sets with the Gaussian CLs Method",
            "text": "where",
            "paragraph_rank": 118,
            "section_rank": 21
        },
        {
            "section": "B. Setting Exclusion Sets with the Gaussian CLs Method",
            "text": "dt is the Gaussian error function for any s \u2208 (\u2212\u221e, \u221e).",
            "paragraph_rank": 119,
            "section_rank": 21
        },
        {
            "text": "Similarly, from the Asimov data set x Asimov",
            "section_rank": 22
        },
        {
            "text": "H1",
            "section_rank": 23
        },
        {
            "section": "H1",
            "text": ", obtain",
            "paragraph_rank": 120,
            "section_rank": 23
        },
        {
            "text": "H1",
            "section_rank": 24
        },
        {
            "section": "H1",
            "text": ") .",
            "paragraph_rank": 121,
            "section_rank": 24
        },
        {
            "section": "H1",
            "text": "according to Eq. (21). Then one can approximate",
            "paragraph_rank": 122,
            "section_rank": 24,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b20",
                    "start": 17,
                    "text": "(21)",
                    "end": 21
                }
            ]
        },
        {
            "section": "H1",
            "text": "4. According to Eq. (30), the CL s value at (sin 2 2\u03b8 1 , |\u2206m 2 1 |) can be approximated by",
            "paragraph_rank": 123,
            "section_rank": 24
        },
        {
            "section": "H1",
            "text": "The point (sin 2 2\u03b8 1 , |\u2206m 2 1 |) is assigned to the 95% CL s exclusion set if and only if its CL s value is smaller than 5%.",
            "paragraph_rank": 124,
            "section_rank": 24
        },
        {
            "section": "H1",
            "text": "In terms of the computing effort, the above CL s procedure requires the calculation of \u2206T (x), \u2206T H1 , and \u2206T H0 at each parameter point in \u0398 \u00d7 M . In comparison, the standard Wilks CI method based on \u2206\u03c7 2 (x) in Eq. (7) requires the calculation of \u2206\u03c7 2 (x) at each parameter point. So the computing cost of the Gaussian CL s method is about three times that of the Wilks' CI method. In summary, both methods are easily affordable computationally, but the CL s method is valid under much less restrictive conditions (CD1-CD3).",
            "paragraph_rank": 125,
            "section_rank": 24
        },
        {
            "text": "V. AN EXAMPLE: SEARCH FOR STERILE NEUTRINO",
            "section_rank": 25
        },
        {
            "section": "V. AN EXAMPLE: SEARCH FOR STERILE NEUTRINO",
            "text": "In this section, we introduce an example based on the search for a sterile neutrino. In this example, various methods to carry out the CL s approach and the CI approach are compared.",
            "paragraph_rank": 126,
            "section_rank": 25
        },
        {
            "text": "A. Model Description",
            "section_rank": 26
        },
        {
            "section": "A. Model Description",
            "text": "In this model, there are two detectors and one neutrino source. One detector is located at 300 kilo-meters from the neutrino source and is called the near detector. The other detector is located at 1000 kilo-meters from the neutrino source and is called the far detector. As shown in Fig. 4, the neutrino energy E \u03bd covers from 1 GeV to 9 GeV, and a flat (energy independent) neutrino energy spectrum is assumed. We further assume the detector can measure the spectrum with 20 energy bins equally spaced between 1 GeV and 9 GeV. The mean number of neutrino events seen by the near (far) detector without any oscillation is 10 k (0.9 k) per bin. We consider two types of oscillation measurements: a disappearance measurement with oscillation formula",
            "paragraph_rank": 127,
            "section_rank": 26,
            "ref_spans": [
                {
                    "type": "figure",
                    "ref_id": "fig_9",
                    "start": 284,
                    "text": "Fig. 4",
                    "end": 290
                }
            ]
        },
        {
            "section": "A. Model Description",
            "text": "and an appearance measurement with oscillation formula",
            "paragraph_rank": 128,
            "section_rank": 26
        },
        {
            "section": "A. Model Description",
            "text": "where \u03b8 is the neutrino mixing angle, \u2206m 2 is the neutrino mass squared difference, and L is the distance that neutrino travels. We further include a background with a linear dependence on E \u03bd . The number of background events starts from 130 per bin for the first bin to 73 per bin for the last (20th) bin. There are three nuisance parameters, \u03f5, \u03b7 n , and \u03b7 f . The first one is associated with the detector efficiency and the neutrino flux, which is assumed to be accurate to 5%. This uncertainty is assumed to be correlated between the near and the far detectors. The second and the third nuisance parameters are associated with the background normalization factors for the near and the far detectors, respectively. The normalization uncertainty is assumed to be 2% and uncorrelated between the two detectors. Fig. 4 shows the expected neutrino spectra. For the disappearance measurement, we compare the nooscillation spectrum (the null hypothesis H 0 : sin 2 2\u03b8 = 0) with an oscillation spectrum (an alternative hypothesis H 1 : sin 2 2\u03b8 = 0.06 at \u2206m 2 = 2.5 \u00d7 10 \u22123 eV 2 ). For the appearance measurement, we compare the no-oscillation spectrum (the null hypothesis H 0 : sin 2 2\u03b8 = 0) with two oscillation spectra (two alternative hypotheses H 1 : sin 2 2\u03b8 = 0.008 or sin 2 2\u03b8 = 0.03 at \u2206m 2 = 2.5 \u00d7 10 \u22123 eV 2 ). Given a Monte Carlo (MC) sample N j i , we use the following test statistic based on the Poisson likelihood, in line of Eq. (15):",
            "paragraph_rank": 129,
            "section_rank": 26,
            "ref_spans": [
                {
                    "type": "figure",
                    "ref_id": "fig_9",
                    "start": 814,
                    "text": "Fig. 4",
                    "end": 820
                }
            ]
        },
        {
            "section": "A. Model Description",
            "text": "Here, i represents the bin number and ranges from 1 to 20. j labels the near or the far detector. \u00b5 j i is the mean number of events in i-th bin and j-th detector. It depends on the oscillation parameters: sin 2 2\u03b8 and \u2206m 2 , and the nuisance parameters: \u03f5 for the detector efficiency and neutrino flux, and \u03b7 n (\u03b7 f ) for the near (far) detector background normalization factors.",
            "paragraph_rank": 130,
            "section_rank": 26
        },
        {
            "text": "B. The Wilks' CI method vs. the MC CI method",
            "section_rank": 27
        },
        {
            "section": "B. The Wilks' CI method vs. the MC CI method",
            "text": "For the example mentioned above, the Wilks' method is unsuitable for setting CI for the parameter sin 2 2\u03b8 because the conditions required are not satisfied as stated in Sec. II. In comparison, the computationally intensive MC CI method need to be used to set CI in this example. The purpose of this section is to demonstrate the practical difference between the two methods. Here, we examine the distribution of the test statistic in Eq. (37) under the hypothesis H 0 : sin 2 2\u03b8 = 0, where the Wilks' method is especially problematic. To implement the MC CI method, we generate a large number of MC samples assuming that sin 2 2\u03b8 = 0. The MC samples have statistical fluctuations according to Poisson distributions, and systematic variations through randomizing the three nuisance parameters according to normal distributions. While the minimization process in calculating T min follows the Frequentist's approach, the randomization of the nuisance parameters corresponds to a Bayesian integral over the nuisance parameters. It is a common hybrid Bayesian/Frequentist approach [19]. As a comparison, we also tried a full Frequentist approach as illustrated in Ref. [7,20]. Results are very similar to that of the hybrid approach. In the latter approach, MCs are generated using the best-fit nuisance parameters obtained in analyzing the data under the sin 2 2\u03b8 = 0 hypothesis. For each MC sample, we find T min and T min H0 , where T min is the minimum value of T from Eq. (36) in the 5dimensional parameter space of (sin 2 2\u03b8, \u2206m 2 , \u03f5, \u03b7 n , \u03b7 f ), and T min H0 is the minimum value of T under the restriction, sin 2 2\u03b8 true = 0. Then we form the test statistic",
            "paragraph_rank": 131,
            "section_rank": 27,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b18",
                    "start": 1078,
                    "text": "[19]",
                    "end": 1082
                },
                {
                    "type": "bibr",
                    "ref_id": "b6",
                    "start": 1166,
                    "text": "[7,",
                    "end": 1169
                },
                {
                    "type": "bibr",
                    "ref_id": "b19",
                    "start": 1169,
                    "text": "20]",
                    "end": 1172
                }
            ]
        },
        {
            "section": "B. The Wilks' CI method vs. the MC CI method",
            "text": "Fig . 5 shows the distribution of \u2206\u03c7 2 , which clearly does not follow a Chi-square distribution with two degrees of freedom. In summary, for this example, the Wilks' method can not be used to correctly set CIs based on the test statistic \u2206\u03c7 2 . It is possible to explore alternative formula than that of the Wilks' method, if one takes the hybrid approach in Ref. [19] and finds an analytic approximation to the solution of t c for the equation Prob(\u2206\u03c7 2 \u2264 t c ) \u2265 c, where the probability is evaluated over the distribution of the nuisance parameters. Otherwise, one can always obtain the distribution of \u2206\u03c7 2 through the computationally intensive MC CI method.",
            "paragraph_rank": 132,
            "section_rank": 27,
            "ref_spans": [
                {
                    "type": "figure",
                    "start": 4,
                    "text": ". 5",
                    "end": 7
                },
                {
                    "type": "bibr",
                    "ref_id": "b18",
                    "start": 365,
                    "text": "[19]",
                    "end": 369
                }
            ]
        },
        {
            "text": "C. Validity of the Gaussian Approximation in the Gaussian CLs method",
            "section_rank": 28
        },
        {
            "section": "C. Validity of the Gaussian Approximation in the Gaussian CLs method",
            "text": "For the example in the previous section, there is no known way to set CI without computationally intensive MC simulations. This is the main motivation for using the Gaussian CL s method as an alternative. In this and \u2206m 2 = 2.5 \u00d7 10 \u22123 eV 2 . The histograms on the left (right) are made from the MC samples assuming H 1 (H 0 ) is true. We also compare them with the expected normal distribution N(\u2206T ,4\u2206T ) from the \u2206T H0 and \u2206T H1 calculated from the Asimov data sets. Good agreements are observed.",
            "paragraph_rank": 133,
            "section_rank": 28
        },
        {
            "section": "C. Validity of the Gaussian Approximation in the Gaussian CLs method",
            "text": "Similarly, we also check the appearance measurements. In Fig. 7, the null hypothesis H 0 corresponds to sin 2 2\u03b8 = 0, and the alternative hypothesis H 1 corresponds to (sin 2 2\u03b8, \u2206m 2 ) = (0.008, 2.5 \u00d7 10 \u22123 eV 2 ). In Fig. 8, H 0 corresponds to sin 2 2\u03b8 = 0, and H 1 corresponds to (sin 2 2\u03b8, \u2206m 2 ) = (0.03, 2.5 \u00d7 10 \u22123 eV 2 ). The agreement between the MCs and expectations in Fig. 7 is slightly worse than that in Fig. 6, but is still reasonably good. However, the difference between the MCs and expectations in Fig. 8 becomes large. This is because the third regularity condition CD3 \"when the prediction of two hypotheses (the null hypotheses H 0 and the alternative hypothesis H 1 are relatively close or |\u00b5 i \u2212 \u03bd i | << \u00b5 i \u223c \u03bd i \" is no longer met. In a disappearance search, CD3 can be easily satisfied. However, this may not be true in an appearance experiment as the mean number of signal events is zero when sin 2 2\u03b8 = 0. When H 0 and H 1 are sin 2 2\u03b8 = 0 and (sin 2 2\u03b8, \u2206m 2 ) = (0.008, 2.5 \u00d7 10 \u22123 eV 2 ), respectively, CD3 is still reasonably well satisfied with the existence of backgrounds. When H 0 and H 1 are sin 2 2\u03b8 = 0 and (sin 2 2\u03b8, \u2206m 2 ) = (0.03, 2.5 \u00d7 10 \u22123 eV 2 ), respectively, CD3 is severely violated. Note that in such situations where H 0 and H 1 are very different, the experimental data is most likely able to exclude one hypothesis easily, making it less interesting to carry out such a statistical test. Nevertheless, we emphasize it is crucial to validate Gaussian approximation with MCs in practice when any of CD1-CD3 listed in the end of Sec. III are marginally satisfied. Recall that the CL s approach is based on the test statistic \u2206T = T min H1 \u2212 T min H0 . Although typical methods to form CIs use a different type of test statistic, namely \u2206\u03c7 2 shown in the previous sections, one can in principle also set CIs based on \u2206T , which we refer to as the \u2206T -based CI method. Below, we use our example to compare the exclusion sets obtained from the Gaussian CL s method and the CIs obtained from the \u2206T -based CI method. For the former method, the exclusion set consists of parameter values that correspond to CL s values, specifically (1 \u2212 p 1 )/(1 \u2212 p 0 ), lower than 0.05; and for the latter method, the CI consists of parameter values that correspond to p-values, specifically (1 \u2212 p 1 ), over 0.05. The results are summarized in Fig. 9. In the example, the true sin 2 2\u03b8 is assumed to be zero. The sensitivity of the Gaussian CL s method is slightly worse than that of ",
            "paragraph_rank": 134,
            "section_rank": 28,
            "ref_spans": [
                {
                    "type": "figure",
                    "start": 57,
                    "text": "Fig. 7",
                    "end": 63
                },
                {
                    "type": "figure",
                    "start": 219,
                    "text": "Fig. 8",
                    "end": 225
                },
                {
                    "type": "figure",
                    "start": 380,
                    "text": "Fig. 7",
                    "end": 386
                },
                {
                    "type": "figure",
                    "start": 418,
                    "text": "Fig. 6",
                    "end": 424
                },
                {
                    "type": "figure",
                    "start": 516,
                    "text": "Fig. 8",
                    "end": 522
                },
                {
                    "type": "figure",
                    "ref_id": "fig_12",
                    "start": 2377,
                    "text": "Fig. 9",
                    "end": 2383
                }
            ]
        },
        {
            "section": "C. Validity of the Gaussian Approximation in the Gaussian CLs method",
            "text": "The true value of sin 2 2\u03b8 is 0. For the CI (CLs) method, the right side of the red (black) line has a p-value (CLs value) smaller than 0.05. The sensitivity curves are generated from a large number of Monte Carlo samples. At each \u2206m 2 , 50% (50%) of MC samples will have a better (worse) exclusion limit than the sensitivity curve.",
            "paragraph_rank": 135,
            "section_rank": 28
        },
        {
            "section": "C. Validity of the Gaussian Approximation in the Gaussian CLs method",
            "text": "\u2206T -based CI method, because the CL s value is by construction larger than the p-value used in the CI method. Despite the slightly worse sensitivity, the CL s produces smoother contours that agree better with intuition (not excluding hypotheses that are close to the null hypothesis) than the CI do. As shown in Fig. 9, the region that correspond to \u2206m 2 \u223c 5.5 \u00d7 10 \u22122 eV 2 and sin 2 2\u03b8 < 0.01 (also \u2206m 2 \u223c 0.1 eV 2 and sin 2 2\u03b8 < 0.01) is excluded from the 95% CI. This is inconsistent with intuition as the expected spectrum for small sin 2 2\u03b8 values should be very similar to that of sin 2 2\u03b8 = 0, and we do not expect to exclude regions with small sin 2 2\u03b8 values. This phenomenon can be understood as follows. With the test statistic \u2206T , we compare two hypotheses each time. Therefore, even when the two hypotheses are very similar, the chance of excluding one hypothesis with CI can still be large as illustrated in Fig. 3. As we explained in Sec. IV A, the definition of the CL s value avoid this problem, giving it an advantage over the traditional CI when test statistic \u2206T is used.",
            "paragraph_rank": 136,
            "section_rank": 28,
            "ref_spans": [
                {
                    "type": "figure",
                    "ref_id": "fig_12",
                    "start": 312,
                    "text": "Fig. 9",
                    "end": 318
                },
                {
                    "type": "figure",
                    "start": 923,
                    "text": "Fig. 3",
                    "end": 929
                }
            ]
        },
        {
            "text": "E. The Gaussian CLs method vs. the MC CI method vs. the Raster-Scan MC CI method",
            "section_rank": 29
        },
        {
            "section": "E. The Gaussian CLs method vs. the MC CI method vs. the Raster-Scan MC CI method",
            "text": "The statistical interpretation of (the complement of) exclusion sets obtained using the CL s method is distinct from that of CIs. Indeed, if an exclusion contour based on thresholding the CL s value at \u03b1 is used to set a CI, its coverage probability will be over 1 \u2212 \u03b1. Nevertheless, it is still interesting to compare these two kinds of sets in specific physics problems, as seen in many literatures (for example, Ref. [15] and Ref. [29]). Below, we perform such a comparison under the set up of our example. Besides the CL s approach and the standard \u2206\u03c7 2 -based CI approach, we also include results from another commonly used approach, the so-called raster-scan CI approach. In short, this approach scans through all values of the parameter |\u2206m 2 |, and at each fixed |\u2206m 2 |, it checks the compatibility of the other parameter sin 2 2\u03b8 to the data. A most popular method to carry out the raster scan approach uses the following statistic at each |\u2206m",
            "paragraph_rank": 137,
            "section_rank": 29,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b14",
                    "start": 420,
                    "text": "[15]",
                    "end": 424
                },
                {
                    "type": "bibr",
                    "ref_id": "b29",
                    "start": 434,
                    "text": "[29]",
                    "end": 438
                }
            ]
        },
        {
            "section": "E. The Gaussian CLs method vs. the MC CI method vs. the Raster-Scan MC CI method",
            "text": "which is similar to the \u2206\u03c7 2 statistic given in Eq. 7except that the global minimum \u03c7 2 min (x) is replaced by the restricted minimum \u03c7 2 RS min (|\u2206m 2 |; x) = min sin 2 2\u03b8,\u03b7 \u03c7 2 (sin 2 2\u03b8, |\u2206m 2 |, \u03b7; x). Given a fixed value of |\u2206m 2 |, the raster scan method examines all sin 2 2\u03b8 1 values, one at a time, and test the hypothesis H 0 : sin 2 2\u03b8 = sin 2 2\u03b8 1 based on the statistic \u2206\u03c7 2 RS (sin 2 2\u03b8 1 , |\u2206m 2 |; x). The raster scan approach is usually considered less ideal than the standard CI approach that we described in Sec. II, mainly because it does not make comparisons between hypotheses that have different values of |\u2206m 2 | and hence can not distinguish a likely value of this parameter from an unlikely one [6]. In addition, according to Eq. (34) and Eq. (35), when sin 2 2\u03b8 = 0, any value of |\u2206m 2 | will result in the same model, namely, the Standard Model. As a consequence, the Standard Model is tested many times against different new physics hypothe-ses that correspond to different values of |\u2206m 2 |, which makes it difficult to interpret the test results. Whereas in the standard CI approach, any model is tested only once. Similar to the case of \u2206\u03c7 2 , the regularity condition of the Wilks' theorem would also break for \u2206\u03c7 2",
            "paragraph_rank": 138,
            "section_rank": 29,
            "ref_spans": [
                {
                    "ref_id": "formula_6",
                    "start": 52,
                    "text": "7",
                    "end": 53
                },
                {
                    "type": "bibr",
                    "ref_id": "b1",
                    "start": 136,
                    "text": "2",
                    "end": 137
                },
                {
                    "type": "bibr",
                    "ref_id": "b5",
                    "start": 721,
                    "text": "[6]",
                    "end": 724
                }
            ]
        },
        {
            "text": "RS",
            "section_rank": 30
        },
        {
            "section": "RS",
            "text": "when the true sin 2 2\u03b8 = 0. Therefore, Monte Carlo is usually necessary to obtain the distribution of \u2206\u03c7 2 RS to compute CIs using the raster scan. Fig. 10 compares the sensitivity of the Gaussian CL s method, the standard MC CI method, and the rasterscan MC CI method. We assumed that the true value of sin 2 2\u03b8 is 0 in generating MC. At each \u2206m 2 , 50% (50%) of MC samples will have a better (worse) exclusion limit than the sensitivity curve. Sensitivities from these three methods are similar. The sensitivity of the 95% exclusion set from the Gaussian CL s method is slightly better than that of the 95% CI from the MC CI method, and is in fact Comparison of the sensitivity of the 95% Gaussian CLs method vs. that of the 95% and the 90% MC CI method. We also added the 95% raster-scan MC CI for comparison. The true value of sin 2 2\u03b8 is 0. See texts for more explanations.",
            "paragraph_rank": 139,
            "section_rank": 30,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b1",
                    "start": 105,
                    "text": "2",
                    "end": 106
                },
                {
                    "type": "figure",
                    "ref_id": "fig_1",
                    "start": 148,
                    "text": "Fig. 10",
                    "end": 155
                }
            ]
        },
        {
            "section": "RS",
            "text": "close to that of the 90% CI from the MC CI method for this setup. This is expected, since the test statistic \u2206T = T min H1 \u2212T min H0 used in the Gaussian CL s method is designed to focus on the differences between the new physics hypotheses (H 1 : sin 2 2\u03b8 = sin 2 2\u03b8 1 for some sin 2 2\u03b8 1 > 0), with the Standard Model (H 0 : sin 2 2\u03b8 = 0). Therefore, when the true value of sin 2 2\u03b8 is 0, the Gaussian CL s method has larger power to exclude new physics hypotheses than the MC CI method. In addition, the 95% sensitivity from the Gaussian CL s method is very close to that from the raster-scan MC CI method. This is actually a coincidence, since the CL s method and the rasterscan method use the ratios of p-values and p-value to set limits, respectively. The left panel of Fig. 11 shows the difference between the CL s sensitivity (CL s value) and raster-scan sensitivity (p-value) at each parameter point. The difference is rather large at small values of sin 2 2\u03b8, which indicates that the similarity of the 95% lines between the CL s method and the raster-scan method is a coincidence. The right panel of Fig. 11 shows the sensitivity difference between the raster-scan MC CI method and the standard MC CI method. The sensitivity are also different, since the choice of test statistics are different between the raster-scan MC CI method and the standard MC CI method.",
            "paragraph_rank": 140,
            "section_rank": 30,
            "ref_spans": [
                {
                    "type": "figure",
                    "ref_id": "fig_1",
                    "start": 776,
                    "text": "Fig. 11",
                    "end": 783
                },
                {
                    "type": "figure",
                    "ref_id": "fig_1",
                    "start": 1111,
                    "text": "Fig. 11",
                    "end": 1118
                }
            ]
        },
        {
            "section": "RS",
            "text": "When the new physics is indeed true, the standard MC CI method has clear advantage in constraining the parameter space over the other two methods. This is shown in Fig. 12. The MC sample is generated with sin 2 2\u03b8 true = 0.1 and \u2206m 2 true = 2.5\u00d710 \u22123 eV 2 with statistical fluctuations and systematic variations. The 90% CI of the MC CI method were able to identify a small region close to the true value. In comparison, the 95% CL s limit successfully excluded the region on the right, but failed to exclude regions (on the left of line) far away from the true value. This again is due to the choice of the test statistic (\u2206T in the Gaussian CL s method vs. \u2206\u03c7 2 in the MC CI method). The proposed test statistic \u2206T focuses on the difference between the new physics hypothesis and the Standard Model, while the test statistic \u2206\u03c7 2 takes into account all the likely values of (sin 2 2\u03b8, \u2206m 2 ). Therefore, we confirm the conclusion from Ref. [15]: \"the CL s technique for setting limits is appropriate for determining exclusion sets while the determination of CIs advocated by the Feldman-Cousins method is more appropriate for treating established signals\". For comparison, we also display the 95% raster-scan MC CI. Since the raster scan can not distinguish likely and unlikely values of the parameter |\u2206m 2 |, it also failed to exclude some regions of the parameter space that are far away from the truth.",
            "paragraph_rank": 141,
            "section_rank": 30,
            "ref_spans": [
                {
                    "type": "figure",
                    "ref_id": "fig_1",
                    "start": 164,
                    "text": "Fig. 12",
                    "end": 171
                },
                {
                    "type": "bibr",
                    "ref_id": "b14",
                    "start": 942,
                    "text": "[15]",
                    "end": 946
                }
            ]
        },
        {
            "text": "VI. DISCUSSION",
            "section_rank": 31
        },
        {
            "section": "VI. DISCUSSION",
            "text": "In order to use the Gaussian CL s method, it is important that the CD1-CD3 listed in the end of Sec. III are met. The first condition CD1 is continuity of the parameter space for the nuisance parameters, under both the null and the alternative hypotheses. This requirement is easier to achieve compared to the first regularity condition required by the Wilks' theorem, since it concerns the nuisance parameters only, not the parameters of interest, (sin 2 2\u03b8, |\u2206m 2 |). The second condition CD2 concerns large enough data size, which is also easier to reach compared to that required by the Wilks' theorem. This is because in the Gaussian CL s method tests a simpler pair of hypotheses, in which the values of (sin 2 2\u03b8, |\u2206m 2 |) are fixed, and one automatically avoids the situation shown in Fig. 1b that involves minimization over a large range of |\u2206m 2 | in calculating the test statistic. The third condition CD3 is that the difference between the predictions of two hypotheses is small comparing to the predictions themselves. In searching for new physics with precision measurements, the signal from the Standard Model is usually much larger than the potential signal from new physics. Therefore, CD3 is generally satisfied. In the case that CD3 is violated or marginally satisfied (see Fig. 8), one should use Monte Carlo simulation to derive the distribution of the test statistic.",
            "paragraph_rank": 142,
            "section_rank": 31,
            "ref_spans": [
                {
                    "type": "figure",
                    "ref_id": "fig_1",
                    "start": 793,
                    "text": "Fig. 1b",
                    "end": 800
                },
                {
                    "type": "figure",
                    "start": 1293,
                    "text": "Fig. 8",
                    "end": 1299
                }
            ]
        },
        {
            "section": "VI. DISCUSSION",
            "text": "Similar to the Wilks' CI method based on the test statistic \u2206\u03c7 2 and predefined constants, the Gaussian CL s method also allows easy combination of multiple independent experimental results that probe the same parameter space. The CL s value at each alternative hypothesis H 1 ). In practice, the main challenge in combining multiple experiment results arise from the potential correlation among different experiments and requires careful examinations.",
            "paragraph_rank": 143,
            "section_rank": 31
        },
        {
            "section": "VI. DISCUSSION",
            "text": "So far, we have argued that, in practice, the CL s method is often simple to use and allows easy combination of multiple results. But it is important to remind the readers that the CL s is a limited method that aims at setting boundaries only. The CL s based on \u2206T does not directly address the question \"do we see new physics or not\", nor does it provide estimate of parameters. To help address the first question, we recommend reporting the p-value based on the test statistic \u2206\u03c7 2 assuming the Standard Model is true, in addition to the obtained exclusion sets. To address the second question, the standard CI approach is needed. Indeed, the standard CI approach is the preferred approach to take whenever one can afford to carry it out correctly, because the standard CI approach is a unified approach to set limits in the absence of new physics signals and to estimate parameters after the discovery of new physics [6].",
            "paragraph_rank": 144,
            "section_rank": 31,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b5",
                    "start": 920,
                    "text": "[6]",
                    "end": 923
                }
            ]
        },
        {
            "text": "VII. SUMMARY",
            "section_rank": 32
        },
        {
            "section": "VII. SUMMARY",
            "text": "In this paper, we describe a method to present results in searching for new physics in a continuous parameter space. This method takes the CL s approach to obtain exclusion sets for parameters. Specifically, the method consists of testing many pairs of hypotheses. Each time, a new physics model is tested against the Standard Model using the log-likelihood ratio test statistic, or certain variations of it, denoted by \u2206T . We provide a mathematical proof to show that the distribution of \u2206T follows a Gaussian distribution at large data limit under either hypothesis, when the two hypotheses are relatively close. This result allows a simple alternative to the computationally intensive Monte Carlo method to calculate CL s values, and thus to set exclusion limits in one or multiple dimensional parameter spaces. This method can also be used to conveniently combine results from multiple experiments. ",
            "paragraph_rank": 145,
            "section_rank": 32
        },
        {
            "section": "VII. SUMMARY",
            "text": "where",
            "paragraph_rank": 146,
            "section_rank": 32
        },
        {
            "section": "VII. SUMMARY",
            "text": "and B * n\u00d7q * = \u2202\u03c4 0 \u2202\u03b6 .",
            "paragraph_rank": 147,
            "section_rank": 32
        },
        {
            "section": "VII. SUMMARY",
            "text": "Further,",
            "paragraph_rank": 148,
            "section_rank": 32
        },
        {
            "section": "VII. SUMMARY",
            "text": "Proof. By definition,\u03b6 is such that That is,",
            "paragraph_rank": 149,
            "section_rank": 32
        },
        {
            "section": "VII. SUMMARY",
            "text": "Note by delta's method",
            "paragraph_rank": 150,
            "section_rank": 32
        },
        {
            "section": "VII. SUMMARY",
            "text": "and",
            "paragraph_rank": 151,
            "section_rank": 32
        },
        {
            "section": "VII. SUMMARY",
            "text": "Hence, the lhs of Eq. (41) becomes",
            "paragraph_rank": 152,
            "section_rank": 32
        },
        {
            "section": "VII. SUMMARY",
            "text": "The rhs of Eq. (41) becomes",
            "paragraph_rank": 153,
            "section_rank": 32
        },
        {
            "section": "VII. SUMMARY",
            "text": "Hence, equating lhs and rhs leads to, for j = 1, ",
            "paragraph_rank": 154,
            "section_rank": 32
        },
        {
            "section": "VII. SUMMARY",
            "text": "That is",
            "paragraph_rank": 155,
            "section_rank": 32
        },
        {
            "section": "VII. SUMMARY",
            "text": "Therefore",
            "paragraph_rank": 156,
            "section_rank": 32
        },
        {
            "section": "VII. SUMMARY",
            "text": "Hence",
            "paragraph_rank": 157,
            "section_rank": 32
        },
        {
            "text": "By fixing a reference value of \u03b2, say \u03b2 ref , one can test a pair of non-nested hypotheses H 0 : \u03b2 = \u03b2 ref versus H 1 : \u03b2 = \u03b2 1 using a test statistic of the form \u2206T (\u03b2 ref , \u03b2 1 ; x) := \u03c7 2 (\u03b2 1 ; x) \u2212 \u03c7 2 (\u03b2 ref ; x) .",
            "paragraph_rank": 158,
            "section_rank": 33
        },
        {
            "text": "FIG. 1 .",
            "section_rank": 34
        },
        {
            "section": "FIG. 1 .",
            "text": "FIG. 1. (color online) Left panel (a): The parameter space of sin 2 2\u03b8 vs. |\u2206m 2 | in the Cartesian coordinate. Physical constraints are sin 2 2\u03b8 \u2265 0 and |\u2206m 2 | \u2265 0. Right panel (b): Schematic illustration of the effective parameter space of sin 2 \u03b8 vs. |\u2206m 2 | taking into account the spectral difference measured by \u03c7 2 defined in Eq. (3). When sin 2 2\u03b8 = 0, points with different values of |\u2206m 2 | will converge into a single point. This can be easily seen from Eq. (9). At sin 2 2\u03b8 = 0, |\u2206m 2 | has no impact on the neutrino spectrum. Therefore, when sin 2 2\u03b8 = 0, there is no open neighborhood around the true value, leading to a failure of regularity conditions required by the Wilks' theorem.",
            "paragraph_rank": 159,
            "section_rank": 34
        },
        {
            "text": "and Eq. (16) origin from the likelihood function of the Poisson and the Gaussian distribution, respectively. Eq. (17) and Eq. (18) are variations of Eq. (16), and are commonly referred to as the Pearson and the Neyman Chi-square, respectively 5 Note that Eq. (3) is a specific example of Eq. (17)",
            "paragraph_rank": 160,
            "section_rank": 35
        },
        {
            "text": "\u2022",
            "section_rank": 36
        },
        {
            "section": "\u2022",
            "text": "Ym = op(Xm) if and only if Ym/Xm \u2192 0 in probability as m \u2192 \u221e, and \u2022 Ym = Op(Xm) if and only if Ym/Xm is bounded in probability as m \u2192 \u221e . In the special case where {Xm} and {Ym} are deterministic sequences, the stochastic op and Op symbols reduce to the o and O symbols. See Ref. [22, sec 2.2] for details on the rules of calculus with these symbols.",
            "paragraph_rank": 161,
            "section_rank": 36
        },
        {
            "text": "Under the correct hypothesis Under the alternative hypothesis General notation",
            "section_rank": 37
        },
        {
            "section": "Under the correct hypothesis Under the alternative hypothesis General notation",
            "text": "Mean bin counts \u00b5(\u03b7) = (\u00b51(\u03b7), \u2022 \u2022 \u2022 , \u00b5N (\u03b7)) \u03bd(\u03b6) = (\u00b51(\u03b6), \u2022 \u2022 \u2022 , \u00b5N (\u03b6)) Per-unit mean counts \u03c0(\u03b7) = \u00b5(\u03b7)/m \u03c4 (\u03b6) = \u03bd(\u03b6)/m True values or their closest approximations under the give model nuisance parameter \u03b70 (a q-dim vector) \u03b60 (a q * -dim vector) Mean bin counts \u00b5 0 = \u00b5(\u03b70) \u03bd 0 = \u03bd(\u03b60) Per-unit mean counts \u03c0 0 = \u00b5 0 /m \u03c4 0 = \u03bd 0 /m Estimation based on observed data nuisance parameter\u03b7 = arg min \u03c7 2 H 0 (\u03b7; X)\u03b6 = arg min \u03c7 2 H 1 (\u03b6; X) Mean bin counts\u03bc = \u00b5(\u03b7)\u03bd = \u03bd(\u03b6) Per-unit mean counts\u03c0 =\u03bc/m\u03c4 =\u03bd/m TABLE I. Legend of symbols used in describing the correct model and the alternative model, respectively.",
            "paragraph_rank": 162,
            "section_rank": 37
        },
        {
            "text": ", n. Then by the Taylor expansion of f = (f 1 , \u2022 \u2022 \u2022 , f n ) T and e = (e 1 , \u2022 \u2022 \u2022 , e n ) T around (a, b, c) = (0, 0, 0), we have",
            "paragraph_rank": 163,
            "section_rank": 38
        },
        {
            "text": "Note that under assumption [A0], D 1 = D is of order O(m). Next denote the second term of D by D 2 . The central limit theorem implies that as m increases to infinity, \u221a m (p \u2212 \u03c0 0 ) converges in distribution to the N(0",
            "paragraph_rank": 164,
            "section_rank": 39
        },
        {
            "text": "i (\u03bdi\u2212Ni) 2",
            "section_rank": 40
        },
        {
            "section": "i (\u03bdi\u2212Ni) 2",
            "text": "\u03bdi are very small compared to the latter and are hence negligible. It follows that the different versions of the test statistic \u2206T (X) will behave similarly as D(X). Finally, it is easy to see that our definition for D in Eq. (25) is equivalent to \u2206T (defined in Eq. (20)) based on Eq. (17). Our main result in Eq. (28) can be stated as \u2206T approx. \u223c N (\u2206T , 4|\u2206T |).",
            "paragraph_rank": 165,
            "section_rank": 40
        },
        {
            "text": "1 . 2 .",
            "section_rank": 41
        },
        {
            "section": "1 . 2 .",
            "text": "From the observed data x, obtain \u2206T (x) := T min H1 (x) \u2212 T min H0 (x). From the Asimov data set x Asimov H0 , obtain \u2206T H0 = \u2206T (x Asimov H0 ) = T min H1 (x Asimov H0 ) according to Eq. (20). Then according to the main result that we prove in Sec. III B, under H 0 , \u2206T (X) follows approximately a Gaussian distribution with mean \u2206T H0 and standard deviation 2 \u221a |\u2206T H0 |. This suggests that one can approximate 1\u2212p 0 using",
            "paragraph_rank": 166,
            "section_rank": 41
        },
        {
            "text": "FIG. 4 .",
            "section_rank": 42
        },
        {
            "section": "FIG. 4 .",
            "text": "FIG. 4. (color online)Top panels show the mean number of events seen at the near and far detectors in a disappearance experiment. Bottom panels show the mean number of events seen at near and far detectors in an appearance experiment. Left and right panels show near and far detectors, respectively. See text for more explanations.",
            "paragraph_rank": 167,
            "section_rank": 42
        },
        {
            "text": "FIG. 5 .FIG. 6 .",
            "section_rank": 43
        },
        {
            "section": "FIG. 5 .FIG. 6 .",
            "text": "FIG. 5. (color online) Distributions of \u2206\u03c7 2 = T (sin 2 2\u03b8 = 0) \u2212 Tmin is plotted for MCs with the true sin 2 2\u03b8 = 0. Distributions based on the hybrid Bayesian/Frequentist and full Frequentist approaches are compared to the Chi-square distribution with two degrees of freedom.",
            "paragraph_rank": 168,
            "section_rank": 43
        },
        {
            "text": "FIG. 7 .FIG. 8 .",
            "section_rank": 44
        },
        {
            "section": "FIG. 7 .FIG. 8 .",
            "text": "FIG. 7. (color online) The test statistic \u2206T= T min H 1 \u2212 T min H 0is plotted for MCs (appearance) assuming the hypothesis H0 or H1 is true. Here, the null hypothesis H0 corresponds to sin 2 2\u03b8 = 0. The alternative hypothesis H1 corresponds to sin 2 2\u03b8 = 0.008 and \u2206m 2 = 2.5 \u00d7 10 \u22123 eV 2 .",
            "paragraph_rank": 169,
            "section_rank": 44
        },
        {
            "text": "FIG. 9 .",
            "section_rank": 45
        },
        {
            "section": "FIG. 9 .",
            "text": "FIG. 9. (color online)Comparison of the exclusion sets determined by the Gaussian CLs method vs. the CI, both using the test statistic \u2206T = T min H 1 \u2212 T min H 0 . The true value of sin 2 2\u03b8 is 0. For the CI (CLs) method, the right side of the red (black) line has a p-value (CLs value) smaller than 0.05. The sensitivity curves are generated from a large number of Monte Carlo samples. At each \u2206m 2 , 50% (50%) of MC samples will have a better (worse) exclusion limit than the sensitivity curve.",
            "paragraph_rank": 170,
            "section_rank": 45
        },
        {
            "text": "FIG. 10",
            "section_rank": 46
        },
        {
            "section": "FIG. 10",
            "text": "FIG. 10. (color online)Comparison of the sensitivity of the 95% Gaussian CLs method vs. that of the 95% and the 90% MC CI method. We also added the 95% raster-scan MC CI for comparison. The true value of sin 2 2\u03b8 is 0. See texts for more explanations.",
            "paragraph_rank": 171,
            "section_rank": 46
        },
        {
            "text": "FIG. 12 .",
            "section_rank": 47
        },
        {
            "section": "FIG. 12 .",
            "text": "FIG. 11. (color online)Left panel: the difference between the CLs sensitivity and the raster-scan sensitivity is shown at each parameter point. Right panel: the difference between the raster-scan sensitivity and the standard CI sensitivity is shown at each parameter point.",
            "paragraph_rank": 172,
            "section_rank": 47
        },
        {
            "text": "Lemma 1 .",
            "section_rank": 48
        },
        {
            "section": "Lemma 1 .",
            "text": "Assuming [A0], we have \u221a m ( p \u2212 \u03c0 0 \u03c4 \u2212 \u03c4 0",
            "paragraph_rank": 173,
            "section_rank": 48
        },
        {
            "text": "j = 1, \u2022 \u2022 \u2022 , q * .",
            "paragraph_rank": 174,
            "section_rank": 49
        },
        {
            "text": "Note that, under assumption [A0], all the terms are O p (m \u2212 1 2 ) or smaller except for the first term on the lhs. Letting m grow to infinity in Eq. (42) implies that n . (40) of Lemma 1. Plugging this result back into Eq. (42), we have 0 + 2B * T diag",
            "paragraph_rank": 175,
            "section_rank": 50
        },
        {
            "text": ").Assumption[A1]  means that the best model under the wrong hypothesis is just barely incorrect. For example, under [A1], Ref.[10] showed that the likelihood ratio test statistic for testing H 1 against the full parameter space (that is, the statistic T min H1 (X) based on Eq. (15) and Eq. (16)) has a limiting non-central Chi-square distribution. The noncentrality parameter has the same form as the test statistic, but with\u03bc and\u03bd replaced by \u00b5 0 and \u03bd 0 . For a simplified presentation of this result, see, for example, Ref.[12, Sec. 3.1]. Under [A1], the non-centrality parameter is finite and the non-central Chi-square approximations are accurate to the extent that the O p",
            "paragraph_rank": 176,
            "section_rank": 51,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b9",
                    "start": 126,
                    "text": "[10]",
                    "end": 130
                },
                {
                    "type": "bibr",
                    "ref_id": "b11",
                    "start": 527,
                    "text": "[12,",
                    "end": 531
                }
            ]
        },
        {
            "text": "Clearly, [A0] is a more relaxed condition than [A1], in the sense that [A1] implies [A0], but not vice versa. An example where [A0] holds and [A1] does not, is the case that the nuisance parameter is absent: each hypothesis allows exactly one model, such that the per unit mean bin counts of the model under H 0 is \u03c0 0 , and that under H 1 is \u03c4 0 , where \u03c0 0 and \u03c4 0 are vectors of constants that do not change with the data size m. As for general cases where there are nontrivial nuisance parameters, it is possible that the best model under H 1 can lead to \u03c4 0 values that move closer to the truth \u03c0 0 as more data become available. Hence [A1] may become satisfied, while [A0] is always satisfied. In situations where one is unwilling to assert a convergence rate as fast as O(m \u2212 1",
            "paragraph_rank": 177,
            "section_rank": 52
        },
        {
            "text": "2 ) for (\u03c4 0 \u2212 \u03c0 0 ), if the convergence occur at all, [A0] is more appropriate than [A1].To see the impact of using [A0] instead of [A1], it turns out that when lim m\u2192\u221e m 1 2 \u03b4 = \u221e, various test statistics similar to \u03c7 2 H1 (\u03b6) (these are the different versions of T min H1 that we mentioned in Sec. III A) would be unbounded in",
            "paragraph_rank": 178,
            "section_rank": 52
        },
        {
            "text": "Rigorously speaking, the word \"confidence set\" should be used instead of \"confidence interval\" when the dimension of the parameter space is higher than one. But as long as there is no ambiguity, we will refer to all confidence sets as confidence intervals for simplicity.3 When the model contains nuisance parameters, extra care are needed in performing Monte Carlo simulation. See Ref. [7] for example. 1 \u00a9 2016. This manuscript version is made available under the Elsevier user license http://www.elsevier.com/open-access/userlicense/1.0/",
            "paragraph_rank": 179,
            "section_rank": 52,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b2",
                    "start": 271,
                    "text": "3",
                    "end": 272
                }
            ]
        },
        {
            "text": "This is usually done in order that T H 0 (\u03b7; x) and T H 1 (\u03b7; x) are asymptotically equivalent under certain conditions to their counterparts in the classical Chi-square forms, namely, the Neyman and the Pearson Chi-square statistics.",
            "paragraph_rank": 180,
            "section_rank": 52
        },
        {
            "text": "An alternative way to define \u2206T (x) is to replace T min H 0 (x) and T min H 1 (x) by T mag H 0 (x) and T mag H 1 (x), respectively, which are the marginalized, or say integrated version of T H 0 (\u03b7; x) and T H 1 (\u03b7; x) over all nuisance parameters. These two methods generally give very similar results in practice. From the statistics point of view, while the minimization method adopts the Frequentist's philosophy, the marginalization method adopts the Bayesian philosophy.",
            "paragraph_rank": 181,
            "section_rank": 52
        },
        {
            "text": "The Big O, the small o, the Big Op, and the small op notation are standard mathematical symbols, such that for two sequences of random variables {Xm} and {Ym}, we write",
            "paragraph_rank": 182,
            "section_rank": 52
        },
        {
            "text": "We would like to thank Wei Wang for helpful discussions. This material is based upon work supported by the National Science Foundation and the U.S. Department of Energy, Office of Science, Office of High Energy Physics, Early Career Research program under contract number DE-SC0012704.",
            "paragraph_rank": 183,
            "section_rank": 54
        },
        {
            "text": "APPENDICES",
            "section_rank": 56
        },
        {
            "text": "A. A few basic properties of the fitted models under H0 and H1",
            "section_rank": 57
        },
        {
            "section": "A. A few basic properties of the fitted models under H0 and H1",
            "text": "Suppose H 0 is the correct hypothesis, that is, the data X came from H 0 . Having observed the data, the best fitting models under H 0 and H 1 have estimated nuisance parameters\u03b7 and\u03b6 respectively, as defined in Sec. IV. The corresponding per unit mean counts are denoted\u03c0 and\u03c4 respectively.",
            "paragraph_rank": 184,
            "section_rank": 57
        },
        {
            "section": "A. A few basic properties of the fitted models under H0 and H1",
            "text": "We show below that there is a unique limit of\u03b6 as the data size increases, and that it leads to the model \u03bd(\u03b6) that is the closest model under H 1 to the true model \u00b5 0 under a certain criteria. Indeed, let t m (\u03b6) = \u03c7 2",
            "paragraph_rank": 185,
            "section_rank": 57
        },
        {
            "section": "A. A few basic properties of the fitted models under H0 and H1",
            "text": ", and let",
            "paragraph_rank": 186,
            "section_rank": 57
        },
        {
            "section": "A. A few basic properties of the fitted models under H0 and H1",
            "text": ". Since p converges almost surely (a.s.) to \u03c0 0 as m increases, we have t m (\u03b6) converges a.s. to t m (\u03b6). Then under regularity conditions, such as t m being twice differentiable and convex in \u03b6,\u03b6 m = arg min \u03b6 t m (\u03b6) also converges a.s. to arg min \u03b6 t(\u03b6) as m increases. By denoting the limit of\u03b6 m by \u03b6 0 , we have mt(\u03b6 0 ) = min \u03b6 mt(\u03b6). That is, \u03b6 0 and \u03bd 0 := \u03bd(\u03b6 0 ) are such that",
            "paragraph_rank": 187,
            "section_rank": 57
        },
        {
            "section": "A. A few basic properties of the fitted models under H0 and H1",
            "text": "We list a few more properties that are useful in the proof of Lemma 1 and the proof of the result in Eq. (26). It is well-known that\u03b7 \u2212 \u03b7 0 and\u03c0 \u2212 \u03c0 0 are both of order O p (m \u2212 1 2 ). And\u03b6 \u2212 \u03b6 0 and\u03c4 \u2212 \u03c4 0 are also both of order O p (m \u2212 1 2 ) according to Ref. [30].",
            "paragraph_rank": 188,
            "section_rank": 57,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b25",
                    "start": 105,
                    "text": "(26)",
                    "end": 109
                },
                {
                    "type": "bibr",
                    "start": 178,
                    "text": "1 2",
                    "end": 181
                },
                {
                    "type": "bibr",
                    "ref_id": "b30",
                    "start": 263,
                    "text": "[30]",
                    "end": 267
                }
            ]
        }
    ]
}