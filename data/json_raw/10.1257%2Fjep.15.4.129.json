{
    "level": "paragraph",
    "abstract": [
        {
            "text": "hen an applied econometrician calculates regression coefficients or other statistics based on a data sample, there is a moment of truth when the statistical precision or reliability of the estimates is evaluated using critical values of relevant probability distributions. If statistical precision is underestimated, producing confidence intervals that are too wide, the researcher may falsely conclude that no useful or reliable evidence has been provided. Since statistically imprecise results often are regarded as noninformative and may never see the light of day, applied econometricians have a strong incentive to obtain tight confidence bands. But overestimation of statistical precision also is a concern, as it potentially causes rejection of a null hypothesis when in fact no statistically reliable evidence has been presented. Such results are misleading and provide poor guidance for future research and perhaps public policy as well. This article describes two techniques that have been developed by statisticians in the last 20 years to enhance the accuracy of estimated confidence bands and critical values: the bootstrap and multiple imputations. These are computationally intensive methods that rely on repeated sampling from empirical data sets and associated estimates. As such, their development and dissemination have been supported by substantial increases in computing power over the same period. These techniques often can provide accurate estimates of statistical precision when standard analytical estimates are biased or, in some cases, unavailable. Both already",
            "paragraph_rank": 1,
            "section_rank": 0
        }
    ],
    "body_text": [
        {
            "text": "are important in the toolkits of applied econometricians and statisticians, and they are likely to be more important in the future. We will focus our intuitive discussion on deriving more accurate confidence bands in ordinary least squares regressions, the most commonly used model in applied econometrics. However, computationally intensive approaches also can be used to remove biases in estimated coefficients and can be applied to more general models. Indeed, these techniques may be most appealing or powerful when applied in more complex settings as an alternative to analytical estimates.",
            "paragraph_rank": 2,
            "section_rank": 1
        },
        {
            "text": "The theoretical justification for the bootstrap, multiple imputations and standard analytic inference methods commonly used in applied econometrics is based on large-sample approximations. In the remainder of this paper, we will use the formal term \"consistent\" to mean that as the sample size of the estimation data set increases, the estimator gets closer to its true value in the underlying population.",
            "paragraph_rank": 3,
            "section_rank": 1
        },
        {
            "text": "The Bootstrap",
            "section_rank": 2
        },
        {
            "section": "The Bootstrap",
            "text": "To motivate the discussion of the bootstrap approach in regression analysis, begin by considering the simpler problem of estimating the sampling distribution (for example, standard errors, confidence intervals and other statistical measures) of the mean height among a country's population, based on a small random sample. The sample mean typically will differ from the population mean due to sampling error, and the sampling distribution summarizes how the sample mean will vary across a large number of independent samples from the same population. Efron's (1979Efron's ( , 1982 bootstrap method treats the observed sample as the population and draws samples from this approximate population to estimate the sampling distribution. These bootstrap samples are drawn with replacement from the original observed data, and the mean is computed for each bootstrap sample. This produces a data set of estimated means, with size equal to the number of bootstrap repetitions. The distribution of these resampled means is used to approximate the sampling distribution of the population mean.",
            "paragraph_rank": 4,
            "section_rank": 2,
            "ref_spans": [
                {
                    "type": "bibr",
                    "start": 551,
                    "text": "Efron's (1979",
                    "end": 564
                },
                {
                    "type": "bibr",
                    "ref_id": "b4",
                    "start": 564,
                    "text": "Efron's ( , 1982",
                    "end": 580
                }
            ]
        },
        {
            "section": "The Bootstrap",
            "text": "This bootstrap method described above will only give accurate estimates if the original sample is large enough to reflect the true population accurately. The traditional analytic approach approximates the sampling distribution by a normal distribution centered at the sample mean with variance equal to the sample variance. This traditional approximation requires that the sample be large enough for the Central Limit Theorem to apply to the sample mean. If the sample size is small and the true population is not normally distributed, then the bootstrap approximation should be more accurate.",
            "paragraph_rank": 5,
            "section_rank": 2
        },
        {
            "section": "The Bootstrap",
            "text": "The application of the bootstrap to linear regression models is similar to the sample mean case above, although several different approaches are possible. The most popular way to implement the bootstrap to estimate the sampling distribution of the least squares coefficient estimators is the XY, or paired bootstrap. Using this approach, with an observed sample of size N, randomly draw N complete data points with replacement from the observed sample (or, in other words, sample rows of the data matrix). Then recompute the least squares estimator for each bootstrap sample. This produces a different estimate of the coefficients for each repetition. The distribution of the resulting set of estimated coefficients is an estimate of their sampling distribution. Whether the error terms are homoskedastic or heteroskedastic, the paired bootstrap will generate consistent estimates of the sampling distribution of the least squares estimator. Brownstone and Kazimi (2000), Efron and Tibshirani (1993), Horowitz (2001) and Jeong and Maddala (1993) provide more complete applied discussions of this and other bootstrap procedures, including details regarding hypothesis tests and confidence intervals based on the bootstrapped estimates. 1 Horowitz (1997) points out that the different nature of the bootstrap approximation provides a rough check on the adequacy of large-sample theory. If the assumptions commonly made for large-sample econometrics hold-for example, residuals that are independent and distributed identically with finite variancethen the bootstrap and standard methods used in econometric software packages should yield the same answers. If significant departures from these assumptions are present, then one or both of the methods are not reliable for the particular problem and data. Deciding between them requires more detailed analysis, which explains why much of the ongoing research on the bootstrap is based on simulations rather than on empirical data sets.",
            "paragraph_rank": 6,
            "section_rank": 2,
            "ref_spans": [
                {
                    "type": "bibr",
                    "start": 972,
                    "text": "Efron and Tibshirani (1993)",
                    "end": 999
                },
                {
                    "type": "bibr",
                    "start": 1001,
                    "text": "Horowitz (2001)",
                    "end": 1016
                },
                {
                    "type": "bibr",
                    "ref_id": "b19",
                    "start": 1235,
                    "text": "1",
                    "end": 1236
                }
            ]
        },
        {
            "section": "The Bootstrap",
            "text": "Despite this inherent uncertainty about the superiority of the bootstrap in specific applications, Hall (1992) has shown that in many cases the bootstrap achieves accurate estimates of sampling distributions at smaller sample sizes than standard large-sample analytic techniques. This finding holds for most of the statistics that are commonly used for testing hypotheses about parameter estimates from econometric models, such as t-statistics and others based on standard normal and chi-square distributions. 2 This feature of the bootstrap provides the strongest argument for its widespread application in common econometric models, although the simple bootstrap methods described in this paper need to be modified to simulate the distribution of test statistics rather than coefficient estimates for Hall's results to apply.",
            "paragraph_rank": 7,
            "section_rank": 2,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b20",
                    "start": 510,
                    "text": "2",
                    "end": 511
                }
            ]
        },
        {
            "text": "An Empirical Example: An Earnings Regression",
            "section_rank": 3
        },
        {
            "section": "An Empirical Example: An Earnings Regression",
            "text": "We illustrate these ideas with an example based on a standard regression equation for explaining male earnings. The dependent variable is the log of yearly earnings, deflated by the GDP deflator for personal consumption expenditure. We report coefficient estimates for the following explanatory variables: years of formal education, potential labor market experience (and its square), and tenure at the current firm, along with dummy variables indicating whether the respondent is a union member, black, married and resides in a metropolitan statistical area (MSA). We also included a constant and dummy variables representing region of residence and observation years. The data are drawn from the Panel Study of Income Dynamics (PSID) for the years 1984 -1993. We restricted the sample to male household heads, aged 21-64 and employed in the private sector (excluding self-employed individuals). We report results based on a 2 percent random sample of individuals observed at any time in the panel (260 observations on 50 individuals), since in large samples all the procedures give the same results. Using a small subsample also allows us to simulate the true sampling distribution by drawing many independent subsamples of 50 individuals from the data and reestimating the model for each subsample. This simulation uses the original PSID sample of 2,789 individuals (16,327 panel observations) to approximate the true population of anybody who could have been sampled by the PSID. Table 1 presents results for the male wage equation. The estimated regression coefficients are listed in the second column. The next column gives the true confidence intervals calculated from 1000 independent subsamples of 50 individuals from the data. These are the true confidence intervals if the PSID main sample is identical to the population. If the PSID main sample is only an approximation of the true population, then these \"true\" confidence intervals are bootstrap intervals with bootstrap sample size much smaller than the data. It is, of course, more efficient to use the entire sample in this case, but we are only interested in using these intervals as a benchmark for comparing results based on only one subsample.",
            "paragraph_rank": 8,
            "section_rank": 3,
            "ref_spans": [
                {
                    "type": "table",
                    "ref_id": "tab_0",
                    "start": 1484,
                    "text": "Table 1",
                    "end": 1491
                }
            ]
        },
        {
            "section": "An Empirical Example: An Earnings Regression",
            "text": "The subsequent four columns in Table 1 list 95 percent confidence intervals estimated in different ways from the original subsample of 260 observations. The first column of confidence intervals shows those obtained through ordinary least squares regression analysis. The next column provides confidence intervals based on the \"robust\" estimator of the model's sampling variance, which was developed independently by Huber (1967) and White (1980White ( , 1982. The robust estimator of sampling variance does not require homoskedasticity-that the errors in the regression model have constant variance. Instead, the Huber-White estimator approximates the variance of each residual by the square of the least squares residual. We use a generalization of the Huber-White estimator that also allows for the residuals to be correlated across time for a given individual. In the example in Table 1, the confidence intervals from the robust regression are noticeably wider than the ordinary least squares confidence intervals, which suggests that heteroskedasticity or a similar departure from the common assumptions used in applying ordinary least squares is present.",
            "paragraph_rank": 9,
            "section_rank": 3,
            "ref_spans": [
                {
                    "type": "table",
                    "ref_id": "tab_0",
                    "start": 31,
                    "text": "Table 1",
                    "end": 38
                },
                {
                    "type": "bibr",
                    "ref_id": "b5",
                    "start": 416,
                    "text": "Huber (1967)",
                    "end": 428
                },
                {
                    "type": "bibr",
                    "start": 433,
                    "text": "White (1980",
                    "end": 444
                },
                {
                    "type": "bibr",
                    "ref_id": "b16",
                    "start": 444,
                    "text": "White ( , 1982",
                    "end": 458
                },
                {
                    "type": "table",
                    "ref_id": "tab_0",
                    "start": 882,
                    "text": "Table 1",
                    "end": 889
                }
            ]
        },
        {
            "section": "An Empirical Example: An Earnings Regression",
            "text": "The third set of confidence intervals is obtained through application of the paired (XY ) bootstrap procedure. For each bootstrap repetition, we sample randomly with replacement from the set of 50 individuals in the original sample. We take all of the dependent and independent variables for all years for the sampled individual and continue until a bootstrap sample of approximately the same size as the empirical data set has been selected. The regression equation is then reestimated for each bootstrap sample. The coefficient estimates from each repetition are saved in a file, and the resulting data set (whose size is determined by the number of coefficients and bootstrap repetitions) provides an estimate of the sampling distribution of the estimated coefficients.",
            "paragraph_rank": 10,
            "section_rank": 3
        },
        {
            "section": "An Empirical Example: An Earnings Regression",
            "text": "All of the bootstrapped distributions reported in the table were obtained using 1000 bootstrap repetitions. Although standard errors can be reliably estimated with fewer repetitions (on the order of 100), confidence intervals directly based on the bootstrap distribution will be more accurate than those based on standard errors if the bootstrap distribution is skewed. Once the repetitions were completed, we obtained the 95 percent confidence intervals through the simple \"percentile method\"-by setting them equal to the values corresponding to the 2.5 and 97.5 percentiles of the bootstrap distributions of the estimated coefficients. 3  For the final column, we used a different bootstrap approach due to Wu (1986), known as the \"wild bootstrap.\" First, run an ordinary least squares regression, and save the residuals. Then multiply each value of the residual by a random variable that takes on one of two values: i) (1 \u03ea \u034c 5)/2 with probability (1 \u03e9 \u034c 5)/(2 \u034c 5); or ii) (1 \u03e9 \u034c 5)/2 with probability 1 \u03ea (1 \u03e9 \u034c 5)/(2 \u034c 5). Note that this random variable has a mean of zero with variance equal to one. The resulting bootstrap residuals therefore will have mean zero and variance equal to the square of the least squares residual for that observation, which is identical to the approximation used to calculate Huber-White robust standard errors. The resampled residual vector is then added to the predicted value from the original regression to create the resampled dependent variable. We modified the wild bootstrap to account for the panel data structure of our data by multiplying the entire vector of residuals for each individual by the same draw of the two-point random variable defined above. The wild bootstrap confidence intervals reported in the table are based on the calculation of t-statistics using the Huber-White robust standard errors for each bootstrap sample. This method should be superior to simple percentile method intervals in small samples (for details, see Brownstone and Kazimi, 2000; Horowitz, 2001). Both paired bootstrapping and wild bootstrapping will provide consistent inference with heteroskedasticity, but wild bootstrapping generally will be more accurate, since, unlike the paired bootstrap, it imposes a restriction on each bootstrap sample that is identical to the conditions used to identify the ordinary least squares model. 4 The least squares confidence intervals in Table 1 are much narrower than the true bands. The other confidence intervals are usually narrower than the true bands, but not for all of the coefficients. The confidence intervals based on the paired bootstrap are somewhat wider than the robust confidence intervals. In contrast, the wild bootstrap confidence intervals are narrower than the paired bootstrap intervals and are close to the robust intervals in width. The bootstrap intervals are closer to the truth for six of the eight coefficients in Table 1, although the theoretical superiority of the wild bootstrap bands is not clear from this example. Of course, the results in Table 1 are just based on one subsample from the PSID, and the differences between the bootstrap and robust regression intervals are not large. Most of the studies finding large differences between the bootstrap and standard methods have used nonlinear models and more complex estimators (Horowitz, 1997; 2001).",
            "paragraph_rank": 11,
            "section_rank": 3,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b21",
                    "start": 638,
                    "text": "3",
                    "end": 639
                },
                {
                    "type": "bibr",
                    "ref_id": "b17",
                    "start": 709,
                    "text": "Wu (1986)",
                    "end": 718
                },
                {
                    "type": "bibr",
                    "ref_id": "b22",
                    "start": 2370,
                    "text": "4",
                    "end": 2371
                },
                {
                    "type": "table",
                    "ref_id": "tab_0",
                    "start": 2414,
                    "text": "Table 1",
                    "end": 2421
                },
                {
                    "type": "table",
                    "ref_id": "tab_0",
                    "start": 2918,
                    "text": "Table 1",
                    "end": 2925
                },
                {
                    "type": "table",
                    "ref_id": "tab_0",
                    "start": 3050,
                    "text": "Table 1",
                    "end": 3057
                }
            ]
        },
        {
            "text": "Extensions and Limitations of the Bootstrap",
            "section_rank": 4
        },
        {
            "section": "Extensions and Limitations of the Bootstrap",
            "text": "The discussion of the bootstrap to this point has focused on a linear regression model with error terms that are independent, but not necessarily normally distrib-uted or homoskedastic. However, there has been considerable work in recent years applying the bootstrap approach to other situations.",
            "paragraph_rank": 12,
            "section_rank": 4
        },
        {
            "section": "Extensions and Limitations of the Bootstrap",
            "text": "Cross-section extensions are numerous. Any of the bootstrapping methods discussed above for the linear regression model can be applied to the nonlinear regression model, with the caveat that bootstrapped residuals must be adjusted to insure that their mean is zero. In the standard analytic approach, obtaining the sampling distribution of nonlinear regression estimates relies on a linear approximation to the regression function. Because bootstrap methods evaluate the appropriate transformations by simulation rather than approximation, we would expect bootstrap estimates of the sampling distribution to be more accurate than the standard analytic estimates as the linear approximation becomes less accurate. Horowitz (2001, section 5.3) verified this in a small simulation study of a particular nonlinear regression model. 5 In time series models, error terms are often serially correlated, and this requires bootstrap approaches that differ from the cross-section case. The simplest approach, called model-based bootstrapping, is to assume that the time series can be fit by an appropriate autoregressive moving average model with serially uncorrelated residuals. Then the bootstrap can be implemented using bootstrap approaches that take samples from the residuals of a regression (as discussed in more detail in footnote 1), although the method of generating the bootstrap samples from these bootstrapped residuals needs to be modified to account for the dynamic nature of the time series model. 6 Similarly, in panel data settings, the bootstrap must be modified to account for time dependence for observations within crosssection units-for example, through bootstrap sampling of the complete set of observations for cross-section units (Ziliak, 1997).",
            "paragraph_rank": 13,
            "section_rank": 4,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b23",
                    "start": 828,
                    "text": "5",
                    "end": 829
                },
                {
                    "type": "bibr",
                    "ref_id": "b24",
                    "start": 1504,
                    "text": "6",
                    "end": 1505
                },
                {
                    "type": "bibr",
                    "ref_id": "b18",
                    "start": 1746,
                    "text": "(Ziliak, 1997)",
                    "end": 1760
                }
            ]
        },
        {
            "section": "Extensions and Limitations of the Bootstrap",
            "text": "Although it is mechanically possible to apply the bootstrap to any model, there are circumstances where the bootstrap will fail to estimate the sampling distribution consistently (for details, see Horowitz, 2001, section 2.1). For example, Andrews (2000) shows that the bootstrap does not accurately estimate the sampling distribution of a coefficient whose true value is zero and whose estimate is constrained to be greater than or equal to zero.",
            "paragraph_rank": 14,
            "section_rank": 4,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b0",
                    "start": 240,
                    "text": "Andrews (2000)",
                    "end": 254
                }
            ]
        },
        {
            "section": "Extensions and Limitations of the Bootstrap",
            "text": "Despite these caveats and inherent uncertainty about the superiority of the bootstrap in specific settings, the availability of powerful desktop computers makes widespread use of the bootstrap very tempting. Bootstrap estimates-especially based on the paired approach-are trivial to obtain once the estimation model is specified. The statistical package Stata has several bootstrap procedures and subroutines available in its core program, and a researcher who has never read a single 5 Other useful cross-section applications, surveyed in Brownstone and Kazimi (2000), include quantile regression, discrete choice and hazard models, and frontier production function estimates. 6  Bickel and Freedman (1983) provided an early example of model-based bootstrapping for time series. Their approach has been generalized to autoregressive moving average models (Berkowitz and Kilian, 2000;Li and Maddala, 1996) and cointegrated regression models (Li and Maddala, 1997). Kilian (1998a, b) considers bootstrapping vector autoregression models and applies this technique to examine international effects of monetary policy.",
            "paragraph_rank": 15,
            "section_rank": 4,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b23",
                    "start": 485,
                    "text": "5",
                    "end": 486
                },
                {
                    "type": "bibr",
                    "ref_id": "b24",
                    "start": 678,
                    "text": "6",
                    "end": 679
                },
                {
                    "type": "bibr",
                    "ref_id": "b1",
                    "start": 681,
                    "text": "Bickel and Freedman (1983)",
                    "end": 707
                },
                {
                    "type": "bibr",
                    "ref_id": "b0",
                    "start": 871,
                    "text": "Kilian, 2000;",
                    "end": 884
                },
                {
                    "type": "bibr",
                    "ref_id": "b7",
                    "start": 884,
                    "text": "Li and Maddala, 1996)",
                    "end": 905
                },
                {
                    "type": "bibr",
                    "ref_id": "b8",
                    "start": 941,
                    "text": "(Li and Maddala, 1997)",
                    "end": 963
                },
                {
                    "type": "bibr",
                    "start": 965,
                    "text": "Kilian (1998a, b)",
                    "end": 982
                }
            ]
        },
        {
            "section": "Extensions and Limitations of the Bootstrap",
            "text": "word about the bootstrap could produce bootstrapped estimates in Stata with little difficulty. Researchers should be cautious about such casual application, because results regarding the reliability and accuracy of bootstrapped sampling distributions are not yet available for many classes of econometric models. However, while we await further results, the bootstrap can be very useful for simple applications such as estimating the sampling distributions of nonparametric statistics (for example, quantiles and percentiles of univariate distributions) or statistics with illdefined distributions (see Valletta, 1993, footnote 13, for an example).",
            "paragraph_rank": 16,
            "section_rank": 4
        },
        {
            "text": "Multiple Imputations",
            "section_rank": 5
        },
        {
            "section": "Multiple Imputations",
            "text": "Applied econometricians routinely face the problem of missing values of key variables in their data. The multiple imputation technique was developed by Rubin (1987) as a general, computationally intensive method \"to handle the problem of missing data in public-use data bases where the data-base constructor and the ultimate user are distinct entities\" (Rubin, 1996). This approach represents a useful compromise between the na\u00efve extremes of eliminating observations with missing values from the analysis sample or replacing missing values with a single imputed (fitted) value. The former approach typically underutilizes the full information available in the data, whereas the latter typically leads to an overstatement of statistical precision in resulting estimates. The technique also can be used for consistent inference using data that are mismeasured rather than missing.",
            "paragraph_rank": 17,
            "section_rank": 5,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b13",
                    "start": 152,
                    "text": "Rubin (1987)",
                    "end": 164
                },
                {
                    "type": "bibr",
                    "ref_id": "b13",
                    "start": 353,
                    "text": "(Rubin, 1996)",
                    "end": 366
                }
            ]
        },
        {
            "section": "Multiple Imputations",
            "text": "Consider the general application to a linear regression model. Multiple imputation consists of two main steps. In the first step, the data provider or analyst uses all available data to estimate a proper imputation model, which provides multiple fitted values of the missing or erroneous variables. In the second step, the analyst combines the estimated imputations with the main data set and estimates the regression equation of interest.",
            "paragraph_rank": 18,
            "section_rank": 5
        },
        {
            "section": "Multiple Imputations",
            "text": "The full definition of a proper imputation procedure is given in Rubin (1987, pp. 118 -119). If we have a model for predicting the missing (or erroneous) values conditional on all observed data, then we can use this model to make independent simulated draws for the missing data. The safest way to implement a proper imputation procedure, as Rubin first proposed, is to draw explicitly from the (Bayesian) posterior predictive distribution of the missing values under a specific model. 7 However, there are other proper multiple imputation procedures that require no explicit Bayesian calculations. Indeed, although Rubin developed the theoretical properties of this methodology for Bayesian models, he showed that these results also apply in large samples to classical statistical models (Rubin, 1987, chapter 4).",
            "paragraph_rank": 19,
            "section_rank": 5,
            "ref_spans": [
                {
                    "type": "bibr",
                    "start": 65,
                    "text": "Rubin (1987, pp. 118 -119",
                    "end": 90
                },
                {
                    "type": "bibr",
                    "ref_id": "b25",
                    "start": 486,
                    "text": "7",
                    "end": 487
                },
                {
                    "type": "bibr",
                    "start": 789,
                    "text": "(Rubin, 1987, chapter 4)",
                    "end": 813
                }
            ]
        },
        {
            "section": "Multiple Imputations",
            "text": "Whether the prediction approach is Bayesian or non-Bayesian, any proper imputation procedure must condition on all observed data. In addition, different sets of imputed values must be drawn independently so that they reflect all sources of uncertainty in the response process.",
            "paragraph_rank": 20,
            "section_rank": 5
        },
        {
            "section": "Multiple Imputations",
            "text": "Once the imputations are provided, the substantive model is estimated once for each set of imputed values. The resulting estimates are averaged to obtain consistent parameter estimates; depending on the pattern of missing responses, the estimates are likely to differ from those obtained using na\u00efve approaches to missing data. The covariance matrix of these coefficient estimates is determined by the average of the usual covariance matrix of the coefficient estimates across different sets of imputations, plus a term that represents the variability of the estimates across the different imputed data sets (see Rubin, 1987, or Brownstone andValletta, 1996, for the exact expressions). This covariance estimator is consistent as long as there are at least two sets of imputed values, but increasing the number of sets of imputed values clearly improves the accuracy of the covariance component due to uncertainty in the imputation process.",
            "paragraph_rank": 21,
            "section_rank": 5,
            "ref_spans": [
                {
                    "type": "bibr",
                    "start": 613,
                    "text": "Rubin, 1987, or Brownstone and",
                    "end": 643
                },
                {
                    "type": "bibr",
                    "start": 643,
                    "text": "Valletta, 1996",
                    "end": 657
                }
            ]
        },
        {
            "section": "Multiple Imputations",
            "text": "The goal is for the imputed data to provide both consistent estimators of coefficients and a measure of the statistical precision or confidence intervals for these coefficients, where the confidence intervals must be broadened somewhat to account for the component of variance associated with imputation error. As long as the variables used in the imputation model have an identifiable, systematic relationship with the imputed variables, the imputation process adds information; the precision of this information is limited by the precision of the imputation process. The computations required typically can be performed using standard econometric packages with relatively minimal programming, although we are not aware of any econometrics packages that currently provide multiple imputation routines or subroutines.",
            "paragraph_rank": 22,
            "section_rank": 5
        },
        {
            "section": "Multiple Imputations",
            "text": "Several statistical agencies have incorporated or are experimenting with multiply imputed values in public-use survey data. Such applications are a key component of Rubin's original vision for the methodology, which he views as an appropriate form of data enhancement by public agencies whose survey information may be more extensive than that available to end users (Rubin, 1996). By exploiting confidential information (such as precise location or financial characteristics) in the imputation procedure, data agencies can better meet the simultaneous demands of providing high-quality data, while protecting survey respondents against breaches of confidentiality. Moreover, one advantage of multiple imputations is that it can be done once by the data provider and then used for a variety of purposes by many analysts.",
            "paragraph_rank": 23,
            "section_rank": 5,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b13",
                    "start": 367,
                    "text": "(Rubin, 1996)",
                    "end": 380
                }
            ]
        },
        {
            "section": "Multiple Imputations",
            "text": "Among the first major applications of multiple imputations was a project conducted at the U.S. Census Bureau to assess the comparability of the different industry and occupation codings in the 1970 and 1980 censuses (Clogg et al., 1991). In this project, analysts used a Bayesian strategy to provide multiply imputed values of 1980 industry and occupation codes in public-use samples from the 1970 census. The imputations were based on logistic regression models estimated from a special subsample of the 1970 census units for which the 1970 and 1980 codes had been recorded. The resulting multiply imputed data set has provided a useful bridge for analysts who rely on accurate assessment of trends in the relative prevalence of precise industry and occupation categories.",
            "paragraph_rank": 24,
            "section_rank": 5,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b2",
                    "start": 216,
                    "text": "(Clogg et al., 1991)",
                    "end": 236
                }
            ]
        },
        {
            "section": "Multiple Imputations",
            "text": "In probably the most important ongoing application, the Federal Reserve has provided multiply imputed income and wealth variables in the public release of its Survey of Consumer Finances since 1989 (Kennickell, 1998). This survey focuses on household finances, a subject for which responses are sensitive and often defined imprecisely. As a result, the survey exhibits high rates of missing information. Fed analysts have developed and implemented a complex iterative imputation procedure for continuous, multinomial and binary variables. The imputation model relies on the publicly available survey information concerning consumer income and wealth, combined with confidential information such as census tract characteristics and industry and occupation characteristics tabulated from the Current Population Survey of the U.S. Bureau of Labor Statistics. Although these multiply imputed data have been used successfully by many researchers, Kennickell reports frequent misuse and notes that \"the need for education is great.\" 8 Multiple imputations have been used less in specific applications than in general data provision. However, Brownstone and Valletta (1996) extended the basic approach to the specific application of correcting for measurement error in the dependent variable in earnings regression equations. 9 Research on earnings data, which compares surveyed earnings responses to validated earnings data gathered through company personnel files or Social Security records, has shown that measurement error in survey earnings is large and correlated with important explanatory variables (for example, Bound et al., 1994; Bound and Krueger, 1991; and Duncan and Hill, 1985). Thus, measurement error in earnings is likely to bias estimated coefficients and statistical confidence in earnings regression models.",
            "paragraph_rank": 25,
            "section_rank": 5,
            "ref_spans": [
                {
                    "type": "bibr",
                    "start": 198,
                    "text": "(Kennickell, 1998)",
                    "end": 216
                },
                {
                    "type": "bibr",
                    "ref_id": "b26",
                    "start": 1027,
                    "text": "8",
                    "end": 1028
                },
                {
                    "type": "bibr",
                    "ref_id": "b27",
                    "start": 1319,
                    "text": "9",
                    "end": 1320
                },
                {
                    "type": "bibr",
                    "ref_id": "b3",
                    "start": 1663,
                    "text": "Duncan and Hill, 1985)",
                    "end": 1685
                }
            ]
        },
        {
            "section": "Multiple Imputations",
            "text": "In Brownstone and Valletta (1996), we estimated equations for the determinants of earnings where the explanatory variables included experience, job tenure, union membership and dummy variables for union membership, race, marriage, blue-collar work and whether the job was hourly. We used survey data on earnings from the Panel Study of Income Dynamics (PSID) combined with information from the PSID Validation Study (PSIDVS). The PSIDVS collected standard PSID variables for several hundred employees from a large Detroit area manufacturing firm in 1983 and 1987 and matched this information with company personnel records on earnings and other variables. The company earnings records are highly accurate and were treated as error-free in our analyses.",
            "paragraph_rank": 26,
            "section_rank": 5
        },
        {
            "section": "Multiple Imputations",
            "text": "The first step in the multiple imputation process is to estimate an imputation model for company record earnings in the PSIDVS as a function of an error-prone survey earnings variable and the other variables as the explanatory variables. The estimated imputation model is used to provide multiply imputed estimates of true earnings in the main data set. The procedure is completed by combining the multiply imputed estimates of true earnings with observed values of the explanatory variables and using the pooled data for estimating the final earnings equation.",
            "paragraph_rank": 27,
            "section_rank": 5
        },
        {
            "section": "Multiple Imputations",
            "text": "In our analysis of data for income years 1982 and 1986, we found that measurement error biases several coefficients from the earnings equation and often causes underestimation of their sampling variance. In both cross-section and longitudinal equations, accounting for measurement error reduces the estimated effect of union membership on earnings and causes the negative effect of working in a blue-collar occupation to become less negative or perhaps positive. We also found that measurement error in earnings appears to pose more significant problems during recessionary periods than expansionary periods, probably because errors in reported earnings are based largely on misperceptions of hours worked. Moreover, the standard errors in the earnings equations typically were increased by the multiple imputation procedure, because the procedure correctly accounts for sampling error that is ignored in analyses that treat earnings as an error-free variable. Clearly, multiple imputation can make a substantial difference in both qualitative conclusions and statistical accuracy.",
            "paragraph_rank": 28,
            "section_rank": 5
        },
        {
            "text": "Conclusions",
            "section_rank": 6
        },
        {
            "section": "Conclusions",
            "text": "Grand claims sometimes have been made for bootstrap analysis. For instance, Efron and Tibshirani (1993) and Vinod (1998) envision the bootstrap as part of a strategy to find universally applicable methods for estimation and inference, which can be implemented with very little effort or analysis by researchers. This vision is tempting, especially given the ease and speed with which bootstrap estimates for many models can be obtained using modern desktop computers. However, Manski (1996) argues that this vision is flawed due to the inherent ambiguity of statistical theory in comparing alternative estimation procedures. Moreover, the current state of the art in bootstrap methods cannot support such universal application because the necessary evidence regarding bootstrap superiority (or even consistency) is not available for many classes of models. At this point, however, the bootstrap can effectively serve the less grandiose but very practical purpose of deriving better confidence bands in models characterized by deviations from a variety of assumptions commonly made in least squares analysis.",
            "paragraph_rank": 29,
            "section_rank": 6,
            "ref_spans": [
                {
                    "type": "bibr",
                    "start": 76,
                    "text": "Efron and Tibshirani (1993)",
                    "end": 103
                },
                {
                    "type": "bibr",
                    "ref_id": "b15",
                    "start": 108,
                    "text": "Vinod (1998)",
                    "end": 120
                },
                {
                    "type": "bibr",
                    "ref_id": "b10",
                    "start": 477,
                    "text": "Manski (1996)",
                    "end": 490
                }
            ]
        },
        {
            "section": "Conclusions",
            "text": "Similarly, multiple imputations is an approach that applied economists should consider to help alleviate problems caused by survey nonresponse, missing data and measurement error. The best way to circumvent these problems is to put more resources into reducing response biases during survey administration. The secondbest solutions include the collection of external validation data or incorporation of confidential survey information to enable accurate estimation of the nonresponse or error process. Under these second-best circumstances, the multiple imputation methods presented in this paper provide a straightforward and consistent way for researchers to adjust for missing and erroneous data in their modeling and forecasting efforts. Multiple imputation is like an adjustable crescent wrench-it rarely is the ideal tool for any particular job, but it works well for a wide variety of problems.",
            "paragraph_rank": 30,
            "section_rank": 6
        },
        {
            "section": "Conclusions",
            "text": "y We gratefully acknowledge the unusually thorough and helpful comments from the editors.",
            "paragraph_rank": 31,
            "section_rank": 6
        },
        {
            "section": "Conclusions",
            "text": "Much of this work was completed while the second author was visiting at the Organization for Economic Cooperation and Development (OECD) in Paris, France. The views expressed in this paper are solely those of the authors and do not necessarily represent the views of the Federal Reserve Bank of San Francisco, the Federal Reserve System or the OECD. The authors also are solely responsible for any errors or omissions.",
            "paragraph_rank": 32,
            "section_rank": 6
        },
        {
            "text": "Table 1 Bootstrap Confidence Intervals, Regression Equation, Log Yearly Earnings, Male Heads, PSID, 1984 -1993, 2 Percent Sample",
            "section_rank": 7
        },
        {
            "section": "Table 1 Bootstrap Confidence Intervals, Regression Equation, Log Yearly Earnings, Male Heads, PSID, 1984 -1993, 2 Percent Sample",
            "text": ".365 \u03ea.717, \u03ea.042 \u03ea.971, .212 \u03ea1.238, .292 a \u03ea.965, .208 Married .076 \u03ea.228, .596 \u03ea.129, .281 \u03ea.407, .559 \u03ea.395, .599 a \u03ea.413, .581 MSA .164 \u03ea.174, .508 .003, .326 \u03ea.114, .443 \u03ea.156, .445 a \u03ea.119, .479 a Denotes closest confidence region to the true confidence bands as measured by the sum of the absolute deviations from the upper and lower bands, respectively.",
            "paragraph_rank": 33,
            "section_rank": 7
        },
        {
            "section": "Table 1 Bootstrap Confidence Intervals, Regression Equation, Log Yearly Earnings, Male Heads, PSID, 1984 -1993, 2 Percent Sample",
            "text": "Notes: The sample is restricted to male heads aged 21-64 and employed in the private sector. Other variables controlled for include three region dummies, nine year dummies and a constant.",
            "paragraph_rank": 34,
            "section_rank": 7
        },
        {
            "section": "Table 1 Bootstrap Confidence Intervals, Regression Equation, Log Yearly Earnings, Male Heads, PSID, 1984 -1993, 2 Percent Sample",
            "text": "Here are two other common ways to implement the bootstrap in this case. In a nonparametric residual bootstrap, first estimate an ordinary least squares regression equation and then sample from the residuals; in this case, the bootstrap distribution will be the empirical distribution function of the least squares residuals. In a parametric residual bootstrap, one again generates a resampled residual vector, but the sampling is done independently from a standard normal distribution. The parametric approach is consistent only if the error terms follow a normal distribution, in which case it yields the most accurate estimates. The nonparametric approach provides consistent estimates as long as the errors are homoskedastic. Neither the parametric nor the nonparametric approach provides consistent estimates if the true errors are heteroskedastic, whereas the paired approach described in the text does.2 We are referring here to \"asymptotically pivotal\" statistics, whose asymptotic distributions do not depend on unknown population parameters; see Horowitz (2001) for discussion.",
            "paragraph_rank": 35,
            "section_rank": 7,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b20",
                    "start": 908,
                    "text": "2",
                    "end": 909
                }
            ]
        },
        {
            "section": "Table 1 Bootstrap Confidence Intervals, Regression Equation, Log Yearly Earnings, Male Heads, PSID, 1984 -1993, 2 Percent Sample",
            "text": " See Efron and Tibshirani (1993)  regarding the appropriate number of repetitions and calculation of confidence intervals.",
            "paragraph_rank": 36,
            "section_rank": 7
        },
        {
            "section": "Table 1 Bootstrap Confidence Intervals, Regression Equation, Log Yearly Earnings, Male Heads, PSID, 1984 -1993, 2 Percent Sample",
            "text": " Mammen (1992, chapter 8)  shows that the distribution of the wild bootstrap converges to the correct sampling distribution faster than the paired bootstrap, and Horowitz (2001, section 5.2) gives supporting simulation evidence.",
            "paragraph_rank": 37,
            "section_rank": 7
        },
        {
            "section": "Table 1 Bootstrap Confidence Intervals, Regression Equation, Log Yearly Earnings, Male Heads, PSID, 1984 -1993, 2 Percent Sample",
            "text": "Meng (2000) shows that the process of drawing proper imputations is identical to Bayesian data augmentation(Tanner and Wong, 1987).",
            "paragraph_rank": 38,
            "section_rank": 7,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b11",
                    "start": 0,
                    "text": "Meng (2000)",
                    "end": 11
                },
                {
                    "type": "bibr",
                    "ref_id": "b14",
                    "start": 107,
                    "text": "(Tanner and Wong, 1987)",
                    "end": 130
                }
            ]
        },
        {
            "section": "Table 1 Bootstrap Confidence Intervals, Regression Equation, Log Yearly Earnings, Male Heads, PSID, 1984 -1993, 2 Percent Sample",
            "text": "In a similar vein, the U.S. Bureau of Labor Statistics is experimenting with multiple imputation as a solution to income item nonresponse in the Consumer Expenditure Survey(Paulin, 1999).9 Brownstone, Golob and Kazimi (1999) and Clogg et al.(1991)  show that multiple imputations also can be implemented with discrete choice models.",
            "paragraph_rank": 39,
            "section_rank": 7,
            "ref_spans": [
                {
                    "type": "bibr",
                    "ref_id": "b12",
                    "start": 172,
                    "text": "(Paulin, 1999",
                    "end": 185
                },
                {
                    "type": "bibr",
                    "ref_id": "b27",
                    "start": 187,
                    "text": "9",
                    "end": 188
                }
            ]
        }
    ]
}