{
    "level": "sentence",
    "abstract": [
        {
            "text": "The use of multivariate classifiers has become commonplace in particle physics. ",
            "paragraph_rank": 1,
            "section_rank": 1
        },
        {
            "text": "To enhance the performance, a series of classifiers is typically trained; this is a technique known as boosting. ",
            "paragraph_rank": 1,
            "section_rank": 1
        },
        {
            "text": "This paper explores several novel boosting methods that have been designed to produce a uniform selection efficiency in a chosen multivariate space. ",
            "paragraph_rank": 1,
            "section_rank": 1
        },
        {
            "text": "Such algorithms have a wide range of applications in particle physics, from producing uniform signal selection efficiency across a Dalitzplot to avoiding the creation of false signal peaks in an invariant mass distribution when searching for new particles.",
            "paragraph_rank": 1,
            "section_rank": 1
        }
    ],
    "body_text": [
        {
            "text": "Introduction",
            "section_rank": 2
        },
        {
            "text": "Methods of machine learning play an important role in modern particles physics. ",
            "section": "Introduction",
            "paragraph_rank": 2,
            "section_rank": 2
        },
        {
            "text": "Multivariate classifiers, e.g., boosted decision trees (BDTs) and artificial neural networks (ANNs), are now commonly used in analysis selection criteria. ",
            "section": "Introduction",
            "paragraph_rank": 2,
            "section_rank": 2
        },
        {
            "text": "BDTs are now even used in software triggers [1,2]. ",
            "section": "Introduction",
            "paragraph_rank": 2,
            "section_rank": 2,
            "ref_spans": [
                {
                    "start": 44,
                    "end": 47,
                    "type": "bibr",
                    "ref_id": "b0",
                    "text": "[1,"
                },
                {
                    "start": 47,
                    "end": 49,
                    "type": "bibr",
                    "ref_id": "b1",
                    "text": "2]"
                }
            ]
        },
        {
            "text": "To enhance the performance, a series of classifiers is typically trained; this is a technique known as boosting. ",
            "section": "Introduction",
            "paragraph_rank": 2,
            "section_rank": 2
        },
        {
            "text": "Boosting involves training many simple classifiers and then building a single composite classifier from their responses. ",
            "section": "Introduction",
            "paragraph_rank": 2,
            "section_rank": 2
        },
        {
            "text": "The classifiers are trained in series with the inputs of each member being augmented based on the performance of its predecessors. ",
            "section": "Introduction",
            "paragraph_rank": 2,
            "section_rank": 2
        },
        {
            "text": "This augmentation is designed such that each new classifier targets those events which were poorly classified by previous members of the series. ",
            "section": "Introduction",
            "paragraph_rank": 2,
            "section_rank": 2
        },
        {
            "text": "The classifier obtained by combining all members of the series is typically much more powerful than any of the individual members.",
            "section": "Introduction",
            "paragraph_rank": 2,
            "section_rank": 2
        },
        {
            "text": "In particle physics, the most common usage of BDTs is in classifying candidates as signal or background. ",
            "section": "Introduction",
            "paragraph_rank": 3,
            "section_rank": 2
        },
        {
            "text": "The BDT is determined by optimizing some figure of merit (FOM), e.g., the signal purity or approximate signal significance. ",
            "section": "Introduction",
            "paragraph_rank": 3,
            "section_rank": 2
        },
        {
            "text": "This approach is optimal for a counting experiment; however, in many cases the BDT-based selection obtained in this way is not optimal. ",
            "section": "Introduction",
            "paragraph_rank": 3,
            "section_rank": 2
        },
        {
            "text": "For example, in a Dalitz-plot (or any angular or amplitude analysis) analysis, obtaining a selection efficiency for signal candidates that is uniform across the Dalitz-plot is more important than any integrated FOM. ",
            "section": "Introduction",
            "paragraph_rank": 3,
            "section_rank": 2
        },
        {
            "text": "Similarly, when measuring a mean particle lifetime, obtaining an efficiency that is uniform in lifetime is what is desired. ",
            "section": "Introduction",
            "paragraph_rank": 3,
            "section_rank": 2
        },
        {
            "text": "In both cases, obtaining a uniform selection efficiency greatly reduces the systematic uncertainties involved in the measurement. ",
            "section": "Introduction",
            "paragraph_rank": 3,
            "section_rank": 2
        },
        {
            "text": "When searching for a new particle, an analyst may want a uniform efficiency in mass for selecting background candidates so that the BDTbased selection does not generate a fake signal peak. ",
            "section": "Introduction",
            "paragraph_rank": 3,
            "section_rank": 2
        },
        {
            "text": "Furthermore, the analyst may also desire a uniform selection efficiency of signal candidates in mass (or other variates) since the mass of the new particle is not known. ",
            "section": "Introduction",
            "paragraph_rank": 3,
            "section_rank": 2
        },
        {
            "text": "In such cases, the BDT is often trained on simulated data generated with several values of mass (lifetime, etc.). ",
            "section": "Introduction",
            "paragraph_rank": 3,
            "section_rank": 2
        },
        {
            "text": "A uniform selection efficiency in mass ensures that the BDT is sensitive to the full range of masses involved in the search.",
            "section": "Introduction",
            "paragraph_rank": 3,
            "section_rank": 2
        },
        {
            "text": "Uniformity Boosting Methods",
            "section_rank": 3
        },
        {
            "text": "The variates used in the BDT are denoted by x, while the variates in which uniformity is desired are denoted by y. ",
            "section": "Uniformity Boosting Methods",
            "paragraph_rank": 4,
            "section_rank": 3
        },
        {
            "text": "Some (perhaps all) of the x variates will be biasing in y, i.e. they provide discriminating power between signal and background that varies in y. ",
            "section": "Uniformity Boosting Methods",
            "paragraph_rank": 4,
            "section_rank": 3
        },
        {
            "text": "A uniform BDT selection efficiency can be obtained by removing all such variates; however, this will also reduce the power of the BDT. ",
            "section": "Uniformity Boosting Methods",
            "paragraph_rank": 4,
            "section_rank": 3
        },
        {
            "text": "The goal of boosting algorithms presented in this paper is to balance the biases to produce the optimal uniform selection.",
            "section": "Uniformity Boosting Methods",
            "paragraph_rank": 4,
            "section_rank": 3
        },
        {
            "text": "One category of boosting works by assigning training events more weight based on classification errors made by previous members of the series. ",
            "section": "Uniformity Boosting Methods",
            "paragraph_rank": 5,
            "section_rank": 3
        },
        {
            "text": "For example, the AdaBoost [3] algorithm updates the weight of event i, w i , according to",
            "section": "Uniformity Boosting Methods",
            "paragraph_rank": 5,
            "section_rank": 3,
            "ref_spans": [
                {
                    "start": 26,
                    "end": 29,
                    "type": "bibr",
                    "ref_id": "b2",
                    "text": "[3]"
                }
            ],
            "entity_spans": [
                {
                    "start": 17,
                    "end": 26,
                    "type": "software",
                    "rawForm": "AdaBoost",
                    "resp": "service",
                    "id": "software-simple-s1"
                }
            ]
        },
        {
            "text": "where \u03b3 = +1(\u22121) for signal(background) events and p is the prediction for each event produced by last classifier in the series. ",
            "section": "Uniformity Boosting Methods",
            "paragraph_rank": 6,
            "section_rank": 3
        },
        {
            "text": "The uBoost technique, described in detail in Ref. ",
            "section": "Uniformity Boosting Methods",
            "paragraph_rank": 6,
            "section_rank": 3,
            "entity_spans": [
                {
                    "start": 4,
                    "end": 11,
                    "type": "software",
                    "rawForm": "uBoost",
                    "resp": "service",
                    "id": "software-simple-s2"
                }
            ]
        },
        {
            "text": "[4], alters the event-weight updating procedure to achieve uniformity in the signal-selection efficiency. ",
            "section": "Uniformity Boosting Methods",
            "paragraph_rank": 6,
            "section_rank": 3,
            "ref_spans": [
                {
                    "start": 0,
                    "end": 3,
                    "type": "bibr",
                    "ref_id": "b3",
                    "text": "[4]"
                }
            ]
        },
        {
            "text": "Another approach to obtain uniformity, introduced in this paper, involves defining a more general expression of the AdaBoost criteria:",
            "section": "Uniformity Boosting Methods",
            "paragraph_rank": 6,
            "section_rank": 3,
            "entity_spans": [
                {
                    "start": 116,
                    "end": 125,
                    "type": "software",
                    "rawForm": "AdaBoost",
                    "resp": "service",
                    "id": "software-simple-s3"
                }
            ]
        },
        {
            "text": "where a i j are the elements of some square matrix A. For the case where A is the identity matrix, the AdaBoost weighting procedure is recovered. ",
            "section": "Uniformity Boosting Methods",
            "paragraph_rank": 7,
            "section_rank": 3,
            "entity_spans": [
                {
                    "start": 103,
                    "end": 112,
                    "type": "software",
                    "rawForm": "AdaBoost",
                    "resp": "service",
                    "id": "software-simple-s4"
                }
            ]
        },
        {
            "text": "Other choices of A will induce non-local effects, e.g., consider the sparse matrix A knn given by",
            "section": "Uniformity Boosting Methods",
            "paragraph_rank": 7,
            "section_rank": 3
        },
        {
            "text": "k , j \u2208 knn(i), events i and j belong to the same class 0, otherwise,",
            "section": "Uniformity Boosting Methods",
            "paragraph_rank": 8,
            "section_rank": 3
        },
        {
            "text": "where knn(i) denotes the set of k-nearest-neighbor events to event i. ",
            "section": "Uniformity Boosting Methods",
            "paragraph_rank": 9,
            "section_rank": 3
        },
        {
            "text": "This procedure for updating the event weights, which we refer to as kNNAdaBoost, accounts for the score of each event's k nearest neighbors and not just each event individually. ",
            "section": "Uniformity Boosting Methods",
            "paragraph_rank": 9,
            "section_rank": 3,
            "entity_spans": [
                {
                    "start": 68,
                    "end": 79,
                    "type": "software",
                    "rawForm": "kNNAdaBoost",
                    "resp": "service",
                    "id": "software-simple-s5"
                }
            ]
        },
        {
            "text": "The gradient boosting [5] (GB) algorithm category requires the analyst to choose a differentiable loss function with the goal of building a classifier that minimizes the loss. ",
            "section": "Uniformity Boosting Methods",
            "paragraph_rank": 9,
            "section_rank": 3,
            "ref_spans": [
                {
                    "start": 22,
                    "end": 25,
                    "type": "bibr",
                    "ref_id": "b4",
                    "text": "[5]"
                }
            ]
        },
        {
            "text": "A popular choice of loss function is the so-called AdaLoss function",
            "section": "Uniformity Boosting Methods",
            "paragraph_rank": 9,
            "section_rank": 3
        },
        {
            "text": "The scores s are obtained for each event as the sum of predictions of all elements in the series. ",
            "section": "Uniformity Boosting Methods",
            "paragraph_rank": 10,
            "section_rank": 3
        },
        {
            "text": "At each stage in the gradient boosting process, a regressor (a decision tree in our case) is trained whose purpose is to decrease the loss. ",
            "section": "Uniformity Boosting Methods",
            "paragraph_rank": 10,
            "section_rank": 3
        },
        {
            "text": "This is accomplished using the gradient decent method and the pseudo-residuals",
            "section": "Uniformity Boosting Methods",
            "paragraph_rank": 10,
            "section_rank": 3
        },
        {
            "text": "which are positive(negative) for signal(background) events and have larger moduli for poorly classified events. ",
            "section": "Uniformity Boosting Methods",
            "paragraph_rank": 11,
            "section_rank": 3
        },
        {
            "text": "The gradient-boosting algorithm is general in that it only requires the analyst specify a loss function and its gradient. ",
            "section": "Uniformity Boosting Methods",
            "paragraph_rank": 11,
            "section_rank": 3
        },
        {
            "text": "The AdaLoss function considers each event individually, but can easily be modified to take into account non-local properties of the classifier as follows:",
            "section": "Uniformity Boosting Methods",
            "paragraph_rank": 11,
            "section_rank": 3,
            "entity_spans": [
                {
                    "start": 4,
                    "end": 12,
                    "type": "software",
                    "rawForm": "AdaLoss",
                    "resp": "service",
                    "id": "software-simple-s6"
                }
            ]
        },
        {
            "text": "For example, the loss function obtained from Eq. ",
            "section": "Uniformity Boosting Methods",
            "paragraph_rank": 12,
            "section_rank": 3
        },
        {
            "text": "2.6 using 1 A knn , which we refer to as kN-NAdaLoss and denote L knn , accounts for the score of each event's k nearest neighbors and not just each event individually. ",
            "section": "Uniformity Boosting Methods",
            "paragraph_rank": 12,
            "section_rank": 3,
            "entity_spans": [
                {
                    "start": 41,
                    "end": 53,
                    "type": "software",
                    "rawForm": "kN-NAdaLoss",
                    "resp": "service",
                    "id": "software-simple-s7"
                }
            ]
        },
        {
            "text": "The pseudo-residuals of L knn are",
            "section": "Uniformity Boosting Methods",
            "paragraph_rank": 12,
            "section_rank": 3
        },
        {
            "text": "One can see that the direction of the gradient will be influenced the most by events whose k-nearestneighbor events are classified poorly. ",
            "section": "Uniformity Boosting Methods",
            "paragraph_rank": 13,
            "section_rank": 3
        },
        {
            "text": "We generically refer to GB methods designed to achieve uniform selection efficiency as uniform GB (uGB). ",
            "section": "Uniformity Boosting Methods",
            "paragraph_rank": 13,
            "section_rank": 3
        },
        {
            "text": "The specific algorithm that uses kNNAdaLoss will be called uGBkNN.",
            "section": "Uniformity Boosting Methods",
            "paragraph_rank": 13,
            "section_rank": 3,
            "entity_spans": [
                {
                    "start": 33,
                    "end": 44,
                    "type": "software",
                    "rawForm": "kNNAdaLoss",
                    "resp": "service",
                    "id": "software-simple-s8"
                },
                {
                    "start": 59,
                    "end": 65,
                    "type": "software",
                    "rawForm": "uGBkNN",
                    "resp": "service",
                    "id": "software-simple-s9"
                }
            ]
        },
        {
            "text": "Another approach is to include some uniformity metric in the definition of the loss function. ",
            "section": "Uniformity Boosting Methods",
            "paragraph_rank": 14,
            "section_rank": 3
        },
        {
            "text": "Consider first the case where the data have been binned in y. ",
            "section": "Uniformity Boosting Methods",
            "paragraph_rank": 14,
            "section_rank": 3
        },
        {
            "text": "If the distribution of classifier responses in each bin, f b (s), is the same as the global response distribution, f (s), then any cut made on the response will produce a uniform selection efficiency in y. ",
            "section": "Uniformity Boosting Methods",
            "paragraph_rank": 14,
            "section_rank": 3
        },
        {
            "text": "Therefore, performing a onedimensional goodness-of-fit test of the hypothesis f b \u2261 f in each bin provides an assessment of the selection uniformity. ",
            "section": "Uniformity Boosting Methods",
            "paragraph_rank": 14,
            "section_rank": 3
        },
        {
            "text": "For example, one could perform the Kolmogorov-Smirnov test in each bin and define a loss function as follows:",
            "section": "Uniformity Boosting Methods",
            "paragraph_rank": 14,
            "section_rank": 3
        },
        {
            "text": "where",
            "section": "Uniformity Boosting Methods",
            "paragraph_rank": 15,
            "section_rank": 3
        },
        {
            "text": "is the fraction of signal events in the bin 2 . ",
            "section": "Uniformity Boosting Methods",
            "paragraph_rank": 16,
            "section_rank": 3,
            "ref_spans": [
                {
                    "start": 44,
                    "end": 45,
                    "type": "bibr",
                    "ref_id": "b1",
                    "text": "2"
                }
            ]
        },
        {
            "text": "The gradient of the Kolmogorov-Smirnov loss function is zero for events with responses greater than the value of s at which max|F b (s) \u2212 F(s)| occurs. ",
            "section": "Uniformity Boosting Methods",
            "paragraph_rank": 16,
            "section_rank": 3
        },
        {
            "text": "Thus, it is not suitable for gradient boosting due to its instability. ",
            "section": "Uniformity Boosting Methods",
            "paragraph_rank": 16,
            "section_rank": 3
        },
        {
            "text": "Instead, we use the following flatness loss function: ",
            "section": "Uniformity Boosting Methods",
            "paragraph_rank": 16,
            "section_rank": 3
        },
        {
            "text": "This so-called flatness loss penalizes non-uniformity but does not consider the quality of the classification. ",
            "section": "Uniformity Boosting Methods",
            "paragraph_rank": 17,
            "section_rank": 3
        },
        {
            "text": "Therefore, the full loss function used is",
            "section": "Uniformity Boosting Methods",
            "paragraph_rank": 17,
            "section_rank": 3
        },
        {
            "text": "where \u03b1 is a real-valued parameter that is typically chosen to be small. ",
            "section": "Uniformity Boosting Methods",
            "paragraph_rank": 18,
            "section_rank": 3
        },
        {
            "text": "The first term in Eq. ",
            "section": "Uniformity Boosting Methods",
            "paragraph_rank": 18,
            "section_rank": 3
        },
        {
            "text": "2.11 penalizes non-uniformity, while the second term penalizes poor classification. ",
            "section": "Uniformity Boosting Methods",
            "paragraph_rank": 18,
            "section_rank": 3
        },
        {
            "text": "We refer to this algorithm as uGB with flatness loss (uGBFL). ",
            "section": "Uniformity Boosting Methods",
            "paragraph_rank": 18,
            "section_rank": 3,
            "entity_spans": [
                {
                    "start": 30,
                    "end": 34,
                    "type": "software",
                    "rawForm": "uGB",
                    "resp": "service",
                    "id": "software-simple-s10"
                },
                {
                    "start": 54,
                    "end": 59,
                    "type": "software",
                    "rawForm": "uGBFL",
                    "resp": "service",
                    "id": "software-simple-s11"
                }
            ]
        },
        {
            "text": "In principle, many different flatness loss functions can be defined and could be substituted for our choice here. ",
            "section": "Uniformity Boosting Methods",
            "paragraph_rank": 18,
            "section_rank": 3
        },
        {
            "text": "See Appendix A for a detailed discussion on this topic. ",
            "section": "Uniformity Boosting Methods",
            "paragraph_rank": 18,
            "section_rank": 3
        },
        {
            "text": "The loss function given in Eq. ",
            "section": "Uniformity Boosting Methods",
            "paragraph_rank": 18,
            "section_rank": 3
        },
        {
            "text": "2.11 can also be constructed without binning the data using knearest-neighbor events. ",
            "section": "Uniformity Boosting Methods",
            "paragraph_rank": 18,
            "section_rank": 3
        },
        {
            "text": "The cumulative distribution F knn (s) is easily obtained and the bin weight, w b , is replaced by a k-nearest-neighbor weight, w knn . ",
            "section": "Uniformity Boosting Methods",
            "paragraph_rank": 18,
            "section_rank": 3
        },
        {
            "text": "First, each event is weighted by the inverse of the number of times it is included in the k-nearest-neighbor sample of another event. ",
            "section": "Uniformity Boosting Methods",
            "paragraph_rank": 18,
            "section_rank": 3
        },
        {
            "text": "Then, w knn is the sum of such weights in a k-nearest-neighbor sample divided by the total sum of such weights in the full sample. ",
            "section": "Uniformity Boosting Methods",
            "paragraph_rank": 18,
            "section_rank": 3
        },
        {
            "text": "This procedure is followed to offset the fact that some events are found in more k-nearest-neighbor samples than other events. ",
            "section": "Uniformity Boosting Methods",
            "paragraph_rank": 18,
            "section_rank": 3
        },
        {
            "text": "We study two versions of uGBFL below: uGBFL using bins denoted by uGBFL(bin) and uGBFL using kNN collections denoted by uGBFL(kNN). ",
            "section": "Uniformity Boosting Methods",
            "paragraph_rank": 18,
            "section_rank": 3,
            "entity_spans": [
                {
                    "start": 25,
                    "end": 31,
                    "type": "software",
                    "rawForm": "uGBFL",
                    "resp": "service",
                    "id": "software-simple-s12"
                },
                {
                    "start": 38,
                    "end": 44,
                    "type": "software",
                    "rawForm": "uGBFL",
                    "resp": "service",
                    "id": "software-simple-s13"
                },
                {
                    "start": 66,
                    "end": 71,
                    "type": "software",
                    "rawForm": "uGBFL",
                    "resp": "service",
                    "id": "software-simple-s14"
                },
                {
                    "start": 81,
                    "end": 87,
                    "type": "software",
                    "rawForm": "uGBFL",
                    "resp": "service",
                    "id": "software-simple-s15"
                },
                {
                    "start": 120,
                    "end": 125,
                    "type": "software",
                    "rawForm": "uGBFL",
                    "resp": "service",
                    "id": "software-simple-s16"
                }
            ]
        },
        {
            "text": "The algorithms are summarized in Table 1.",
            "section": "Uniformity Boosting Methods",
            "paragraph_rank": 18,
            "section_rank": 3,
            "ref_spans": [
                {
                    "start": 33,
                    "end": 40,
                    "type": "table",
                    "ref_id": "tab_0",
                    "text": "Table 1"
                }
            ]
        },
        {
            "text": "Example Analysis",
            "section_rank": 4
        },
        {
            "text": "The example analysis studied here involves a so-called Daltiz-plot analysis. ",
            "section": "Example Analysis",
            "paragraph_rank": 19,
            "section_rank": 4
        },
        {
            "text": "In such analyses, the distribution of events in a 2-D space is typically fit to extract some information of physical interest. ",
            "section": "Example Analysis",
            "paragraph_rank": 19,
            "section_rank": 4
        },
        {
            "text": "The regions of the Daltiz-plot that tend to have the highest sensitivity to the desired information are the edges. ",
            "section": "Example Analysis",
            "paragraph_rank": 19,
            "section_rank": 4
        },
        {
            "text": "Unfortunately, the edge regions also typically have the most background contamination and the least discrimination against background. ",
            "section": "Example Analysis",
            "paragraph_rank": 19,
            "section_rank": 4
        },
        {
            "text": "Therefore, traditional classifier-based selections tend to produce selections for Dalitz-plot analyses with lower efficiency near the edges.",
            "section": "Example Analysis",
            "paragraph_rank": 19,
            "section_rank": 4
        },
        {
            "text": "This study uses simulated event samples produced using the official LHCb simulation framework. ",
            "section": "Example Analysis",
            "paragraph_rank": 20,
            "section_rank": 4
        },
        {
            "text": "The software used for the generation of the events is described in LHCb publications as follows : In the simulation, pp collisions are generated using PYTHIA [6] with a specific LHCb configuration [7]. ",
            "section": "Example Analysis",
            "paragraph_rank": 20,
            "section_rank": 4,
            "ref_spans": [
                {
                    "start": 158,
                    "end": 161,
                    "type": "bibr",
                    "ref_id": "b5",
                    "text": "[6]"
                },
                {
                    "start": 197,
                    "end": 200,
                    "type": "bibr",
                    "ref_id": "b6",
                    "text": "[7]"
                }
            ],
            "entity_spans": [
                {
                    "start": 151,
                    "end": 158,
                    "type": "software",
                    "rawForm": "PYTHIA",
                    "resp": "service",
                    "id": "software-simple-s17"
                }
            ]
        },
        {
            "text": "Decays of hadronic particles are described by EvtGen [8], in which final state radiation is generated using PHOTOS [9]. ",
            "section": "Example Analysis",
            "paragraph_rank": 20,
            "section_rank": 4,
            "ref_spans": [
                {
                    "start": 53,
                    "end": 56,
                    "type": "bibr",
                    "ref_id": "b7",
                    "text": "[8]"
                },
                {
                    "start": 115,
                    "end": 118,
                    "type": "bibr",
                    "ref_id": "b8",
                    "text": "[9]"
                }
            ],
            "entity_spans": [
                {
                    "start": 46,
                    "end": 53,
                    "type": "software",
                    "rawForm": "EvtGen",
                    "resp": "service",
                    "id": "software-simple-s18"
                }
            ]
        },
        {
            "text": "The interaction of the generated particles with the detector and its response are implemented using the GEANT toolkit [10,11] as described in Ref. ",
            "section": "Example Analysis",
            "paragraph_rank": 20,
            "section_rank": 4,
            "ref_spans": [
                {
                    "start": 118,
                    "end": 122,
                    "type": "bibr",
                    "ref_id": "b9",
                    "text": "[10,"
                },
                {
                    "start": 122,
                    "end": 125,
                    "type": "bibr",
                    "ref_id": "b10",
                    "text": "11]"
                }
            ],
            "entity_spans": [
                {
                    "start": 104,
                    "end": 110,
                    "type": "software",
                    "rawForm": "GEANT",
                    "resp": "service",
                    "id": "software-simple-s19"
                }
            ]
        },
        {
            "text": "[12].",
            "section": "Example Analysis",
            "paragraph_rank": 20,
            "section_rank": 4,
            "ref_spans": [
                {
                    "start": 0,
                    "end": 4,
                    "type": "bibr",
                    "ref_id": "b11",
                    "text": "[12]"
                }
            ]
        },
        {
            "text": "All simulated event samples are generated inside the LHCb detector acceptance. ",
            "section": "Example Analysis",
            "paragraph_rank": 21,
            "section_rank": 4
        },
        {
            "text": "The signal used in this analysis consists of D \u00b1 s \u2192 \u03c0 + \u03c0 \u2212 \u03c0 \u00b1 decays, simulated using the D_DALITZ model of EvtGen to simulate the intermediate resonances which contribute to the three pion final state. ",
            "section": "Example Analysis",
            "paragraph_rank": 21,
            "section_rank": 4
        },
        {
            "text": "The background candidates are three pion combinations reconstructed in simulated samples of cc and bb events, where the charm and bottom quark decays are inclusively modelled by EvtGen. ",
            "section": "Example Analysis",
            "paragraph_rank": 21,
            "section_rank": 4
        },
        {
            "text": "The simulated events contain \"truth\" information which identifies them as signal or background, and which identifies the physical origin of the three pion combinations reconstructed in the cc and bb simulated samples. ",
            "section": "Example Analysis",
            "paragraph_rank": 21,
            "section_rank": 4
        },
        {
            "text": "Figure 1 shows the Dalitz-plot distributions for signal and background events. ",
            "section": "Example Analysis",
            "paragraph_rank": 21,
            "section_rank": 4,
            "ref_spans": [
                {
                    "start": 0,
                    "end": 8,
                    "type": "figure",
                    "ref_id": "fig_0",
                    "text": "Figure 1"
                }
            ]
        },
        {
            "text": "These samples are split into training and testing samples and then various BDTs are trained. ",
            "section": "Example Analysis",
            "paragraph_rank": 21,
            "section_rank": 4
        },
        {
            "text": "For the BDTs designed to produce uniform selections, the y variates are the Dalitz masses with the choice of uniform selection efficiency on signal candidates in the Dalitz-plot. ",
            "section": "Example Analysis",
            "paragraph_rank": 21,
            "section_rank": 4
        },
        {
            "text": "Figure 2 shows the ROC curves obtained for the various classifiers studied in this paper. ",
            "section": "Example Analysis",
            "paragraph_rank": 21,
            "section_rank": 4,
            "ref_spans": [
                {
                    "start": 0,
                    "end": 8,
                    "type": "figure",
                    "text": "Figure 2"
                }
            ]
        },
        {
            "text": "For the uGBFL algorithms, there is a choice to be made for the value \u03b1 which defines the relative weight of the flatness loss vs AdaLoss. ",
            "section": "Example Analysis",
            "paragraph_rank": 21,
            "section_rank": 4,
            "entity_spans": [
                {
                    "start": 8,
                    "end": 14,
                    "type": "software",
                    "rawForm": "uGBFL",
                    "resp": "service",
                    "id": "software-simple-s20"
                },
                {
                    "start": 129,
                    "end": 136,
                    "type": "software",
                    "rawForm": "AdaLoss",
                    "resp": "service",
                    "id": "software-simple-s21"
                }
            ]
        },
        {
            "text": "As expected, increasing \u03b1, which increases the weight of AdaLoss, drives the ROC curve to be similar to AdaBoost. ",
            "section": "Example Analysis",
            "paragraph_rank": 21,
            "section_rank": 4,
            "entity_spans": [
                {
                    "start": 57,
                    "end": 64,
                    "type": "software",
                    "rawForm": "AdaLoss",
                    "resp": "service",
                    "id": "software-simple-s22"
                },
                {
                    "start": 104,
                    "end": 112,
                    "type": "software",
                    "rawForm": "AdaBoost",
                    "resp": "service",
                    "id": "software-simple-s23"
                }
            ]
        },
        {
            "text": "Analysts will need to choose how much ROC performance to sacrifice to gain uniformity in the selection efficiency. ",
            "section": "Example Analysis",
            "paragraph_rank": 21,
            "section_rank": 4
        },
        {
            "text": "In general, the ROC curves for the uniform-driven BDTs are not too different from AdaBoost. ",
            "section": "Example Analysis",
            "paragraph_rank": 21,
            "section_rank": 4,
            "entity_spans": [
                {
                    "start": 82,
                    "end": 90,
                    "type": "software",
                    "rawForm": "AdaBoost",
                    "resp": "service",
                    "id": "software-simple-s24"
                }
            ]
        },
        {
            "text": "Figure 3 shows how the uniformity of the selection efficiency depends on \u03b1. ",
            "section": "Example Analysis",
            "paragraph_rank": 21,
            "section_rank": 4,
            "ref_spans": [
                {
                    "start": 0,
                    "end": 8,
                    "type": "figure",
                    "text": "Figure 3"
                }
            ]
        },
        {
            "text": "As expected, as \u03b1 is decreased the selection becomes more uniform. ",
            "section": "Example Analysis",
            "paragraph_rank": 21,
            "section_rank": 4
        },
        {
            "text": "Figure 4 shows the efficiency obtained for each classifier vs distance from the a corner of the Dalitz-plot 3 . ",
            "section": "Example Analysis",
            "paragraph_rank": 21,
            "section_rank": 4,
            "ref_spans": [
                {
                    "start": 0,
                    "end": 8,
                    "type": "figure",
                    "text": "Figure 4"
                },
                {
                    "start": 108,
                    "end": 109,
                    "type": "bibr",
                    "ref_id": "b2",
                    "text": "3"
                }
            ]
        },
        {
            "text": "The AdaBoost algorithm, as expected, produces a much lower efficiency in the inter-  esting corner regions. ",
            "section": "Example Analysis",
            "paragraph_rank": 21,
            "section_rank": 4,
            "entity_spans": [
                {
                    "start": 4,
                    "end": 12,
                    "type": "software",
                    "rawForm": "AdaBoost",
                    "resp": "service",
                    "id": "software-simple-s25"
                }
            ]
        },
        {
            "text": "The kNNAdaBoost algorithm does not improve upon the AdaBoost result much. ",
            "section": "Example Analysis",
            "paragraph_rank": 21,
            "section_rank": 4,
            "entity_spans": [
                {
                    "start": 4,
                    "end": 16,
                    "type": "software",
                    "rawForm": "kNNAdaBoost",
                    "resp": "service",
                    "id": "software-simple-s26"
                },
                {
                    "start": 52,
                    "end": 60,
                    "type": "software",
                    "rawForm": "AdaBoost",
                    "resp": "service",
                    "id": "software-simple-s27"
                }
            ]
        },
        {
            "text": "This is likely due to the fact that while kNNAdaBoost uses non-local kNN information, it does not utilize global information. ",
            "section": "Example Analysis",
            "paragraph_rank": 21,
            "section_rank": 4,
            "entity_spans": [
                {
                    "start": 42,
                    "end": 54,
                    "type": "software",
                    "rawForm": "kNNAdaBoost",
                    "resp": "service",
                    "id": "software-simple-s28"
                }
            ]
        },
        {
            "text": "The uGBkNN algorithm overcompensates and drives the efficiency higher at the corners. ",
            "section": "Example Analysis",
            "paragraph_rank": 21,
            "section_rank": 4,
            "entity_spans": [
                {
                    "start": 4,
                    "end": 11,
                    "type": "software",
                    "rawForm": "uGBkNN",
                    "resp": "service",
                    "id": "software-simple-s29"
                }
            ]
        },
        {
            "text": "This suggests that if this algorithm is to be used some stopping criteria or throttle of the event-weighting updating should be implemented. ",
            "section": "Example Analysis",
            "paragraph_rank": 21,
            "section_rank": 4
        },
        {
            "text": "The uGBFL (binned and unbinned kNN) and uBoost algorithms each produce an efficiency which is statistically consistent with uniform across the Dalitz plot. ",
            "section": "Example Analysis",
            "paragraph_rank": 21,
            "section_rank": 4,
            "entity_spans": [
                {
                    "start": 4,
                    "end": 10,
                    "type": "software",
                    "rawForm": "uGBFL",
                    "resp": "service",
                    "id": "software-simple-s30"
                },
                {
                    "start": 40,
                    "end": 47,
                    "type": "software",
                    "rawForm": "uBoost",
                    "resp": "service",
                    "id": "software-simple-s31"
                }
            ]
        },
        {
            "text": "As stated above, the analyst is free to optimize the choice of \u03b1 for uGBFL by defining a metric that involves signal efficiency, background rejection and uniformity, e.g., using uniformity metrics discussed in detail in Appendix A.",
            "section": "Example Analysis",
            "paragraph_rank": 21,
            "section_rank": 4,
            "entity_spans": [
                {
                    "start": 69,
                    "end": 74,
                    "type": "software",
                    "rawForm": "uGBFL",
                    "resp": "service",
                    "id": "software-simple-s32"
                }
            ]
        },
        {
            "text": "As a separate study using the same data samples, consider the case where one has simulated signal events and uses data from a nearby region, a so-called sideband, for background. ",
            "section": "Example Analysis",
            "paragraph_rank": 22,
            "section_rank": 4
        },
        {
            "text": "This is a common situation in particle-physics analyses. ",
            "section": "Example Analysis",
            "paragraph_rank": 22,
            "section_rank": 4
        },
        {
            "text": "Figure 5 shows the training samples used. ",
            "section": "Example Analysis",
            "paragraph_rank": 22,
            "section_rank": 4,
            "ref_spans": [
                {
                    "start": 0,
                    "end": 8,
                    "type": "figure",
                    "text": "Figure 5"
                }
            ]
        },
        {
            "text": "A major problem can arise in these situations as typically input variates to the BDT are correlated with the parent particle mass. ",
            "section": "Example Analysis",
            "paragraph_rank": 22,
            "section_rank": 4
        },
        {
            "text": "Therefore, the BDT may learn to reject the background in the training using the fact that the mass of the background and signal candidates is different. ",
            "section": "Example Analysis",
            "paragraph_rank": 22,
            "section_rank": 4
        },
        {
            "text": "This is just an artifact of how the background sample is obtained and will not be true for background candidates under the signal peak. ",
            "section": "Example Analysis",
            "paragraph_rank": 22,
            "section_rank": 4
        },
        {
            "text": "Figure 5 shows the background mis-identification rate vs D candidate mass. ",
            "section": "Example Analysis",
            "paragraph_rank": 22,
            "section_rank": 4,
            "ref_spans": [
                {
                    "start": 0,
                    "end": 8,
                    "type": "figure",
                    "text": "Figure 5"
                }
            ]
        },
        {
            "text": "AdaBoost has clearly learned to use this mis-match in signal and background candidate masses in the training. ",
            "section": "Example Analysis",
            "paragraph_rank": 22,
            "section_rank": 4,
            "entity_spans": [
                {
                    "start": 0,
                    "end": 9,
                    "type": "software",
                    "rawForm": "AdaBoost",
                    "resp": "service",
                    "id": "software-simple-s33"
                }
            ]
        },
        {
            "text": "The background in the region of the signal is about three times higher than one would expect from looking only at the sideband data. ",
            "section": "Example Analysis",
            "paragraph_rank": 22,
            "section_rank": 4
        },
        {
            "text": "Figure 5 also shows the background mis-identification rate vs D candidate mass for the various uniform classifiers where y = m(D) and the choice is for uniformity in the background efficiency 4 . ",
            "section": "Example Analysis",
            "paragraph_rank": 22,
            "section_rank": 4,
            "ref_spans": [
                {
                    "start": 0,
                    "end": 8,
                    "type": "figure",
                    "text": "Figure 5"
                },
                {
                    "start": 192,
                    "end": 193,
                    "type": "bibr",
                    "ref_id": "b3",
                    "text": "4"
                }
            ]
        },
        {
            "text": "The uBoost algorithm does better than AdaBoost here but is still not optimal. ",
            "section": "Example Analysis",
            "paragraph_rank": 22,
            "section_rank": 4,
            "entity_spans": [
                {
                    "start": 4,
                    "end": 11,
                    "type": "software",
                    "rawForm": "uBoost",
                    "resp": "service",
                    "id": "software-simple-s34"
                },
                {
                    "start": 38,
                    "end": 47,
                    "type": "software",
                    "rawForm": "AdaBoost",
                    "resp": "service",
                    "id": "software-simple-s35"
                }
            ]
        },
        {
            "text": "The way that uBoost achieves uniformity is not such that it can be trusted to work outside the region of training. ",
            "section": "Example Analysis",
            "paragraph_rank": 22,
            "section_rank": 4,
            "entity_spans": [
                {
                    "start": 13,
                    "end": 20,
                    "type": "software",
                    "rawForm": "uBoost",
                    "resp": "service",
                    "id": "software-simple-s36"
                }
            ]
        },
        {
            "text": "The algorithms presented in this paper each does well in achieving similar performance in the training and signal regions. ",
            "section": "Example Analysis",
            "paragraph_rank": 22,
            "section_rank": 4
        },
        {
            "text": "Consider, e.g., the uGBFL approach to achieving uniform selection efficiency. ",
            "section": "Example Analysis",
            "paragraph_rank": 22,
            "section_rank": 4,
            "entity_spans": [
                {
                    "start": 20,
                    "end": 26,
                    "type": "software",
                    "rawForm": "uGBFL",
                    "resp": "service",
                    "id": "software-simple-s37"
                }
            ]
        },
        {
            "text": "In this case the training drives the BDT response itself to have the same PDF everywhere in the region 1.75 < m(D) < 1.85 GeV (the training region). ",
            "section": "Example Analysis",
            "paragraph_rank": 22,
            "section_rank": 4
        },
        {
            "text": "This does not guarantee that the BDT re-sponse is truly independent of m(D) outside the training region, but does strongly suppress learning to use m(D) and in this example results in the desired behavior. ",
            "section": "Example Analysis",
            "paragraph_rank": 22,
            "section_rank": 4
        },
        {
            "text": "Finally, if both high and low m(D) sidebands had been used, it is possible for a BDT to create a fake peak near the signal peak location. ",
            "section": "Example Analysis",
            "paragraph_rank": 22,
            "section_rank": 4
        },
        {
            "text": "The use of uGBFL greatly reduces the chances and possible size of such an effect.",
            "section": "Example Analysis",
            "paragraph_rank": 22,
            "section_rank": 4,
            "entity_spans": [
                {
                    "start": 11,
                    "end": 17,
                    "type": "software",
                    "rawForm": "uGBFL",
                    "resp": "service",
                    "id": "software-simple-s38"
                }
            ]
        },
        {
            "text": "CPU Resources",
            "section_rank": 5
        },
        {
            "text": "One drawback of the uBoost technique is that it has a high degree of computational complexity: while AdaBoost trains M trees (a user-defined number), uBoost builds 100 \u00d7 M trees. ",
            "section": "CPU Resources",
            "paragraph_rank": 23,
            "section_rank": 5,
            "entity_spans": [
                {
                    "start": 20,
                    "end": 27,
                    "type": "software",
                    "rawForm": "uBoost",
                    "resp": "service",
                    "id": "software-simple-s39"
                },
                {
                    "start": 101,
                    "end": 110,
                    "type": "software",
                    "rawForm": "AdaBoost",
                    "resp": "service",
                    "id": "software-simple-s40"
                },
                {
                    "start": 150,
                    "end": 157,
                    "type": "software",
                    "rawForm": "uBoost",
                    "resp": "service",
                    "id": "software-simple-s41"
                }
            ]
        },
        {
            "text": "The algorithms presented in this paper only build M trees; however, the boosting involves some more complicated algorithms. ",
            "section": "CPU Resources",
            "paragraph_rank": 23,
            "section_rank": 5
        },
        {
            "text": "Training each of the M trees scales as follows for N training events:",
            "section": "CPU Resources",
            "paragraph_rank": 23,
            "section_rank": 5
        },
        {
            "text": "\u2022 uGBkNNknn: O(k \u00d7 N) for A knn , and O(#nonzero elements in the matrix) for arbitrary matrix A;",
            "section": "CPU Resources",
            "paragraph_rank": 24,
            "section_rank": 5,
            "entity_spans": [
                {
                    "start": 2,
                    "end": 11,
                    "type": "software",
                    "rawForm": "uGBkNNknn",
                    "resp": "service",
                    "id": "software-simple-s42"
                }
            ]
        },
        {
            "text": "\u2022 uGBFL(bin): O(N ln N);",
            "section": "CPU Resources",
            "paragraph_rank": 25,
            "section_rank": 5,
            "entity_spans": [
                {
                    "start": 2,
                    "end": 7,
                    "type": "software",
                    "rawForm": "uGBFL",
                    "resp": "service",
                    "id": "software-simple-s43"
                }
            ]
        },
        {
            "text": "\u2022 uGBFL(kNN): O(N ln N + Nk ln k).",
            "section": "CPU Resources",
            "paragraph_rank": 26,
            "section_rank": 5,
            "entity_spans": [
                {
                    "start": 2,
                    "end": 7,
                    "type": "software",
                    "rawForm": "uGBFL",
                    "resp": "service",
                    "id": "software-simple-s44"
                }
            ]
        },
        {
            "text": "In the example analysis studied in this paper, we find that the training time for these new algorithms is within a factor of two the same as AdaBoost. ",
            "section": "CPU Resources",
            "paragraph_rank": 27,
            "section_rank": 5,
            "entity_spans": [
                {
                    "start": 141,
                    "end": 149,
                    "type": "software",
                    "rawForm": "AdaBoost",
                    "resp": "service",
                    "id": "software-simple-s45"
                }
            ]
        },
        {
            "text": "The CPU-resource usage of these new algorithms is not prohibitive.",
            "section": "CPU Resources",
            "paragraph_rank": 27,
            "section_rank": 5
        },
        {
            "text": "Summary",
            "section_rank": 6
        },
        {
            "text": "A number of novel boosting algorithms have been presented that consider uniformity of selection efficiency in a multivariate space in addition to mis-classifcation errors. ",
            "section": "Summary",
            "paragraph_rank": 28,
            "section_rank": 6
        },
        {
            "text": "Of these, the uGBFL algorithm has the best performance on the example analyses studied in this paper. ",
            "section": "Summary",
            "paragraph_rank": 28,
            "section_rank": 6,
            "entity_spans": [
                {
                    "start": 14,
                    "end": 20,
                    "type": "software",
                    "rawForm": "uGBFL",
                    "resp": "service",
                    "id": "software-simple-s46"
                }
            ]
        },
        {
            "text": "This algorithm is expected to be useful in a wide-variety of analyses performed in particle physics.",
            "section": "Summary",
            "paragraph_rank": 28,
            "section_rank": 6
        },
        {
            "text": "Source code",
            "section_rank": 7
        },
        {
            "text": "The code for classifiers proposed in this article as well as for metrics of uniformity is publicly available at repository https://github.com/anaderi/lhcb_trigger_ml. ",
            "section": "Source code",
            "paragraph_rank": 29,
            "section_rank": 7
        },
        {
            "text": "design a uniform classifier with respect to a given feature by not using this feature, or any correlated features, in the classification; in practice, however, this approach also tends to lead to poorly performing classifiers. ",
            "section": "Source code",
            "paragraph_rank": 29,
            "section_rank": 7
        },
        {
            "text": "The approach which we take in this paper is to explicitly let the classifier learn how to balance non-uniformities coming from different features in such a way as to generate a classification which is uniform on average. ",
            "section": "Source code",
            "paragraph_rank": 29,
            "section_rank": 7
        },
        {
            "text": "It is then important to be able to accurately measure the uniformity of classification. ",
            "section": "Source code",
            "paragraph_rank": 29,
            "section_rank": 7
        },
        {
            "text": "Before proceeding, it is useful to define some desirable properties of uniformity metrics 1. ",
            "section": "Source code",
            "paragraph_rank": 29,
            "section_rank": 7
        },
        {
            "text": "The metric shouldn't depend strongly on the number of events used to test uniformity;",
            "section": "Source code",
            "paragraph_rank": 29,
            "section_rank": 7
        },
        {
            "text": "2. The metric shouldn't depend on the normalization of the event weights: if we multiply all the weights by some arbitrary number, it shouldn't change at all;",
            "section": "Source code",
            "paragraph_rank": 30,
            "section_rank": 7
        },
        {
            "text": "3. The metric should depend only on the order of predictions, not the exact values of probabilities. ",
            "section": "Source code",
            "paragraph_rank": 31,
            "section_rank": 7
        },
        {
            "text": "This is because we care about which events pass the cut and which don't, not about the exact values of predictions. ",
            "section": "Source code",
            "paragraph_rank": 31,
            "section_rank": 7
        },
        {
            "text": "For example: correlation of prediction and mass doesn't satisfy this restriction.",
            "section": "Source code",
            "paragraph_rank": 31,
            "section_rank": 7
        },
        {
            "text": "4. The metric should be stable against any of its own free parameters: if it uses bins, changing the number of bins shouldn't affect the result, if it uses k-nearest neighbors, it should be stable against different values of k.",
            "section": "Source code",
            "paragraph_rank": 32,
            "section_rank": 7
        },
        {
            "text": "In what follows we will consider different metrics which satisfy these criteria, and then compare their performance in some test cases.",
            "section": "Source code",
            "paragraph_rank": 33,
            "section_rank": 7
        },
        {
            "text": "Standard Deviation of Efficiency on Bins (SDE)",
            "section_rank": 8
        },
        {
            "text": "If the space of uniform features is split into bins, it is possible to define the global efficiency eff = total weight of signal events that passed the cut total weight of signal events , as well as the efficiency in every bin, eff bin = weight of signal events in bin that passed the cut weight of signal events in this bin .",
            "section": "Standard Deviation of Efficiency on Bins (SDE)",
            "paragraph_rank": 34,
            "section_rank": 8
        },
        {
            "text": "One measure of non-uniformity is the standard deviation of bin efficiencies from the global efficiency:",
            "section": "Standard Deviation of Efficiency on Bins (SDE)",
            "paragraph_rank": 35,
            "section_rank": 8
        },
        {
            "text": "To make the metric more stable against fluctuations in bins which contain very few events, we add weights to the bins (note that \u2211 bin weight bin = 1):",
            "section": "Standard Deviation of Efficiency on Bins (SDE)",
            "paragraph_rank": 36,
            "section_rank": 8
        },
        {
            "text": "total weight of signal events in bin total weight of signal events , giving the weighted standard deviation (SDE) formula",
            "section": "Standard Deviation of Efficiency on Bins (SDE)",
            "paragraph_rank": 37,
            "section_rank": 8
        },
        {
            "text": "This formula is valid for any given cut value. ",
            "section": "Standard Deviation of Efficiency on Bins (SDE)",
            "paragraph_rank": 38,
            "section_rank": 8
        },
        {
            "text": "To measure the overall non-flatness of the selection, we take several global efficiencies and use",
            "section": "Standard Deviation of Efficiency on Bins (SDE)",
            "paragraph_rank": 38,
            "section_rank": 8
        },
        {
            "text": "Another power p = 2 can be used as well, but p = 2 is considered as the default value.",
            "section": "Standard Deviation of Efficiency on Bins (SDE)",
            "paragraph_rank": 39,
            "section_rank": 8
        },
        {
            "text": "Theil Index of Efficiency",
            "section_rank": 9
        },
        {
            "text": "The Theil Index is frequently used to measure economic inequality:",
            "section": "Theil Index of Efficiency",
            "paragraph_rank": 40,
            "section_rank": 9
        },
        {
            "text": "In our case we have to alter formula a bit to take into account that different bins have different impact, thus the formula turns into Theil(eff) = \u2211 bin weight bin eff bin eff ln eff bin eff .",
            "section": "Theil Index of Efficiency",
            "paragraph_rank": 41,
            "section_rank": 9,
            "entity_spans": [
                {
                    "start": 135,
                    "end": 140,
                    "type": "software",
                    "rawForm": "Theil",
                    "resp": "service",
                    "id": "software-simple-s47"
                }
            ]
        },
        {
            "text": "To measure the overall non-flatness, we average values for several global efficiencies:",
            "section": "Theil Index of Efficiency",
            "paragraph_rank": 42,
            "section_rank": 9
        },
        {
            "text": "Theil(eff)",
            "section": "Theil Index of Efficiency",
            "paragraph_rank": 43,
            "section_rank": 9
        },
        {
            "text": "Distribution Similarity Approach Instead of measuring uniformity in terms of binned efficiencies, it is possible to consider the distribution of the binned classifier predictions, F bin , directly. ",
            "section": "Theil Index of Efficiency",
            "paragraph_rank": 44,
            "section_rank": 9
        },
        {
            "text": "Ideal uniformity means that all the distributions F bin are equal and hence equal to the global distribution F(x). ",
            "section": "Theil Index of Efficiency",
            "paragraph_rank": 44,
            "section_rank": 9
        },
        {
            "text": "This is demonstrated on figure 6. ",
            "section": "Theil Index of Efficiency",
            "paragraph_rank": 44,
            "section_rank": 9
        },
        {
            "text": "To 'measure' non-flatness we can use some distribution distance, like Kolmogorov-Smirnov:",
            "section": "Theil Index of Efficiency",
            "paragraph_rank": 44,
            "section_rank": 9
        },
        {
            "text": "but Cram\u00e9r-von Mises similarity is more informative (usually p = 2 is used):",
            "section": "Theil Index of Efficiency",
            "paragraph_rank": 45,
            "section_rank": 9
        },
        {
            "text": "in particular because Kolmogorov-Smirnov measures are too sensitive to local non-uniformities. ",
            "section": "Theil Index of Efficiency",
            "paragraph_rank": 46,
            "section_rank": 9
        },
        {
            "text": "The advantage of this method is that we don't need to select some global efficiencies like in the previous metrics.",
            "section": "Theil Index of Efficiency",
            "paragraph_rank": 46,
            "section_rank": 9
        },
        {
            "text": "Knn-based modifications",
            "section_rank": 10
        },
        {
            "text": "Though operating with bins is usually both simple and very efficient, in many cases it is hard to find the optimal size of bins in the space of uniform features (specifically in the case of more than two dimensions). ",
            "section": "Knn-based modifications",
            "paragraph_rank": 47,
            "section_rank": 10
        },
        {
            "text": "As mentioned earlier, problems can also arise due to bins with very low populations.",
            "section": "Knn-based modifications",
            "paragraph_rank": 47,
            "section_rank": 10
        },
        {
            "text": "In these cases we can switch to k-nearest neighbors: for each signal event we find k nearest signal events (including the event itself) in the space of uniform features. ",
            "section": "Knn-based modifications",
            "paragraph_rank": 48,
            "section_rank": 10
        },
        {
            "text": "Now we can compute the efficiency eff knn(i) , from the empirical distribution F knn(i) of nearest neighbors. ",
            "section": "Knn-based modifications",
            "paragraph_rank": 48,
            "section_rank": 10
        },
        {
            "text": "The weights for knn(i) are proportional to the total weight of events in knn(i):",
            "section": "Knn-based modifications",
            "paragraph_rank": 48,
            "section_rank": 10
        },
        {
            "text": "so again weights are normed to 1:",
            "section": "Knn-based modifications",
            "paragraph_rank": 49,
            "section_rank": 10
        },
        {
            "text": "It is then possible to write the knn versions of SDE ",
            "section": "Knn-based modifications",
            "paragraph_rank": 50,
            "section_rank": 10,
            "entity_spans": [
                {
                    "start": 49,
                    "end": 53,
                    "type": "software",
                    "rawForm": "SDE",
                    "resp": "service",
                    "id": "software-simple-s48"
                }
            ]
        },
        {
            "text": "The knn approach suffers from a drawback: the impact of different events has very little connection with the weights, because some events are selected as nearest neighbours much more frequently than others. ",
            "section": "Knn-based modifications",
            "paragraph_rank": 51,
            "section_rank": 10
        },
        {
            "text": "This effect can be suppressed by dividing the initial weight of the event by the number of times it is selected as a nearest neighbour.  ",
            "section": "Knn-based modifications",
            "paragraph_rank": 51,
            "section_rank": 10
        },
        {
            "text": "From figure 9 we can see that SDE doesn't make any difference between these distributions, while Theil has lower value in the second case which indicates that distribution is flatter. ",
            "section": "Knn-based modifications",
            "paragraph_rank": 51,
            "section_rank": 10,
            "ref_spans": [
                {
                    "start": 5,
                    "end": 13,
                    "type": "figure",
                    "text": "figure 9"
                }
            ],
            "entity_spans": [
                {
                    "start": 30,
                    "end": 34,
                    "type": "software",
                    "rawForm": "SDE",
                    "resp": "service",
                    "id": "software-simple-s49"
                },
                {
                    "start": 97,
                    "end": 103,
                    "type": "software",
                    "rawForm": "Theil",
                    "resp": "service",
                    "id": "software-simple-s50"
                }
            ]
        },
        {
            "text": "This example demonstrates that Theil has larger penalty for distributions with narrow peaks rather than with narrow pits in the distribution of efficiecies.  ",
            "section": "Knn-based modifications",
            "paragraph_rank": 51,
            "section_rank": 10,
            "entity_spans": [
                {
                    "start": 31,
                    "end": 37,
                    "type": "software",
                    "rawForm": "Theil",
                    "resp": "service",
                    "id": "software-simple-s51"
                }
            ]
        },
        {
            "text": "D \u2192 hhh",
            "section_rank": 11
        },
        {
            "text": "Finally, we compare the SDE and Theil uniformity measures for the classifiers applied to the D \u2192 hhh data set, as shown in Fig. 10. ",
            "section": "D \u2192 hhh",
            "paragraph_rank": 52,
            "section_rank": 11,
            "ref_spans": [
                {
                    "start": 123,
                    "end": 130,
                    "type": "figure",
                    "ref_id": "fig_0",
                    "text": "Fig. 10"
                }
            ]
        },
        {
            "text": "Both measures show similar results, so there is no significant difference between them, nor the CvM metric, on this dataset.  ",
            "section": "D \u2192 hhh",
            "paragraph_rank": 52,
            "section_rank": 11,
            "entity_spans": [
                {
                    "start": 96,
                    "end": 100,
                    "type": "software",
                    "rawForm": "CvM",
                    "resp": "service",
                    "id": "software-simple-s52"
                }
            ]
        },
        {
            "text": "Figure 1 :",
            "section_rank": 12
        },
        {
            "text": "Figure 1: Dalitz-plot distributions for (left) signal and (right) background for the D \u00b1 s \u2192 \u03c0 + \u03c0 \u2212 \u03c0 \u00b1 . ",
            "section": "Figure 1 :",
            "paragraph_rank": 53,
            "section_rank": 12
        },
        {
            "text": "The three pions are labeled here as 1, 2 and 3 and ordered according to increases momentum.",
            "section": "Figure 1 :",
            "paragraph_rank": 53,
            "section_rank": 12
        },
        {
            "text": "Figure 2 :Figure 3 :",
            "section_rank": 13
        },
        {
            "text": "Figure 2: (left) ROC curves for classifier algorithms studied in this paper. ",
            "section": "Figure 2 :Figure 3 :",
            "paragraph_rank": 54,
            "section_rank": 13
        },
        {
            "text": "For the uGBFL algorithms \u03b1 = 0.02 is shown. ",
            "section": "Figure 2 :Figure 3 :",
            "paragraph_rank": 54,
            "section_rank": 13,
            "entity_spans": [
                {
                    "start": 8,
                    "end": 13,
                    "type": "software",
                    "rawForm": "uGBFL",
                    "resp": "service",
                    "id": "software-simple-s53"
                }
            ]
        },
        {
            "text": "(right) ",
            "section": "Figure 2 :Figure 3 :",
            "paragraph_rank": 54,
            "section_rank": 13
        },
        {
            "text": "ROC curves for uGBFL(bin) for differnet values of \u03b1.",
            "section": "Figure 2 :Figure 3 :",
            "paragraph_rank": 54,
            "section_rank": 13,
            "entity_spans": [
                {
                    "start": 15,
                    "end": 20,
                    "type": "software",
                    "rawForm": "uGBFL",
                    "resp": "service",
                    "id": "software-simple-s54"
                }
            ]
        },
        {
            "text": "Figure 4 :Figure 5 :",
            "section_rank": 14
        },
        {
            "text": "Figure 4: Efficiency vs distance to a corner of the Dalitz-plot. ",
            "section": "Figure 4 :Figure 5 :",
            "paragraph_rank": 55,
            "section_rank": 14
        },
        {
            "text": "An arbitrary working point of 50% integrated efficiency is displayed. ",
            "section": "Figure 4 :Figure 5 :",
            "paragraph_rank": 55,
            "section_rank": 14
        },
        {
            "text": "For the uGBFL algorithms \u03b1 = 0.02 is shown.",
            "section": "Figure 4 :Figure 5 :",
            "paragraph_rank": 55,
            "section_rank": 14,
            "entity_spans": [
                {
                    "start": 8,
                    "end": 14,
                    "type": "software",
                    "rawForm": "uGBFL",
                    "resp": "service",
                    "id": "software-simple-s55"
                }
            ]
        },
        {
            "text": "Figure 6 :",
            "section_rank": 15
        },
        {
            "text": "Figure 6: Demonstration of the distribution similarity approach. ",
            "section": "Figure 6 :",
            "paragraph_rank": 56,
            "section_rank": 15
        },
        {
            "text": "(left) ",
            "section": "Figure 6 :",
            "paragraph_rank": 56,
            "section_rank": 15
        },
        {
            "text": "Predictions are uniform in mass, the distribution of predictions in the bin (yellow) is close to the global (blue). ",
            "section": "Figure 6 :",
            "paragraph_rank": 56,
            "section_rank": 15
        },
        {
            "text": "(right) ",
            "section": "Figure 6 :",
            "paragraph_rank": 56,
            "section_rank": 15
        },
        {
            "text": "Distribution with peak in the middle, the distribution in the bin is quite different from the global distribution. ",
            "section": "Figure 6 :",
            "paragraph_rank": 56,
            "section_rank": 15
        },
        {
            "text": "In both cases the yellow rectangle shows the events in the bin over mass.",
            "section": "Figure 6 :",
            "paragraph_rank": 56,
            "section_rank": 15
        },
        {
            "text": "Figures 7",
            "section_rank": 16
        },
        {
            "text": "Figures 7 and 8show the distribution of the predictions and classifier efficiency as a function of mass for the peak and pit distributions, respectively.",
            "section": "Figures 7",
            "paragraph_rank": 57,
            "section_rank": 16
        },
        {
            "text": "Figure 7 :",
            "section_rank": 17
        },
        {
            "text": "Figure 7: Peak response distribution and efficiencies as a function of mass.",
            "section": "Figure 7 :",
            "paragraph_rank": 58,
            "section_rank": 17
        },
        {
            "text": "Mass vs efficiency. ",
            "paragraph_rank": 59,
            "section_rank": 18
        },
        {
            "text": "The different coloured lines correspond to different global classifier efficiencies, as explained in the legend on the leftmost subplot.",
            "paragraph_rank": 59,
            "section_rank": 18
        },
        {
            "text": "Figure 8 :Figure 9 :",
            "section_rank": 19
        },
        {
            "text": "Figure 8: Pit response distribution and efficiencies as a function of mass.",
            "section": "Figure 8 :Figure 9 :",
            "paragraph_rank": 60,
            "section_rank": 19
        },
        {
            "text": "Metrics for D \u2192 hhh signal. ",
            "paragraph_rank": 61,
            "section_rank": 20
        },
        {
            "text": "Metrics for D \u2192 hhh background.",
            "paragraph_rank": 61,
            "section_rank": 20
        },
        {
            "text": "Figure 10 :",
            "section_rank": 21
        },
        {
            "text": "Figure 10: Uniformity metrics for the D \u2192 hhh data set. ",
            "section": "Figure 10 :",
            "paragraph_rank": 62,
            "section_rank": 21
        },
        {
            "text": "The (top) signal and (bottom) background uniformities are plotted as a function of the training stage of a given classifier, listed in the legends on the leftmost plots. ",
            "section": "Figure 10 :",
            "paragraph_rank": 62,
            "section_rank": 21
        },
        {
            "text": "Within each figure the leftmost plot is the SDE and the rightmost plot is the Theil uniformity metric.",
            "section": "Figure 10 :",
            "paragraph_rank": 62,
            "section_rank": 21
        },
        {
            "text": "Table 1 :",
            "section_rank": 22
        },
        {
            "text": "Description of uniform boosting algorithms. ",
            "section": "Table 1 :",
            "paragraph_rank": 63,
            "section_rank": 22
        },
        {
            "text": ") gradient boost using flatness loss +\u03b1 AdaLoss as in Eq. ",
            "section": "Table 1 :",
            "paragraph_rank": 63,
            "section_rank": 22,
            "entity_spans": [
                {
                    "start": 40,
                    "end": 48,
                    "type": "software",
                    "rawForm": "AdaLoss",
                    "resp": "service",
                    "id": "software-simple-s56"
                }
            ]
        },
        {
            "text": "2.11 (data binned for FL) uGBFL(kNN) same as uGBFL(bin) except kNN events are used rather than bins whose pseudo-residuals are (b is the bin containing the kth event)",
            "section": "Table 1 :",
            "paragraph_rank": 63,
            "section_rank": 22,
            "entity_spans": [
                {
                    "start": 26,
                    "end": 31,
                    "type": "software",
                    "rawForm": "uGBFL",
                    "resp": "service",
                    "id": "software-simple-s57"
                },
                {
                    "start": 45,
                    "end": 50,
                    "type": "software",
                    "rawForm": "uGBFL",
                    "resp": "service",
                    "id": "software-simple-s58"
                }
            ]
        },
        {
            "text": "A natural choice is a square n \u00d7 n matrix, but this is not required.2 ",
            "section": "Table 1 :",
            "paragraph_rank": 64,
            "section_rank": 22,
            "ref_spans": [
                {
                    "start": 68,
                    "end": 69,
                    "type": "bibr",
                    "ref_id": "b1",
                    "text": "2"
                }
            ]
        },
        {
            "text": "If weighted events are used, then the fractional sum of weights should be used for w b .",
            "section": "Table 1 :",
            "paragraph_rank": 64,
            "section_rank": 22
        },
        {
            "text": "The Dalitz-plot is essentially a triangle with three corners. ",
            "section": "Table 1 :",
            "paragraph_rank": 65,
            "section_rank": 22
        },
        {
            "text": "Our definition of this distance isMIN (m(D s ) \u2212 m(\u03c0)) 2 \u2212 m 2i j , where i j is 12, 13 and 23.",
            "section": "Table 1 :",
            "paragraph_rank": 65,
            "section_rank": 22
        },
        {
            "text": "The algorithms in this paper can easily be made uniform on signal, background or both.",
            "section": "Table 1 :",
            "paragraph_rank": 66,
            "section_rank": 22
        },
        {
            "text": "Acknowledgments",
            "section_rank": 24
        },
        {
            "text": "These results were obtained using events generated with the official LHCb simulation, and we are grateful to the LHCb collaboration for this privilege. ",
            "section": "Acknowledgments",
            "paragraph_rank": 67,
            "section_rank": 24
        },
        {
            "text": "We particularly acknowledge the work of the LHCb Simulation and Core Computing teams in tuning the simulation software and managing the productions of simulated events. ",
            "section": "Acknowledgments",
            "paragraph_rank": 67,
            "section_rank": 24
        },
        {
            "text": "MW is supported by NSF grant PHY-1306550.",
            "section": "Acknowledgments",
            "paragraph_rank": 67,
            "section_rank": 24
        },
        {
            "text": "A. Measures of uniformity",
            "section_rank": 26
        },
        {
            "text": "In this section we discuss different methods for measuring the uniformity of prediction. ",
            "section": "A. Measures of uniformity",
            "paragraph_rank": 68,
            "section_rank": 26
        },
        {
            "text": "One typical way of 'checking' uniformity of prediction used by physicists is fitting the distribution of the events that were classified as signal (or background) over the feature for which you wish to check uniformity. ",
            "section": "A. Measures of uniformity",
            "paragraph_rank": 68,
            "section_rank": 26
        },
        {
            "text": "This approach requires assumptions about the shape of the distribution, which makes quantitative comparisons of different classifiers difficult. ",
            "section": "A. Measures of uniformity",
            "paragraph_rank": 68,
            "section_rank": 26
        },
        {
            "text": "Our aim here is to explore uniformity figures of merit which make comparing classifiers easier, analogously to how the area under the ROC curve can be used to compare absolute classifier performance.",
            "section": "A. Measures of uniformity",
            "paragraph_rank": 68,
            "section_rank": 26
        },
        {
            "text": "The output of event classification is the probability of each event being signal or background, and it is only after we apply a cut on this probability that events are classified. ",
            "section": "A. Measures of uniformity",
            "paragraph_rank": 69,
            "section_rank": 26
        },
        {
            "text": "An ideal uniformity of signal prediction can then be defined for a given \"uniform feature\" of interest. ",
            "section": "A. Measures of uniformity",
            "paragraph_rank": 69,
            "section_rank": 26
        },
        {
            "text": "It means that whichever cut we select, the efficiency for a signal event to pass the cut doesn't depend on the uniform feature. ",
            "section": "A. Measures of uniformity",
            "paragraph_rank": 69,
            "section_rank": 26
        },
        {
            "text": "Uniformity for background can be defined in the same manner, but for simplicity, in what follows we will only discuss the uniformity of efficiency for signal events.",
            "section": "A. Measures of uniformity",
            "paragraph_rank": 69,
            "section_rank": 26
        },
        {
            "text": "A trivial example of a classifier that has ideal uniformity is a classifier which returns a random classification probability, but such a classifier is of course not very useful. ",
            "section": "A. Measures of uniformity",
            "paragraph_rank": 70,
            "section_rank": 26
        },
        {
            "text": "One can try to",
            "section": "A. Measures of uniformity",
            "paragraph_rank": 70,
            "section_rank": 26
        },
        {
            "text": "Advantages and Disadvantages of Different Metrics",
            "section_rank": 27
        },
        {
            "text": "Theil and SDE Let us compare two metrics that have proven most appropriate for our problem, SDE and Theil. ",
            "section": "Advantages and Disadvantages of Different Metrics",
            "paragraph_rank": 71,
            "section_rank": 27,
            "entity_spans": [
                {
                    "start": 0,
                    "end": 5,
                    "type": "software",
                    "rawForm": "Theil",
                    "resp": "service",
                    "id": "software-simple-s59"
                },
                {
                    "start": 10,
                    "end": 14,
                    "type": "software",
                    "rawForm": "SDE",
                    "resp": "service",
                    "id": "software-simple-s60"
                },
                {
                    "start": 92,
                    "end": 96,
                    "type": "software",
                    "rawForm": "SDE",
                    "resp": "service",
                    "id": "software-simple-s61"
                },
                {
                    "start": 100,
                    "end": 105,
                    "type": "software",
                    "rawForm": "Theil",
                    "resp": "service",
                    "id": "software-simple-s62"
                }
            ]
        },
        {
            "text": "We have some masses distributed uniformly in [0, 1], some constant \u03b1 from interval [0, 1]. ",
            "section": "Advantages and Disadvantages of Different Metrics",
            "paragraph_rank": 71,
            "section_rank": 27
        },
        {
            "text": "The predictions are correlated with mass via beta distribution. ",
            "section": "Advantages and Disadvantages of Different Metrics",
            "paragraph_rank": 71,
            "section_rank": 27
        },
        {
            "text": "First distribution with a peak is obtained by generating a prediction for each event according to its mass:",
            "section": "Advantages and Disadvantages of Different Metrics",
            "paragraph_rank": 71,
            "section_rank": 27
        },
        {
            "text": "The second distribution is obtained by flipping:",
            "section": "Advantages and Disadvantages of Different Metrics",
            "paragraph_rank": 72,
            "section_rank": 27
        },
        {
            "text": "SDE should show no changes if we flip the distribution, while the Theil should make difference between pit and peak.",
            "section": "Advantages and Disadvantages of Different Metrics",
            "paragraph_rank": 73,
            "section_rank": 27,
            "entity_spans": [
                {
                    "start": 0,
                    "end": 4,
                    "type": "software",
                    "rawForm": "SDE",
                    "resp": "service",
                    "id": "software-simple-s63"
                }
            ]
        }
    ]
}