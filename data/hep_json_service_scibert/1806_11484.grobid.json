{
    "level": "sentence",
    "abstract": [
        {
            "text": "Machine learning has played an important role in the analysis of highenergy physics data for decades. ",
            "paragraph_rank": 1,
            "section_rank": 1
        },
        {
            "text": "The emergence of deep learning in 2012 allowed for machine learning tools which could adeptly handle higherdimensional and more complex problems than previously feasible. ",
            "paragraph_rank": 1,
            "section_rank": 1
        },
        {
            "text": "This review is aimed at the reader who is familiar with high energy physics but not machine learning. ",
            "paragraph_rank": 1,
            "section_rank": 1
        },
        {
            "text": "The connections between machine learning and high energy physics data analysis are explored, followed by an introduction to the core concepts of neural networks, examples of the key results demonstrating the power of deep learning for analysis of LHC data, and discussion of future prospects and concerns.",
            "paragraph_rank": 1,
            "section_rank": 1
        }
    ],
    "body_text": [
        {
            "text": "INTRODUCTION",
            "section_rank": 2
        },
        {
            "text": "The physics program of the Large Hadron Collider (LHC) has the potential to address many of the most fundamental questions in modern physics: the nature of mass, the dimensionality of space, the unification of the fundamental forces, the particle nature of dark matter, and the fine-tuning of the Standard Model. ",
            "section": "INTRODUCTION",
            "paragraph_rank": 2,
            "section_rank": 2
        },
        {
            "text": "The importance of these questions and the scale of the experimental program needed to probe them demand that we do our utmost to extract the relevant information from the collected data.",
            "section": "INTRODUCTION",
            "paragraph_rank": 2,
            "section_rank": 2
        },
        {
            "text": "The data collected by high-energy physics (HEP) experiments are complex and high dimensional. ",
            "section": "INTRODUCTION",
            "paragraph_rank": 3,
            "section_rank": 2
        },
        {
            "text": "Traditional data analysis techniques in HEP use a sequence of boolean decisions followed by statistical analysis on the selected data. ",
            "section": "INTRODUCTION",
            "paragraph_rank": 3,
            "section_rank": 2
        },
        {
            "text": "Typically, both the individual decisions and the subsequent statistical analysis are based on the distribution of a single observed quantity motivated by physics considerations, which is not easily extended to higher dimensions.",
            "section": "INTRODUCTION",
            "paragraph_rank": 3,
            "section_rank": 2
        },
        {
            "text": "For several decades, particle physicists have sought to improve the power of their analyses by employing algorithms that utilize multiple variables simultaneously. ",
            "section": "INTRODUCTION",
            "paragraph_rank": 4,
            "section_rank": 2
        },
        {
            "text": "Within HEP this approach is often referred to as multivariate analysis (MVA); however, outside of physics these techniques would be considered examples of machine learning. ",
            "section": "INTRODUCTION",
            "paragraph_rank": 4,
            "section_rank": 2
        },
        {
            "text": "Physicists have used a wide variety of machine learning techniques, including artificial neural networks, kernel density estimation, support vector machines, genetic algorithms, random forests, and boosted decision trees. ",
            "section": "INTRODUCTION",
            "paragraph_rank": 4,
            "section_rank": 2
        },
        {
            "text": "For several years, the status quo of machine learning in HEP was to use boosted decision trees implemented in the software package TMVA (1). ",
            "section": "INTRODUCTION",
            "paragraph_rank": 4,
            "section_rank": 2,
            "ref_spans": [
                {
                    "start": 136,
                    "end": 139,
                    "type": "bibr",
                    "ref_id": "b0",
                    "text": "(1)"
                }
            ],
            "entity_spans": [
                {
                    "start": 131,
                    "end": 136,
                    "type": "software",
                    "rawForm": "TMVA",
                    "resp": "service",
                    "id": "software-simple-s1"
                }
            ]
        },
        {
            "text": "These tools provided an important boost for many data analysis tasks, but their capabilities were understood to be limited; they often failed to match the performance of physicist-engineered solutions, especially when the dimensionality of the data grew large.",
            "section": "INTRODUCTION",
            "paragraph_rank": 4,
            "section_rank": 2
        },
        {
            "text": "The emergence of deep learning began around 2012, when a convergence of techniques enabled training of very large neural networks that greatly outperformed the previous state of the art (2)(3)(4)(5). ",
            "section": "INTRODUCTION",
            "paragraph_rank": 5,
            "section_rank": 2,
            "ref_spans": [
                {
                    "start": 186,
                    "end": 189,
                    "type": "bibr",
                    "ref_id": "b1",
                    "text": "(2)"
                },
                {
                    "start": 189,
                    "end": 192,
                    "type": "bibr",
                    "ref_id": "b2",
                    "text": "(3)"
                },
                {
                    "start": 192,
                    "end": 195,
                    "type": "bibr",
                    "ref_id": "b3",
                    "text": "(4)"
                },
                {
                    "start": 195,
                    "end": 198,
                    "type": "bibr",
                    "ref_id": "b4",
                    "text": "(5)"
                }
            ]
        },
        {
            "text": "These new tools could adeptly handle higher-dimensional and more complex problems than previously feasible. ",
            "section": "INTRODUCTION",
            "paragraph_rank": 5,
            "section_rank": 2
        },
        {
            "text": "In the intervening years there has been an explosion in deep learning research moving beyond the application to image classification into natural languages, self driving cars, and many areas of science.",
            "section": "INTRODUCTION",
            "paragraph_rank": 5,
            "section_rank": 2
        },
        {
            "text": "This review is aimed at the reader who is familiar with data analysis for high energy physics but less familiarity with machine learning. ",
            "section": "INTRODUCTION",
            "paragraph_rank": 6,
            "section_rank": 2
        },
        {
            "text": "The remainder of this section explains the importance of machine learning to high-energy physics data analysis and describes the basics of neural networks in order to clearly define the concept of deep learning. ",
            "section": "INTRODUCTION",
            "paragraph_rank": 6,
            "section_rank": 2
        },
        {
            "text": "Section 2 reviews many of the key applications of deep learning to open challenging problems in LHC data analysis, including several breakthroughs in tasks which were previously thought to be intractable. ",
            "section": "INTRODUCTION",
            "paragraph_rank": 6,
            "section_rank": 2
        },
        {
            "text": "Section 3 discusses the direction of current work and potential concerns regarding the application of deep learning. ",
            "section": "INTRODUCTION",
            "paragraph_rank": 6,
            "section_rank": 2
        },
        {
            "text": "The final section discusses several possible future directions and prospects.",
            "section": "INTRODUCTION",
            "paragraph_rank": 6,
            "section_rank": 2
        },
        {
            "text": "Why Is Machine Learning Relevant for Physics?",
            "section_rank": 3,
            "entity_spans": [
                {
                    "start": 15,
                    "end": 24,
                    "type": "software",
                    "rawForm": "Learning",
                    "resp": "service",
                    "id": "software-simple-s2"
                }
            ]
        },
        {
            "text": "The data collected by the LHC experiments are vast in both the number of collisions and in the complexity of each collision. ",
            "section": "Why Is Machine Learning Relevant for Physics?",
            "paragraph_rank": 7,
            "section_rank": 3
        },
        {
            "text": "The colliding beams at the LHC are grouped into bunches of protons, which cross with a frequency of \u223c40 MHz. ",
            "section": "Why Is Machine Learning Relevant for Physics?",
            "paragraph_rank": 7,
            "section_rank": 3
        },
        {
            "text": "Each collion has the potential to produce a large number of particles, and the LHC detectors have O(10 8 ) sensors used to record these particles. ",
            "section": "Why Is Machine Learning Relevant for Physics?",
            "paragraph_rank": 7,
            "section_rank": 3
        },
        {
            "text": "These high data rates are necessary because collisions which produce interesting products are very rare.",
            "section": "Why Is Machine Learning Relevant for Physics?",
            "paragraph_rank": 7,
            "section_rank": 3
        },
        {
            "text": "As a result of the quantum-mechanical nature of the collisions and the interaction of their products with LHC detectors, the observations resulting from a particular interaction are fundamentally probabilistic. ",
            "section": "Why Is Machine Learning Relevant for Physics?",
            "paragraph_rank": 8,
            "section_rank": 3
        },
        {
            "text": "Therefore, the approach to data analysis and the conclusions drawn from the data must be framed in statistical terms, which includes not only low-level tasks such as particle identification and reconstruction of the particles' energy and momentum, but also high-level tasks such as searches for new particles and measurements. ",
            "section": "Why Is Machine Learning Relevant for Physics?",
            "paragraph_rank": 8,
            "section_rank": 3
        },
        {
            "text": "In classical statistics, tasks such as classification, hypothesis testing, regression, and goodness-of-fit testing are based a statistical model p(x|\u03b8) describing the probability of observing x given the parameters of a theory \u03b8.",
            "section": "Why Is Machine Learning Relevant for Physics?",
            "paragraph_rank": 8,
            "section_rank": 3
        },
        {
            "text": "The high dimensionality and large volume of LHC data pose a problem because the statistical model p(x|\u03b8) over the high-dimensional space of their experimental data is not known explicitly in terms of an equation that can be evaulated. ",
            "section": "Why Is Machine Learning Relevant for Physics?",
            "paragraph_rank": 9,
            "section_rank": 3
        },
        {
            "text": "Instead, one typically has access to large samples of simulated data that was generated by stochastic simulation programs that model the physics of particle interactions on various scales. ",
            "section": "Why Is Machine Learning Relevant for Physics?",
            "paragraph_rank": 9,
            "section_rank": 3
        },
        {
            "text": "If the data were fairly low dimensional (d < 5), the problem of estimating the unknown statistical from the simulated samples would not be difficult. ",
            "section": "Why Is Machine Learning Relevant for Physics?",
            "paragraph_rank": 9,
            "section_rank": 3
        },
        {
            "text": "Histograms or kernel-based density estimates provide reasonable estimates in low-dimensional spaces. ",
            "section": "Why Is Machine Learning Relevant for Physics?",
            "paragraph_rank": 9,
            "section_rank": 3
        },
        {
            "text": "These fairly na\u00efve strategies, however, suffer from the curse of dimensionality. ",
            "section": "Why Is Machine Learning Relevant for Physics?",
            "paragraph_rank": 9,
            "section_rank": 3
        },
        {
            "text": "In a single dimension, N samples may be required to describe the source probability density function. ",
            "section": "Why Is Machine Learning Relevant for Physics?",
            "paragraph_rank": 9,
            "section_rank": 3
        },
        {
            "text": "In d dimensions, the number of samples required grows to the power of the data's dimensionality: O(N d ). ",
            "section": "Why Is Machine Learning Relevant for Physics?",
            "paragraph_rank": 9,
            "section_rank": 3
        },
        {
            "text": "The consequence is that any dimensionality greater than five or ten requires impractical or impossible computational resources, regardless of the speed of the sample generator.",
            "section": "Why Is Machine Learning Relevant for Physics?",
            "paragraph_rank": 9,
            "section_rank": 3
        },
        {
            "text": "Traditionally, HEP physicists have approached this problem by reducing the dimensionality of the data through a series of steps that operate both on individual collision events and on collections of events. ",
            "section": "Why Is Machine Learning Relevant for Physics?",
            "paragraph_rank": 10,
            "section_rank": 3
        },
        {
            "text": "For an individual event, reconstruction algorithms process the raw sensor data into low-level objects such as calorimeter clusters and tracks. ",
            "section": "Why Is Machine Learning Relevant for Physics?",
            "paragraph_rank": 10,
            "section_rank": 3
        },
        {
            "text": "From these low-level components, the algorithms attempt to estimate the energy, momentum, and identity of individual particles. ",
            "section": "Why Is Machine Learning Relevant for Physics?",
            "paragraph_rank": 10,
            "section_rank": 3
        },
        {
            "text": "From these reconstructed objects, event-level summaries are constructed. ",
            "section": "Why Is Machine Learning Relevant for Physics?",
            "paragraph_rank": 10,
            "section_rank": 3
        },
        {
            "text": "Event selection algorithms then select subsets of the collision data for further analysis on the basis of the information associated to individual events. ",
            "section": "Why Is Machine Learning Relevant for Physics?",
            "paragraph_rank": 10,
            "section_rank": 3
        },
        {
            "text": "Traditionally, the reconstruction and event selection operations are based on specific, engineered features in the data. ",
            "section": "Why Is Machine Learning Relevant for Physics?",
            "paragraph_rank": 10,
            "section_rank": 3
        },
        {
            "text": "For instance, the identification of an electron and a photon is based on specific features that summarize the shape of the shower in the electromagnetic calorimeter and discriminate from the energy deposits left by charged and neutral hadrons. ",
            "section": "Why Is Machine Learning Relevant for Physics?",
            "paragraph_rank": 10,
            "section_rank": 3
        },
        {
            "text": "The cumulative product of these steps reduces the dimensionality of the problem to a number small enough to allow the missing statistical model p(x|\u03b8) to be estimated using samples generated by simulation tools.",
            "section": "Why Is Machine Learning Relevant for Physics?",
            "paragraph_rank": 10,
            "section_rank": 3
        },
        {
            "text": "While the traditional approaches to reconstruction and event selection have worked fairly well, there is no guarantee that they are optimal. ",
            "section": "Why Is Machine Learning Relevant for Physics?",
            "paragraph_rank": 11,
            "section_rank": 3
        },
        {
            "text": "Given the complex nature of the data and the subtle signatures of potential new physics, it is reasonable to suspect that there may be a significant performance gap between traditional approaches and the optimal one. ",
            "section": "Why Is Machine Learning Relevant for Physics?",
            "paragraph_rank": 11,
            "section_rank": 3
        },
        {
            "text": "A central role of machine learning in LHC physics is to improve this data reduction, reducing the relevant information contained in the low-level, high-dimensional data into a higher-level and smaller-dimensional space.",
            "section": "Why Is Machine Learning Relevant for Physics?",
            "paragraph_rank": 11,
            "section_rank": 3
        },
        {
            "text": "The Role of Simulators",
            "section_rank": 4
        },
        {
            "text": "Physicists often refer to the set of simulation tools such as pythia (6), herwig (7), Mad-Graph (8), Sherpa (9), and Geant (10) as Monte Carlo tools, since these simulations are probabilistic and rely heavily on Monte Carlo sampling techniques. ",
            "section": "The Role of Simulators",
            "paragraph_rank": 12,
            "section_rank": 4,
            "entity_spans": [
                {
                    "start": 62,
                    "end": 69,
                    "type": "software",
                    "rawForm": "pythia",
                    "resp": "service",
                    "id": "software-simple-s3"
                },
                {
                    "start": 74,
                    "end": 81,
                    "type": "software",
                    "rawForm": "herwig",
                    "resp": "service",
                    "id": "software-simple-s4"
                },
                {
                    "start": 86,
                    "end": 96,
                    "type": "software",
                    "rawForm": "Mad-Graph",
                    "resp": "service",
                    "id": "software-simple-s5"
                },
                {
                    "start": 101,
                    "end": 108,
                    "type": "software",
                    "rawForm": "Sherpa",
                    "resp": "service",
                    "id": "software-simple-s6"
                },
                {
                    "start": 117,
                    "end": 123,
                    "type": "software",
                    "rawForm": "Geant",
                    "resp": "service",
                    "id": "software-simple-s7"
                }
            ]
        },
        {
            "text": "The simulators capture the relevant physics on a hierarchy of scales starting with the microscopic interactions within a proton-proton collision and ending with the interaction of particles in the enormous LHC detectors. ",
            "section": "The Role of Simulators",
            "paragraph_rank": 12,
            "section_rank": 4
        },
        {
            "text": "1 We can think of a simulated data set {xi} N i=1 as being N independent and identically distributed samples from some underlying distribution p(x|\u03b8), where \u03b8 corresponds to the settings of the simulator. ",
            "section": "The Role of Simulators",
            "paragraph_rank": 12,
            "section_rank": 4,
            "ref_spans": [
                {
                    "start": 0,
                    "end": 1,
                    "type": "bibr",
                    "ref_id": "b0",
                    "text": "1"
                }
            ]
        },
        {
            "text": "Moreover, we know the settings of the simulator used, which means that the generated data automatically come with ground-truth labels. ",
            "section": "The Role of Simulators",
            "paragraph_rank": 12,
            "section_rank": 4
        },
        {
            "text": "For instance, we can generate samples of interactions involving a Higgs boson for any desired mass value.",
            "section": "The Role of Simulators",
            "paragraph_rank": 12,
            "section_rank": 4
        },
        {
            "text": "In this framing, the goal of simulation is to approximate the probability p(x|\u03b8) by sampling from an enormous space of unobserved, or latent, processes: p(x|\u03b8) = p(x, z|\u03b8)dz. ",
            "section": "The Role of Simulators",
            "paragraph_rank": 13,
            "section_rank": 4
        },
        {
            "text": "A fixed value of z specifies everything about the simulated events, from the momentum of the initial particles created in the hard scattering to the detailed interactions in the detector. ",
            "section": "The Role of Simulators",
            "paragraph_rank": 13,
            "section_rank": 4
        },
        {
            "text": "Physicists often refer to z as Monte Carlo truth. ",
            "section": "The Role of Simulators",
            "paragraph_rank": 13,
            "section_rank": 4
        },
        {
            "text": "Most reconstruction algorithms can be regarded as estimates of some components of z (particle type, momentum, energy, etc.) given the observed data x. ",
            "section": "The Role of Simulators",
            "paragraph_rank": 13,
            "section_rank": 4
        },
        {
            "text": "Here simulation fulfills a second experimental need: in addition to an estimate of p(x|\u03b8), the simulation provides a dataset {xi, zi} N i=1 which allows physicists to study reconstruction algorithms directly.",
            "section": "The Role of Simulators",
            "paragraph_rank": 13,
            "section_rank": 4
        },
        {
            "text": "Core Concepts in Machine Learning",
            "section_rank": 5
        },
        {
            "text": "Fortunately, many of the tasks encountered in high energy physics can be naturally reformulated as machine learning problems. ",
            "section": "Core Concepts in Machine Learning",
            "paragraph_rank": 14,
            "section_rank": 5
        },
        {
            "text": "Typically, the problems are formulated in terms of a search for some function f : X \u2192 Y , from the space of the observed data X to a lowdimensional space of a desired target label Y , which optimizes some metric of our choosing. ",
            "section": "Core Concepts in Machine Learning",
            "paragraph_rank": 14,
            "section_rank": 5
        },
        {
            "text": "This metric is called a loss function and written as L(y, f (x)).",
            "section": "Core Concepts in Machine Learning",
            "paragraph_rank": 14,
            "section_rank": 5
        },
        {
            "text": "Ideally, a learning algorithm would find the function that optimizes L over all possible values of (x, ",
            "section": "Core Concepts in Machine Learning",
            "paragraph_rank": 15,
            "section_rank": 5
        },
        {
            "text": "y), but this is intractable owing to the curse of dimensionality and an infinite number of functions to choose from. ",
            "section": "Core Concepts in Machine Learning",
            "paragraph_rank": 15,
            "section_rank": 5
        },
        {
            "text": "Instead, in supervised learning, one has labeled training data {xi, yi} N i=1 sampled from p(x, ",
            "section": "Core Concepts in Machine Learning",
            "paragraph_rank": 15,
            "section_rank": 5
        },
        {
            "text": "y). ",
            "section": "Core Concepts in Machine Learning",
            "paragraph_rank": 15,
            "section_rank": 5
        },
        {
            "text": "2 Furthermore, the function space is restricted to a model -a highly flexible family of functions f \u03c6 ",
            "section": "Core Concepts in Machine Learning",
            "paragraph_rank": 15,
            "section_rank": 5,
            "ref_spans": [
                {
                    "start": 0,
                    "end": 1,
                    "type": "bibr",
                    "ref_id": "b1",
                    "text": "2"
                }
            ]
        },
        {
            "text": "(x) parametrized by \u03c6. ",
            "section": "Core Concepts in Machine Learning",
            "paragraph_rank": 15,
            "section_rank": 5
        },
        {
            "text": "In this case, the algorithms minimize directly with respect to the model parameters \u03c6. ",
            "section": "Core Concepts in Machine Learning",
            "paragraph_rank": 15,
            "section_rank": 5
        },
        {
            "text": "Neural networks, support vector machines, and decision trees are examples of types of models commonly used in machine learning. ",
            "section": "Core Concepts in Machine Learning",
            "paragraph_rank": 15,
            "section_rank": 5
        },
        {
            "text": "These models often have a large number of parameters, and in the case of neural networks, finding the optimal f \u03c6 can be a difficult problem.",
            "section": "Core Concepts in Machine Learning",
            "paragraph_rank": 15,
            "section_rank": 5
        },
        {
            "text": "An essential goal in machine learning is generalization-the ability of the model to perform well on data which was not used in training. ",
            "section": "Core Concepts in Machine Learning",
            "paragraph_rank": 16,
            "section_rank": 5
        },
        {
            "text": "Failure in this task is called overtraining. ",
            "section": "Core Concepts in Machine Learning",
            "paragraph_rank": 16,
            "section_rank": 5
        },
        {
            "text": "There are a vast array of techniques to avoid overtraining (or overfitting) that can all be considered forms of regularization. ",
            "section": "Core Concepts in Machine Learning",
            "paragraph_rank": 16,
            "section_rank": 5
        },
        {
            "text": "Regularization techniques like dropout were key to advances in image recognition with deep learning (2,3). ",
            "section": "Core Concepts in Machine Learning",
            "paragraph_rank": 16,
            "section_rank": 5,
            "ref_spans": [
                {
                    "start": 100,
                    "end": 103,
                    "type": "bibr",
                    "ref_id": "b1",
                    "text": "(2,"
                },
                {
                    "start": 103,
                    "end": 105,
                    "type": "bibr",
                    "ref_id": "b2",
                    "text": "3)"
                }
            ]
        },
        {
            "text": "Theoretical analysis of the generalization of deep learning is difficult because it involves a complicated interaction between the specifics of the model f \u03c6 , the optimization algorithms, the loss function, regularization techniques, and the specifics of the finite training samples and the true underlying distribution p(x, y). ",
            "section": "Core Concepts in Machine Learning",
            "paragraph_rank": 16,
            "section_rank": 5
        },
        {
            "text": "Empirically, deep learning models generalize much better than existing theoretical analysis might suggest. ",
            "section": "Core Concepts in Machine Learning",
            "paragraph_rank": 16,
            "section_rank": 5
        },
        {
            "text": "While a more powerful theoretical analysis of generalization for deep learning would be valuable, in practical terms it is not necessary as long as statistically independent data, not used in training, is available to validate the performance.",
            "section": "Core Concepts in Machine Learning",
            "paragraph_rank": 16,
            "section_rank": 5
        },
        {
            "text": "Neural Network Basics",
            "section_rank": 6
        },
        {
            "text": "In the language of neural networks, the space of functions searched is defined by the structure of the networks, which defines a series of transformations. ",
            "section": "Neural Network Basics",
            "paragraph_rank": 17,
            "section_rank": 6
        },
        {
            "text": "These transformations map the input x onto internal or \"hidden\" states hi, until the final transformation maps these hidden states onto the function output y.",
            "section": "Neural Network Basics",
            "paragraph_rank": 17,
            "section_rank": 6
        },
        {
            "text": "Mathematically, these transformation are expressed as",
            "section": "Neural Network Basics",
            "paragraph_rank": 18,
            "section_rank": 6
        },
        {
            "text": "where gi is a some function, called the activation function, and a particular hi is the i-th transformation of the information in x, called the embedding. ",
            "section": "Neural Network Basics",
            "paragraph_rank": 19,
            "section_rank": 6
        },
        {
            "text": "In a simple case, the first embedding is simply the input vector h0 \u2261 x, and the final embedding is the output of the network. ",
            "section": "Neural Network Basics",
            "paragraph_rank": 19,
            "section_rank": 6
        },
        {
            "text": "The elements of the matrix W are referred to as weights and those of vector b as biases. ",
            "section": "Neural Network Basics",
            "paragraph_rank": 19,
            "section_rank": 6
        },
        {
            "text": "The general structure of these transformations, such as the dimensionality of each W and the choice of activation function is referred to as the network architecture, which, taken together with the training parameters constitute the hyperparameters of the network.",
            "section": "Neural Network Basics",
            "paragraph_rank": 19,
            "section_rank": 6
        },
        {
            "text": "The weights and biases of the network are initalized randomly. ",
            "section": "Neural Network Basics",
            "paragraph_rank": 20,
            "section_rank": 6
        },
        {
            "text": "Finding the function which optimizes the loss function is done through an iterative process called training. ",
            "section": "Neural Network Basics",
            "paragraph_rank": 20,
            "section_rank": 6
        },
        {
            "text": "Conceptually, this uses the labeled training examples (x, ",
            "section": "Neural Network Basics",
            "paragraph_rank": 20,
            "section_rank": 6
        },
        {
            "text": "y) and calculates the gradient of the loss function with respect to the model parameters, \u2207 \u03c6 L(f \u03c6 ",
            "section": "Neural Network Basics",
            "paragraph_rank": 20,
            "section_rank": 6
        },
        {
            "text": "(x), ",
            "section": "Neural Network Basics",
            "paragraph_rank": 20,
            "section_rank": 6
        },
        {
            "text": "y). ",
            "section": "Neural Network Basics",
            "paragraph_rank": 20,
            "section_rank": 6
        },
        {
            "text": "In practice, the calculations are done through a technique called backpropagation, which is an efficient means of computing this gradient. ",
            "section": "Neural Network Basics",
            "paragraph_rank": 20,
            "section_rank": 6
        },
        {
            "text": "In principal, backpropagation puts only one restriction on L and f ",
            "section": "Neural Network Basics",
            "paragraph_rank": 20,
            "section_rank": 6
        },
        {
            "text": "(x): they must be be differentiable for a gradient to be defined.",
            "section": "Neural Network Basics",
            "paragraph_rank": 20,
            "section_rank": 6
        },
        {
            "text": "Deep Learning",
            "section_rank": 7,
            "entity_spans": [
                {
                    "start": 0,
                    "end": 13,
                    "type": "software",
                    "rawForm": "Deep Learning",
                    "resp": "service",
                    "id": "software-simple-s8"
                }
            ]
        },
        {
            "text": "Initially, the term deep neural networks referred to neural networks with many hidden layers, and it was used to differentiate such networks from shallow neural networks, which had only one hidden layer. ",
            "section": "Deep Learning",
            "paragraph_rank": 21,
            "section_rank": 7
        },
        {
            "text": "For many years, it was argued that using a shallow network was not a restriction, because of the theoretical analysis that demonstrated that any function can be approximated by a shallow network (14). ",
            "section": "Deep Learning",
            "paragraph_rank": 21,
            "section_rank": 7,
            "ref_spans": [
                {
                    "start": 195,
                    "end": 199,
                    "type": "bibr",
                    "ref_id": "b12",
                    "text": "(14)"
                }
            ]
        },
        {
            "text": "However, an effective shallow network may require an enormous number of nodes in the hidden layer, and in practice, shallow neural networks often failed to discover useful functions from high-dimensional data sets.",
            "section": "Deep Learning",
            "paragraph_rank": 21,
            "section_rank": 7
        },
        {
            "text": "The traditional strategy for discovering the optimal function for a given application involves a gradient search through f \u03c6 . ",
            "section": "Deep Learning",
            "paragraph_rank": 22,
            "section_rank": 7
        },
        {
            "text": "In practice, this becomes much more difficult to accomplish as the neural network becomes deeper. ",
            "section": "Deep Learning",
            "paragraph_rank": 22,
            "section_rank": 7
        },
        {
            "text": "As the difference between the function value f \u03c6 ",
            "section": "Deep Learning",
            "paragraph_rank": 22,
            "section_rank": 7
        },
        {
            "text": "(x) and the desired output y is propagated back through the various embeddings, the gradient \u2207 \u03c6 L(f \u03c6 ",
            "section": "Deep Learning",
            "paragraph_rank": 22,
            "section_rank": 7
        },
        {
            "text": "(x), ",
            "section": "Deep Learning",
            "paragraph_rank": 22,
            "section_rank": 7
        },
        {
            "text": "y) rapidly approaches zero, making it difficult to improve the performance by adjusting the model parameters. ",
            "section": "Deep Learning",
            "paragraph_rank": 22,
            "section_rank": 7
        },
        {
            "text": "This vanishing gradient problem (15,16) has been overcome in recent years using a variety of strategies, including computational boosts from graphical processing units, larger training samples, new regularization techniques such as dropout (17), pre-training of initial embeddings with unsupervised learning methods such as autoencoders (18,19). ",
            "section": "Deep Learning",
            "paragraph_rank": 22,
            "section_rank": 7,
            "ref_spans": [
                {
                    "start": 32,
                    "end": 36,
                    "type": "bibr",
                    "ref_id": "b13",
                    "text": "(15,"
                },
                {
                    "start": 36,
                    "end": 39,
                    "type": "bibr",
                    "ref_id": "b14",
                    "text": "16)"
                },
                {
                    "start": 240,
                    "end": 244,
                    "type": "bibr",
                    "ref_id": "b15",
                    "text": "(17)"
                },
                {
                    "start": 337,
                    "end": 341,
                    "type": "bibr",
                    "ref_id": "b16",
                    "text": "(18,"
                },
                {
                    "start": 341,
                    "end": 344,
                    "type": "bibr",
                    "text": "19)"
                }
            ]
        },
        {
            "text": "Autoencoders attempt to learn a useful layered representation of the data without having to backpropagate through a deep network; standard gradient descent is only used at the end to fine-tune the network.",
            "section": "Deep Learning",
            "paragraph_rank": 22,
            "section_rank": 7
        },
        {
            "text": "More generally, deep learning can refer to a broad class of machine learning methods emphasizing hierarchical representations of the data and modular, differentiable components. ",
            "section": "Deep Learning",
            "paragraph_rank": 23,
            "section_rank": 7
        },
        {
            "text": "Not only do these deep networks have more expressive capacity, but also the layers can be interpreted as building up a hierarchical representation of the data. ",
            "section": "Deep Learning",
            "paragraph_rank": 23,
            "section_rank": 7
        },
        {
            "text": "In natural images, for example, the first layers learn low-level features like edges and corners, the middle layers learn midlevel features like eyes, and the final layers learn high-level features like faces. ",
            "section": "Deep Learning",
            "paragraph_rank": 23,
            "section_rank": 7
        },
        {
            "text": "The processes that produce particle physics data naturally lead to compositionality and hierarchical structured data. ",
            "section": "Deep Learning",
            "paragraph_rank": 23,
            "section_rank": 7
        },
        {
            "text": "For instance, a typical event at the LHC is composed of jets, jets are composed of hadrons, hadrons lead to tracks and calorimeter clusters, tracks are composed of hits, and calorimeter clusters are composed of calorimeter cells. ",
            "section": "Deep Learning",
            "paragraph_rank": 23,
            "section_rank": 7
        },
        {
            "text": "The analogy also extends to higher levels with groups of particles forming resonances in a cascade decay. ",
            "section": "Deep Learning",
            "paragraph_rank": 23,
            "section_rank": 7
        },
        {
            "text": "For these reasons, one might anticipate deep learning to be particularly effective at the LHC.",
            "section": "Deep Learning",
            "paragraph_rank": 23,
            "section_rank": 7
        },
        {
            "text": "Modern deep learning is characterized by the composition of modular, differentiable components (20). ",
            "section": "Deep Learning",
            "paragraph_rank": 24,
            "section_rank": 7,
            "ref_spans": [
                {
                    "start": 95,
                    "end": 99,
                    "type": "bibr",
                    "ref_id": "b17",
                    "text": "(20)"
                }
            ]
        },
        {
            "text": "Among the first of these modular components was the convolutional filter, which is arguably the most important innovation in deep learning applied to image processing (21). ",
            "section": "Deep Learning",
            "paragraph_rank": 24,
            "section_rank": 7,
            "ref_spans": [
                {
                    "start": 167,
                    "end": 171,
                    "type": "bibr",
                    "ref_id": "b18",
                    "text": "(21)"
                }
            ]
        },
        {
            "text": "Convolutional architectures are natural when the input data has some notion of locality, the individual components of x are the same type (e.g., neighboring pixels in an image), and the interesting features are equally likely to appear in any local patch. ",
            "section": "Deep Learning",
            "paragraph_rank": 24,
            "section_rank": 7
        },
        {
            "text": "The kernel k of the convolution can be interpreted as a bank of filters that operates on a local patch of the input xi as",
            "section": "Deep Learning",
            "paragraph_rank": 24,
            "section_rank": 7
        },
        {
            "text": "where i indexes the local patches and j indexes the filters. ",
            "section": "Deep Learning",
            "paragraph_rank": 25,
            "section_rank": 7
        },
        {
            "text": "Because the same kernel is applied as it is swept over the input, it has the effect of sharing weights in a dense network.",
            "section": "Deep Learning",
            "paragraph_rank": 25,
            "section_rank": 7
        },
        {
            "text": "Weight sharing imposes translational symmetry on the network, and it drastically reduces the number of parameters in the network and the amount of data needed to train them. ",
            "section": "Deep Learning",
            "paragraph_rank": 26,
            "section_rank": 7
        },
        {
            "text": "Convolutional layers are usually followed by a pooling layer, which summarizes the result of applying the filters in a local patch (e.g., by taking the maximum or average). ",
            "section": "Deep Learning",
            "paragraph_rank": 26,
            "section_rank": 7
        },
        {
            "text": "These convolutional and pooling layers can be composed to build a hierarchical representation of the data going from low-to mid-to high-level features. ",
            "section": "Deep Learning",
            "paragraph_rank": 26,
            "section_rank": 7
        },
        {
            "text": "Other modular components include normalization layers (22) and residual layers (23). ",
            "section": "Deep Learning",
            "paragraph_rank": 26,
            "section_rank": 7,
            "ref_spans": [
                {
                    "start": 54,
                    "end": 58,
                    "type": "bibr",
                    "ref_id": "b19",
                    "text": "(22)"
                },
                {
                    "start": 79,
                    "end": 83,
                    "type": "bibr",
                    "ref_id": "b20",
                    "text": "(23)"
                }
            ]
        },
        {
            "text": "By training the different layers of these networks jointly, deep convolutional neural networks learn hierarchical features that tend to outperform engineered features for image processing tasks.",
            "section": "Deep Learning",
            "paragraph_rank": 26,
            "section_rank": 7
        },
        {
            "text": "Working with variable-length input (e.g., words in a sentence) requires the network architecture to be adaptive in some way. ",
            "section": "Deep Learning",
            "paragraph_rank": 27,
            "section_rank": 7
        },
        {
            "text": "Variable-length input can be cropped or padded with zeros to fit into a fixed-size vector x, but these blunt solutions either discard potentially useful information or force a network to accommodate placeholder values. ",
            "section": "Deep Learning",
            "paragraph_rank": 27,
            "section_rank": 7
        },
        {
            "text": "A far more natural solution is to rely on networks that can adjust to the input size dynamically. ",
            "section": "Deep Learning",
            "paragraph_rank": 27,
            "section_rank": 7
        },
        {
            "text": "A particularly illustrative case is a simple recursive unit, which maps a pair of inputs, h1 and h2, onto an output, h, as follows:",
            "section": "Deep Learning",
            "paragraph_rank": 27,
            "section_rank": 7
        },
        {
            "text": "Assuming that one or both of the input vectors are of the same dimension as h, the output can be fed into the input recursively and condense an arbitrary length sequence of inputs into a fixed-dimensional representation {hi} \u2192 h. ",
            "section": "Deep Learning",
            "paragraph_rank": 28,
            "section_rank": 7
        },
        {
            "text": "More generally, a neural network can be visualized as a directed acyclic graph in which edges represent the various internal h vectors. ",
            "section": "Deep Learning",
            "paragraph_rank": 28,
            "section_rank": 7
        },
        {
            "text": "Figure 1 illustrates several such graphs. ",
            "section": "Deep Learning",
            "paragraph_rank": 28,
            "section_rank": 7,
            "ref_spans": [
                {
                    "start": 0,
                    "end": 8,
                    "type": "figure",
                    "ref_id": "fig_0",
                    "text": "Figure 1"
                }
            ]
        },
        {
            "text": "In practice, since recursive networks can grow very deep, simple recursive units encounter problems with vanishing or exploding gradients. ",
            "section": "Deep Learning",
            "paragraph_rank": 28,
            "section_rank": 7
        },
        {
            "text": "These longer sequences can be handled using a technique known as gating, where activation functions and transformations are applied selectively, or inputs can be ignored entirely. ",
            "section": "Deep Learning",
            "paragraph_rank": 28,
            "section_rank": 7
        },
        {
            "text": "These alleviates the exploding and vanishing gradient problem at the expense of a more complicated recurrent unit; examples are long-short-term-memory (LSTM) units (24), and gated recurrent units (GRU) (25).",
            "section": "Deep Learning",
            "paragraph_rank": 28,
            "section_rank": 7,
            "ref_spans": [
                {
                    "start": 164,
                    "end": 168,
                    "type": "bibr",
                    "ref_id": "b21",
                    "text": "(24)"
                },
                {
                    "start": 202,
                    "end": 206,
                    "type": "bibr",
                    "ref_id": "b22",
                    "text": "(25)"
                }
            ]
        },
        {
            "text": "Both convolutional and recurrent layers are examples of network architectures that use shared weights. ",
            "section": "Deep Learning",
            "paragraph_rank": 29,
            "section_rank": 7
        },
        {
            "text": "In the convolutional case, each element of k acts in multiple dot products, whereas in the recurrent case, the transformation in Equation 3 is applied multiple times for each pattern. ",
            "section": "Deep Learning",
            "paragraph_rank": 29,
            "section_rank": 7
        },
        {
            "text": "Weight sharing can be viewed as a type of regularization: by reusing the same transformation in multiple places throughout the network, the network designer can encode domain-specific structure.",
            "section": "Deep Learning",
            "paragraph_rank": 29,
            "section_rank": 7
        },
        {
            "text": "SURVEY OF APPLICATIONS",
            "section_rank": 8
        },
        {
            "text": "Machine learning has found numerous natural applications in particle physics, where many tasks require classification in high-dimensional variable spaces. ",
            "section": "SURVEY OF APPLICATIONS",
            "paragraph_rank": 30,
            "section_rank": 8
        },
        {
            "text": "At the lowest level, ma- Schematic showing feed-forward, recurrent, and recursive neural network architectures. ",
            "section": "SURVEY OF APPLICATIONS",
            "paragraph_rank": 30,
            "section_rank": 8,
            "entity_spans": [
                {
                    "start": 21,
                    "end": 35,
                    "type": "software",
                    "rawForm": "ma- Schematic",
                    "resp": "service",
                    "id": "software-simple-s9"
                }
            ]
        },
        {
            "text": "Diamonds represent inputs and outputs, while processing units are represented with circles and squares. ",
            "section": "SURVEY OF APPLICATIONS",
            "paragraph_rank": 30,
            "section_rank": 8
        },
        {
            "text": "Arrows between processing units represent embeddings h. ",
            "section": "SURVEY OF APPLICATIONS",
            "paragraph_rank": 30,
            "section_rank": 8
        },
        {
            "text": "Standard feed-forward networks map a fixed length x into y, whereas recurrent and recursive networks can process a sequence of inputs {x i }. ",
            "section": "SURVEY OF APPLICATIONS",
            "paragraph_rank": 30,
            "section_rank": 8
        },
        {
            "text": "Units represented as circles are shared throughout the network: once the network is trained, the units can be used to build a network of arbitrary size. ",
            "section": "SURVEY OF APPLICATIONS",
            "paragraph_rank": 30,
            "section_rank": 8
        },
        {
            "text": "Recurrent networks can be viewed as a subset of recursive networks, in which each node combines one input x i and the output from the previous recurrent node h i\u22121 to produce h i , and where h 0 = 0. ",
            "section": "SURVEY OF APPLICATIONS",
            "paragraph_rank": 30,
            "section_rank": 8
        },
        {
            "text": "Recursive units map each pair of inputs to an output in the same space, (h i , h j ) \u2192 h k . ",
            "section": "SURVEY OF APPLICATIONS",
            "paragraph_rank": 30,
            "section_rank": 8
        },
        {
            "text": "Note that these components can also be chained: Any output node can also serve as an input node to another component. ",
            "section": "SURVEY OF APPLICATIONS",
            "paragraph_rank": 30,
            "section_rank": 8
        },
        {
            "text": "chine learning tools can perform hit reconstruction (26) or track finding (27) in individual detector systems. ",
            "section": "SURVEY OF APPLICATIONS",
            "paragraph_rank": 30,
            "section_rank": 8,
            "ref_spans": [
                {
                    "start": 52,
                    "end": 56,
                    "type": "bibr",
                    "ref_id": "b23",
                    "text": "(26)"
                },
                {
                    "start": 74,
                    "end": 78,
                    "type": "bibr",
                    "ref_id": "b24",
                    "text": "(27)"
                }
            ]
        },
        {
            "text": "These tools can also perform object identification by using information from various detector systems, such as electron (28), photon (29), or \u03c4 lepton (30) identification. ",
            "section": "SURVEY OF APPLICATIONS",
            "paragraph_rank": 30,
            "section_rank": 8,
            "ref_spans": [
                {
                    "start": 120,
                    "end": 124,
                    "type": "bibr",
                    "ref_id": "b25",
                    "text": "(28)"
                },
                {
                    "start": 133,
                    "end": 137,
                    "type": "bibr",
                    "ref_id": "b26",
                    "text": "(29)"
                }
            ]
        },
        {
            "text": "Finally, machine learning tools have been widely used to classify entire events as background-like or signal-like, both in the final statistical analysis (31) or at the initial trigger decision (32). ",
            "section": "SURVEY OF APPLICATIONS",
            "paragraph_rank": 30,
            "section_rank": 8,
            "ref_spans": [
                {
                    "start": 154,
                    "end": 158,
                    "type": "bibr",
                    "ref_id": "b29",
                    "text": "(31)"
                },
                {
                    "start": 194,
                    "end": 198,
                    "type": "bibr",
                    "ref_id": "b30",
                    "text": "(32)"
                }
            ]
        },
        {
            "text": "These machine learning tools have found high-profile application in single t quark searches (33), early Higgs boson searches (34), and the Higgs boson discovery (29).",
            "section": "SURVEY OF APPLICATIONS",
            "paragraph_rank": 30,
            "section_rank": 8,
            "ref_spans": [
                {
                    "start": 125,
                    "end": 129,
                    "type": "bibr",
                    "ref_id": "b32",
                    "text": "(34)"
                }
            ]
        },
        {
            "text": "Event Selection and High-Level Physics Tasks",
            "section_rank": 9
        },
        {
            "text": "The earliest successes of deep learning in high energy physics came in improvements in event selection for signal events with complex topologies. ",
            "section": "Event Selection and High-Level Physics Tasks",
            "paragraph_rank": 31,
            "section_rank": 9
        },
        {
            "text": "In the past few years, several studies have demonstrated that the traditional shallow networks based on physics-inspired engineered (\"high-level\") features are outperformed by deep networks based on the higherdimensional features which receive less pre-processing (\"lower-level\") features. ",
            "section": "Event Selection and High-Level Physics Tasks",
            "paragraph_rank": 31,
            "section_rank": 9
        },
        {
            "text": "Prior to the advent of deep learning, such pre-processing was necessary, as shallow network performance on low-level features fell short. ",
            "section": "Event Selection and High-Level Physics Tasks",
            "paragraph_rank": 31,
            "section_rank": 9
        },
        {
            "text": "The deep learning results discussed below demonstrate that deep networks using the low-level features surpass the shallow networks using high-level features. ",
            "section": "Event Selection and High-Level Physics Tasks",
            "paragraph_rank": 31,
            "section_rank": 9
        },
        {
            "text": "This confirms the suspicion that feature engineering, applying physics knowledge (left) Deep networks (DN) performance in signal-background classification compared to shallow networks (NN) with a variety of low-and high-level features demonstrate that deep networks with only low-level features outperform all other approaches, from (35). ",
            "section": "Event Selection and High-Level Physics Tasks",
            "paragraph_rank": 31,
            "section_rank": 9,
            "ref_spans": [
                {
                    "start": 333,
                    "end": 337,
                    "type": "bibr",
                    "ref_id": "b33",
                    "text": "(35)"
                }
            ]
        },
        {
            "text": "(right) ",
            "section": "Event Selection and High-Level Physics Tasks",
            "paragraph_rank": 31,
            "section_rank": 9
        },
        {
            "text": "Comparison of the distributions of invariant mass of events selected by a deep network (DN21) using only object momentum to a shallow network (NN7) that has been trained using this feature, at equivalent background rejection. ",
            "section": "Event Selection and High-Level Physics Tasks",
            "paragraph_rank": 31,
            "section_rank": 9
        },
        {
            "text": "Also shown are the distributions in pure signal and background samples. ",
            "section": "Event Selection and High-Level Physics Tasks",
            "paragraph_rank": 31,
            "section_rank": 9
        },
        {
            "text": "The shallow network relies heavily on this invariant mass quantity to discriminate. ",
            "section": "Event Selection and High-Level Physics Tasks",
            "paragraph_rank": 31,
            "section_rank": 9
        },
        {
            "text": "The deep network has also discovered the value of this feature, but is able to recover signal further from peak.",
            "section": "Event Selection and High-Level Physics Tasks",
            "paragraph_rank": 31,
            "section_rank": 9
        },
        {
            "text": "to construct high-level features, is often sub-optimal. ",
            "section": "Event Selection and High-Level Physics Tasks",
            "paragraph_rank": 32,
            "section_rank": 9
        },
        {
            "text": "An early study (35) compared the performance of shallow and deep networks in distinguishing a cascading decay of new exotic Higgs bosons from the dominant background. ",
            "section": "Event Selection and High-Level Physics Tasks",
            "paragraph_rank": 32,
            "section_rank": 9,
            "ref_spans": [
                {
                    "start": 15,
                    "end": 19,
                    "type": "bibr",
                    "ref_id": "b33",
                    "text": "(35)"
                }
            ]
        },
        {
            "text": "This study used a structured data set in which a large set of basic low-level features (object four-momenta) were reduced to a smaller set of physics-inspired high-level engineered features. ",
            "section": "Event Selection and High-Level Physics Tasks",
            "paragraph_rank": 32,
            "section_rank": 9
        },
        {
            "text": "Because the high-level features were a strict function of the low-level features, they contained a subset of the information, so that the expertise encoded by the high-level features was solely in the design of these dimensionality-reducing functions rather than the introduction of new information. ",
            "section": "Event Selection and High-Level Physics Tasks",
            "paragraph_rank": 32,
            "section_rank": 9
        },
        {
            "text": "This gave rise to revealing comparisons about the relative information content of the low-and high-level features and the power of classifiers to extract it. ",
            "section": "Event Selection and High-Level Physics Tasks",
            "paragraph_rank": 32,
            "section_rank": 9
        },
        {
            "text": "In their study, Baldi et al. (35) found that deep networks using the lower-level data significantly outperformed shallow networks that relied on physics-inspired features such as reconstructed invariant masses ( Figure 2). ",
            "section": "Event Selection and High-Level Physics Tasks",
            "paragraph_rank": 32,
            "section_rank": 9,
            "ref_spans": [
                {
                    "start": 29,
                    "end": 33,
                    "type": "bibr",
                    "ref_id": "b33",
                    "text": "(35)"
                },
                {
                    "start": 212,
                    "end": 221,
                    "type": "figure",
                    "text": "Figure 2)"
                }
            ]
        },
        {
            "text": "The high-level engineered features captured real insights, but clearly sacrificed some useful information.",
            "section": "Event Selection and High-Level Physics Tasks",
            "paragraph_rank": 32,
            "section_rank": 9
        },
        {
            "text": "Such conclusions are not universal, however, but rather are dependent on the specifics of the classification task. ",
            "section": "Event Selection and High-Level Physics Tasks",
            "paragraph_rank": 33,
            "section_rank": 9
        },
        {
            "text": "Using the same approach, Baldi et al. (35) analyzed a supersymmetric particle search that has received significant feature engineering in the literature, and found that shallow networks using low-level data very nearly matched the performance of both shallow networks using engineered features as well as deep networks on either set of features. ",
            "section": "Event Selection and High-Level Physics Tasks",
            "paragraph_rank": 33,
            "section_rank": 9,
            "ref_spans": [
                {
                    "start": 38,
                    "end": 42,
                    "type": "bibr",
                    "ref_id": "b33",
                    "text": "(35)"
                }
            ]
        },
        {
            "text": "The authors concluded that this application requires only simple linear functions on the lower-level data, and may not require a deep network or deserve such attention to feature engineering. ",
            "section": "Event Selection and High-Level Physics Tasks",
            "paragraph_rank": 33,
            "section_rank": 9
        },
        {
            "text": "Similar conclusions were reached in Reference 36.",
            "section": "Event Selection and High-Level Physics Tasks",
            "paragraph_rank": 33,
            "section_rank": 9
        },
        {
            "text": "In addition to optimizing event selection for a fixed signal versus background problem, Cranmer and colleagues (39) showed that it is possible to approximate the likelihood ratio p(x|\u03b8)/p0(x) for a continuous family of signal models parameterized by \u03b8. ",
            "section": "Event Selection and High-Level Physics Tasks",
            "paragraph_rank": 34,
            "section_rank": 9,
            "ref_spans": [
                {
                    "start": 111,
                    "end": 115,
                    "type": "bibr",
                    "ref_id": "b37",
                    "text": "(39)"
                }
            ]
        },
        {
            "text": "Since binary classification amounts to approximating a likelihood ratio, this generalization is called a parameterized classifier. ",
            "section": "Event Selection and High-Level Physics Tasks",
            "paragraph_rank": 34,
            "section_rank": 9
        },
        {
            "text": "This is a common-use case at the LHC because most signals are predicted by theories with several free parameters. ",
            "section": "Event Selection and High-Level Physics Tasks",
            "paragraph_rank": 34,
            "section_rank": 9
        },
        {
            "text": "For instance, Baldi et al. (38) used this technique to create a classifier for X \u2192 tt versus Standard Model background parameterized by the mass of the resonance mX . ",
            "section": "Event Selection and High-Level Physics Tasks",
            "paragraph_rank": 34,
            "section_rank": 9,
            "ref_spans": [
                {
                    "start": 27,
                    "end": 31,
                    "type": "bibr",
                    "ref_id": "b36",
                    "text": "(38)"
                }
            ]
        },
        {
            "text": "The approximate likelihood can also be used in a likelihood fit to estimate the parameters \u03b8 (e.g., masses, coupling constants), providing a novel form of likelihood-free inference. ",
            "section": "Event Selection and High-Level Physics Tasks",
            "paragraph_rank": 34,
            "section_rank": 9
        },
        {
            "text": "The carl software package provides a convenient interface for this technique (39).",
            "section": "Event Selection and High-Level Physics Tasks",
            "paragraph_rank": 34,
            "section_rank": 9,
            "entity_spans": [
                {
                    "start": 4,
                    "end": 9,
                    "type": "software",
                    "rawForm": "carl",
                    "resp": "service",
                    "id": "software-simple-s10"
                }
            ]
        },
        {
            "text": "Jet Classification",
            "section_rank": 10
        },
        {
            "text": "Machine learning has been applied to a wide range of jet classification problems, to identify jets from heavy (c, b, ",
            "section": "Jet Classification",
            "paragraph_rank": 35,
            "section_rank": 10
        },
        {
            "text": "t) or light (u, d, ",
            "section": "Jet Classification",
            "paragraph_rank": 35,
            "section_rank": 10
        },
        {
            "text": "s) quarks, gluons, and W , Z, and H bosons. ",
            "section": "Jet Classification",
            "paragraph_rank": 35,
            "section_rank": 10
        },
        {
            "text": "Traditionally these classification problems have been grouped into flavor tagging, which discriminates between b, c, and light quarks, jet substructure tagging, which discriminates between jets from W , Z, t and H, and quark-gluon tagging.",
            "section": "Jet Classification",
            "paragraph_rank": 35,
            "section_rank": 10
        },
        {
            "text": "In flavor tagging, the discriminating information is spatial: Heavy quarks decay weakly in a matter of picoseconds, which is sufficient time for a highly boosted quark to travel roughly a centimeter from the interaction point. ",
            "section": "Jet Classification",
            "paragraph_rank": 36,
            "section_rank": 10
        },
        {
            "text": "Due to this measurable separation, flavor tagging relies heavily on tracks reconstructed by high-granularity sensors near the interaction point, and on vertices fit to these tracks. ",
            "section": "Jet Classification",
            "paragraph_rank": 36,
            "section_rank": 10
        },
        {
            "text": "The use of machine learning in flavor tagging dates to LEP (40), where libraries such as JETNET (41) were used to identify b and c quarks, and has continued through LHC runs 1 and 2 (42-44).",
            "section": "Jet Classification",
            "paragraph_rank": 36,
            "section_rank": 10,
            "ref_spans": [
                {
                    "start": 59,
                    "end": 63,
                    "type": "bibr",
                    "ref_id": "b38",
                    "text": "(40)"
                }
            ],
            "entity_spans": [
                {
                    "start": 89,
                    "end": 96,
                    "type": "software",
                    "rawForm": "JETNET",
                    "resp": "service",
                    "id": "software-simple-s11"
                }
            ]
        },
        {
            "text": "In contrast to jet flavor tagging, jet substructure and quark-gluon tagging rely on information created at one spatial location during the decay of the original particle. ",
            "section": "Jet Classification",
            "paragraph_rank": 37,
            "section_rank": 10
        },
        {
            "text": "The spread in decay product momenta translates to spatial separation as the particles travel away from the interaction point, but the underlying physics is localized at the decay point. ",
            "section": "Jet Classification",
            "paragraph_rank": 37,
            "section_rank": 10
        },
        {
            "text": "Thus, while the power of flavor tagging is limited primarily by the tracking detector resolution, jet substructure and quark-gluon discrimination are subject to quantum-mechanical limitations. ",
            "section": "Jet Classification",
            "paragraph_rank": 37,
            "section_rank": 10
        },
        {
            "text": "Theoretical and experimental physicists have expended considerable effort in quantifying these limitations, and in engineering jet-substructure-based discriminating variables (45).",
            "section": "Jet Classification",
            "paragraph_rank": 37,
            "section_rank": 10,
            "ref_spans": [
                {
                    "start": 175,
                    "end": 179,
                    "type": "bibr",
                    "ref_id": "b41",
                    "text": "(45)"
                }
            ]
        },
        {
            "text": "Recently, the realization that lower-level, higher-dimensional data could contain additional power led to a rapid proliferation of studies that challenged established substructure approaches (45; see https://indico.physics.lbl.gov/indico/event/546/overview). ",
            "section": "Jet Classification",
            "paragraph_rank": 38,
            "section_rank": 10
        },
        {
            "text": "In 2014, Cogan et al. (47) recognized that the projective tower structure of calorimeters present in nearly all modern HEP detectors was similar to the pixels of an image (Figure 3). ",
            "section": "Jet Classification",
            "paragraph_rank": 38,
            "section_rank": 10,
            "ref_spans": [
                {
                    "start": 22,
                    "end": 26,
                    "type": "bibr",
                    "ref_id": "b43",
                    "text": "(47)"
                },
                {
                    "start": 171,
                    "end": 180,
                    "type": "figure",
                    "ref_id": "fig_2",
                    "text": "(Figure 3"
                }
            ]
        },
        {
            "text": "This representation of the data allowed physicists to leverage the advances in image classification such as convolutional neural networks. ",
            "section": "Jet Classification",
            "paragraph_rank": 38,
            "section_rank": 10
        },
        {
            "text": "The image-based networks discriminated as well as or better than shallow networks using jet-substructure-based inputs (46,48). ",
            "section": "Jet Classification",
            "paragraph_rank": 38,
            "section_rank": 10,
            "ref_spans": [
                {
                    "start": 118,
                    "end": 122,
                    "type": "bibr",
                    "ref_id": "b42",
                    "text": "(46,"
                },
                {
                    "start": 122,
                    "end": 125,
                    "type": "bibr",
                    "ref_id": "b44",
                    "text": "48)"
                }
            ]
        },
        {
            "text": "The discriminating persisted in the presence of pileup and jet grooming (49), across generators (50), and could be generalized to three-dimensional detectors using multiple stacked channels analogous to colors (51). ",
            "section": "Jet Classification",
            "paragraph_rank": 38,
            "section_rank": 10,
            "ref_spans": [
                {
                    "start": 72,
                    "end": 76,
                    "type": "bibr",
                    "ref_id": "b45",
                    "text": "(49)"
                },
                {
                    "start": 210,
                    "end": 214,
                    "type": "bibr",
                    "ref_id": "b47",
                    "text": "(51)"
                }
            ]
        },
        {
            "text": "Outside collider physics, a similar approach was used to tackle object-identification tasks in neutrino experiments (52,53).",
            "section": "Jet Classification",
            "paragraph_rank": 38,
            "section_rank": 10,
            "ref_spans": [
                {
                    "start": 116,
                    "end": 120,
                    "type": "bibr",
                    "ref_id": "b48",
                    "text": "(52,"
                },
                {
                    "start": 120,
                    "end": 123,
                    "type": "bibr",
                    "ref_id": "b49",
                    "text": "53)"
                }
            ]
        },
        {
            "text": "While the image-based approach has been successful, the actual detector geometry is Example jet image inputs from the jet substructure classification problem described in Ref. (46). ",
            "section": "Jet Classification",
            "paragraph_rank": 39,
            "section_rank": 10,
            "ref_spans": [
                {
                    "start": 176,
                    "end": 180,
                    "type": "bibr",
                    "ref_id": "b42",
                    "text": "(46)"
                }
            ]
        },
        {
            "text": "The background jets (left) are characterized by a large central core of deposited energy from a single hard hadronic parton, while the signal jets (right) tend to have a subtle secondary deposition due to the two-prong hadronic decay of a high-p T vector boson. ",
            "section": "Jet Classification",
            "paragraph_rank": 39,
            "section_rank": 10
        },
        {
            "text": "Use of image-analysis techniques such as convolutional neural networks allow for powerful analysis of this high-dimensional input data.",
            "section": "Jet Classification",
            "paragraph_rank": 39,
            "section_rank": 10
        },
        {
            "text": "not perfectly regular; thus, some preprocessing is required to represent the jet as an image. ",
            "section": "Jet Classification",
            "paragraph_rank": 40,
            "section_rank": 10
        },
        {
            "text": "In addition, jet images are typically very sparse. ",
            "section": "Jet Classification",
            "paragraph_rank": 40,
            "section_rank": 10
        },
        {
            "text": "The sparsity can be alleviated by enlarging pixels, but the harsher discretization sacrifices resolution in \u03b7 and \u03c6. ",
            "section": "Jet Classification",
            "paragraph_rank": 40,
            "section_rank": 10
        },
        {
            "text": "Given that the jets themselves are composed of a varying number of reconstructed constituents, each with welldefined coordinates and parameters, jet tagging algorithms that can work with a variable number of inputs are desirable. ",
            "section": "Jet Classification",
            "paragraph_rank": 40,
            "section_rank": 10
        },
        {
            "text": "Several flavor-tagging applications have made use of deep networks trained on variable length arrays of track parameters. ",
            "section": "Jet Classification",
            "paragraph_rank": 40,
            "section_rank": 10
        },
        {
            "text": "Guest et al. (54) investigated the need for featureengineering by defining low-level, mid-level, and high-level features, where the mid-and high-level features were inspired by typical flavor-tagging variables and derived from a strict subset of the low-level feature information. ",
            "section": "Jet Classification",
            "paragraph_rank": 40,
            "section_rank": 10,
            "ref_spans": [
                {
                    "start": 13,
                    "end": 17,
                    "type": "bibr",
                    "ref_id": "b50",
                    "text": "(54)"
                }
            ]
        },
        {
            "text": "The authors found similar discrimination using fixed-size, zero-padded networks and recurrent architectures, and that the best performance came from using all three levels of features (Figure 4). ",
            "section": "Jet Classification",
            "paragraph_rank": 40,
            "section_rank": 10,
            "ref_spans": [
                {
                    "start": 184,
                    "end": 193,
                    "type": "figure",
                    "text": "(Figure 4"
                }
            ]
        },
        {
            "text": "Both ATLAS and CMS have since commissioned flavor-tagging neural networks that rely on individual tracks or, in the CMS case, particle-flow candidates. ",
            "section": "Jet Classification",
            "paragraph_rank": 40,
            "section_rank": 10
        },
        {
            "text": "The ATLAS recurrent-network-based approach reduces backgrounds by roughly a factor of two when combined with traditional high-level variables (44,55). ",
            "section": "Jet Classification",
            "paragraph_rank": 40,
            "section_rank": 10
        },
        {
            "text": "CMS's DeepFlavor (56,57) neural network first embeds each flow candidate with a transformation that is shared across candidates, then combines the candidates' high-level variables in a single zero-padded dense network.",
            "section": "Jet Classification",
            "paragraph_rank": 40,
            "section_rank": 10,
            "entity_spans": [
                {
                    "start": 6,
                    "end": 17,
                    "type": "software",
                    "rawForm": "DeepFlavor",
                    "resp": "service",
                    "id": "software-simple-s12"
                }
            ]
        },
        {
            "text": "Networks trained on variable-length arrays of jet constituents proved equally useful in boosted top tagging. ",
            "section": "Jet Classification",
            "paragraph_rank": 41,
            "section_rank": 10
        },
        {
            "text": "In one series of studies, a zero-padded dense network showed promise (59), but backgrounds were halved by replacing the dense network with a recurrent network (60). ",
            "section": "Jet Classification",
            "paragraph_rank": 41,
            "section_rank": 10,
            "ref_spans": [
                {
                    "start": 69,
                    "end": 73,
                    "type": "bibr",
                    "ref_id": "b51",
                    "text": "(59)"
                },
                {
                    "start": 159,
                    "end": 163,
                    "type": "bibr",
                    "ref_id": "b52",
                    "text": "(60)"
                }
            ]
        },
        {
            "text": "The CMS Collaboration experimented with two variants of the DeepJet (61) algorithm. ",
            "section": "Jet Classification",
            "paragraph_rank": 41,
            "section_rank": 10,
            "entity_spans": [
                {
                    "start": 60,
                    "end": 68,
                    "type": "software",
                    "rawForm": "DeepJet",
                    "resp": "service",
                    "id": "software-simple-s13"
                }
            ]
        },
        {
            "text": "The first was similar to DeepFlavor, whereas the second replaced the dense network with a recurrent neural network. ",
            "section": "Jet Classification",
            "paragraph_rank": 41,
            "section_rank": 10,
            "entity_spans": [
                {
                    "start": 25,
                    "end": 35,
                    "type": "software",
                    "rawForm": "DeepFlavor",
                    "resp": "service",
                    "id": "software-simple-s14"
                }
            ]
        },
        {
            "text": "In comparison to a baseline that combined high-  A schematic showing the hierarchical composition of a deep learning model following the outline of traditional high-energy physics data pipeline. ",
            "section": "Jet Classification",
            "paragraph_rank": 41,
            "section_rank": 10
        },
        {
            "text": "The lowest-level detector inputs are represented by x i (diamonds), which are then fed into recurrent networks (lower boxes) to form jet embeddings h jet 1 . ",
            "section": "Jet Classification",
            "paragraph_rank": 41,
            "section_rank": 10
        },
        {
            "text": "These are augmented by jet-level features v i . ",
            "section": "Jet Classification",
            "paragraph_rank": 41,
            "section_rank": 10
        },
        {
            "text": "The jet embeddings are processed by a network to form a final event-level embedding h event , which is then fed into a classifier, leading to the output f event ({x i }). ",
            "section": "Jet Classification",
            "paragraph_rank": 41,
            "section_rank": 10
        },
        {
            "text": "The entire network can be learned jointly, or individual components can be pre-trained. ",
            "section": "Jet Classification",
            "paragraph_rank": 41,
            "section_rank": 10
        },
        {
            "text": "Adapted from (62).",
            "section": "Jet Classification",
            "paragraph_rank": 41,
            "section_rank": 10,
            "ref_spans": [
                {
                    "start": 13,
                    "end": 17,
                    "type": "bibr",
                    "ref_id": "b54",
                    "text": "(62)"
                }
            ]
        },
        {
            "text": "level variables in a boosted decision tree, QCD multijet misidentification was reduced by a factor of approximately four at 60% top-tagging efficiency (Figure 4) (58).",
            "section": "Jet Classification",
            "paragraph_rank": 42,
            "section_rank": 10,
            "ref_spans": [
                {
                    "start": 151,
                    "end": 160,
                    "type": "figure",
                    "text": "(Figure 4"
                }
            ]
        },
        {
            "text": "Recurrent networks act on sequences, requiring an ordering of the particles. ",
            "section": "Jet Classification",
            "paragraph_rank": 43,
            "section_rank": 10
        },
        {
            "text": "While sev-eral natural orderings exist, the most natural is arguably the kT jet clustering history, which defines a tree that can be used as the scaffolding for a recursive neural network. ",
            "section": "Jet Classification",
            "paragraph_rank": 43,
            "section_rank": 10
        },
        {
            "text": "Louppe et al. (62) applied a recursive neural network over the jet clustering history, providing a hybrid QCD-aware neural network strategy. ",
            "section": "Jet Classification",
            "paragraph_rank": 43,
            "section_rank": 10,
            "ref_spans": [
                {
                    "start": 14,
                    "end": 18,
                    "type": "bibr",
                    "ref_id": "b54",
                    "text": "(62)"
                }
            ]
        },
        {
            "text": "In the same W -versus-QCD jet classification problem as studied in Reference 50, the recursive network showed no improvement over image-based networks when trained on jet images, but improved substantially when the image preprocessing was removed. ",
            "section": "Jet Classification",
            "paragraph_rank": 43,
            "section_rank": 10
        },
        {
            "text": "Notably, the recursive and recurrent networks had fewer parameters and thus required far fewer data to train. ",
            "section": "Jet Classification",
            "paragraph_rank": 43,
            "section_rank": 10
        },
        {
            "text": "The same recursive neural network has been applied to quark-gluon tagging (63). ",
            "section": "Jet Classification",
            "paragraph_rank": 43,
            "section_rank": 10,
            "ref_spans": [
                {
                    "start": 74,
                    "end": 78,
                    "type": "bibr",
                    "ref_id": "b55",
                    "text": "(63)"
                }
            ]
        },
        {
            "text": "The clustering of objects need not end at the jet level ( Figure 5). ",
            "section": "Jet Classification",
            "paragraph_rank": 43,
            "section_rank": 10,
            "ref_spans": [
                {
                    "start": 58,
                    "end": 66,
                    "type": "figure",
                    "text": "Figure 5"
                }
            ]
        },
        {
            "text": "The outputs from the jet-level recurrent network can be fed into a recurrent network to produce a high-level event embedding.",
            "section": "Jet Classification",
            "paragraph_rank": 43,
            "section_rank": 10
        },
        {
            "text": "More recently, Henrion et. ",
            "section": "Jet Classification",
            "paragraph_rank": 44,
            "section_rank": 10
        },
        {
            "text": "al investigated representing jets as a graph instead of as a tree (64). ",
            "section": "Jet Classification",
            "paragraph_rank": 44,
            "section_rank": 10,
            "ref_spans": [
                {
                    "start": 66,
                    "end": 70,
                    "type": "bibr",
                    "ref_id": "b56",
                    "text": "(64)"
                }
            ]
        },
        {
            "text": "Graph convolutional networks provide a generalization of convolutional neural networks that can be applied to irregularly sampled data (65). ",
            "section": "Jet Classification",
            "paragraph_rank": 44,
            "section_rank": 10,
            "ref_spans": [
                {
                    "start": 135,
                    "end": 139,
                    "type": "bibr",
                    "ref_id": "b57",
                    "text": "(65)"
                }
            ]
        },
        {
            "text": "In this picture, the particles represent the nodes of the graph and the edges can encode how close the particles are in a learned adjacency matrix. ",
            "section": "Jet Classification",
            "paragraph_rank": 44,
            "section_rank": 10
        },
        {
            "text": "Henrion et. ",
            "section": "Jet Classification",
            "paragraph_rank": 44,
            "section_rank": 10
        },
        {
            "text": "al showed that such a network outperformed a recursive network for the same W vs. QCD jet tagging problem studied in Refs. ",
            "section": "Jet Classification",
            "paragraph_rank": 44,
            "section_rank": 10
        },
        {
            "text": "(50,62).",
            "section": "Jet Classification",
            "paragraph_rank": 44,
            "section_rank": 10,
            "ref_spans": [
                {
                    "start": 0,
                    "end": 4,
                    "type": "bibr",
                    "ref_id": "b46",
                    "text": "(50,"
                },
                {
                    "start": 4,
                    "end": 7,
                    "type": "bibr",
                    "ref_id": "b54",
                    "text": "62)"
                }
            ]
        },
        {
            "text": "Tracking",
            "section_rank": 11
        },
        {
            "text": "Track-reconstruction algorithms are among the most central processing unit (CPU) and data intensive of all low-level reconstruction tasks. ",
            "section": "Tracking",
            "paragraph_rank": 45,
            "section_rank": 11
        },
        {
            "text": "The initial stage of track reconstruction involves finding hits, or points where some charge is deposited on a sensing element. ",
            "section": "Tracking",
            "paragraph_rank": 45,
            "section_rank": 11
        },
        {
            "text": "In the case of the pixel sensors that form the innermost layer of the detector, neighboring hits are clustered into pixel clusters, which then form track seeds. ",
            "section": "Tracking",
            "paragraph_rank": 45,
            "section_rank": 11
        },
        {
            "text": "These seeds form a starting point for a Kalman filter, which extends the seeds into full tracks that extend to the calorimeters. ",
            "section": "Tracking",
            "paragraph_rank": 45,
            "section_rank": 11
        },
        {
            "text": "The entire procedure can be viewed as a sequence of clustering algorithms, in which the zero-suppressed readout from O(10 8 ) channels provides O(10 4 ) hits, which are then clustered into O(10 3 ) tracks per event.",
            "section": "Tracking",
            "paragraph_rank": 45,
            "section_rank": 11
        },
        {
            "text": "Machine learning has proven useful in several aspects of track reconstruction. ",
            "section": "Tracking",
            "paragraph_rank": 46,
            "section_rank": 11
        },
        {
            "text": "In cases where multiple tracks pass through the same pixel cluster, ATLAS relies on neural networks to return a measurement for each track rather than assigning each to the cluster center (66,67). ",
            "section": "Tracking",
            "paragraph_rank": 46,
            "section_rank": 11,
            "ref_spans": [
                {
                    "start": 188,
                    "end": 192,
                    "type": "bibr",
                    "ref_id": "b58",
                    "text": "(66,"
                },
                {
                    "start": 192,
                    "end": 195,
                    "type": "bibr",
                    "ref_id": "b59",
                    "text": "67)"
                }
            ]
        },
        {
            "text": "LHCb makes use of several shallow networks in track reconstruction as well. ",
            "section": "Tracking",
            "paragraph_rank": 46,
            "section_rank": 11
        },
        {
            "text": "Due to the long distances between sensor elements in the LHCb tracker, falsely connected hits forming \"ghost tracks\" are a large source of backgrounds. ",
            "section": "Tracking",
            "paragraph_rank": 46,
            "section_rank": 11
        },
        {
            "text": "A simple three-layer network reduces this background by a factor of two in comparison to a \u03c7 2 -based discriminant (68). ",
            "section": "Tracking",
            "paragraph_rank": 46,
            "section_rank": 11,
            "ref_spans": [
                {
                    "start": 115,
                    "end": 119,
                    "type": "bibr",
                    "ref_id": "b60",
                    "text": "(68)"
                }
            ]
        },
        {
            "text": "Several other networks are evaluated to filter out fake tracks before running a full track fit or to recover tracks with missing hits.",
            "section": "Tracking",
            "paragraph_rank": 46,
            "section_rank": 11
        },
        {
            "text": "Thanks to these algorithms and careful tuning, track reconstruction is nearly 100% efficient and spuriously reconstructed tracks are rare, meaning that the clustering aspect of tracking is largely solved. ",
            "section": "Tracking",
            "paragraph_rank": 47,
            "section_rank": 11
        },
        {
            "text": "Reducing the CPU overhead remains a significant problem, however, especially within high-level trigger farms. ",
            "section": "Tracking",
            "paragraph_rank": 47,
            "section_rank": 11
        },
        {
            "text": "Within ATLAS and CMS, these are clusters of O(10 4 ) processors that must reconstruct O(10 5 ) events per second (69). ",
            "section": "Tracking",
            "paragraph_rank": 47,
            "section_rank": 11,
            "ref_spans": [
                {
                    "start": 113,
                    "end": 117,
                    "type": "bibr",
                    "ref_id": "b61",
                    "text": "(69)"
                }
            ]
        },
        {
            "text": "To keep tracking CPU costs manageable, the experiments reconstruct tracks only in limited regions of the detector. ",
            "section": "Tracking",
            "paragraph_rank": 47,
            "section_rank": 11
        },
        {
            "text": "These regions are selected on the basis of their proximity to muons or to calorimeter energy deposits that are consistent with relatively rare physical signatures like leptons or high-pT jets. ",
            "section": "Tracking",
            "paragraph_rank": 47,
            "section_rank": 11
        },
        {
            "text": "While effective, this selective tracking severely hampers searches and physical measurements that rely on low-pT track-based signatures.",
            "section": "Tracking",
            "paragraph_rank": 47,
            "section_rank": 11
        },
        {
            "text": "These issues will be compounded considerably in the high-luminosity LHC (HL-LHC). ",
            "section": "Tracking",
            "paragraph_rank": 48,
            "section_rank": 11
        },
        {
            "text": "The majority of the tracking CPU budget is currently allocated to track-building phase, which depends on an expensive Kalman filter (70,71). ",
            "section": "Tracking",
            "paragraph_rank": 48,
            "section_rank": 11,
            "ref_spans": [
                {
                    "start": 132,
                    "end": 136,
                    "type": "bibr",
                    "ref_id": "b62",
                    "text": "(70,"
                },
                {
                    "start": 136,
                    "end": 139,
                    "type": "bibr",
                    "text": "71)"
                }
            ]
        },
        {
            "text": "With the higher track densities expected at the HL-LHC, the number of false seeds is expected to increase combinatorially (72), as is the probability of the Kalman filter building a branching track. ",
            "section": "Tracking",
            "paragraph_rank": 48,
            "section_rank": 11,
            "ref_spans": [
                {
                    "start": 122,
                    "end": 126,
                    "type": "bibr",
                    "ref_id": "b63",
                    "text": "(72)"
                }
            ]
        },
        {
            "text": "Morediscriminating seeding algorithms or trainable deep Kalman filters (73,74) could help reduce the number of track-fitting iterations.",
            "section": "Tracking",
            "paragraph_rank": 48,
            "section_rank": 11,
            "ref_spans": [
                {
                    "start": 71,
                    "end": 75,
                    "type": "bibr",
                    "ref_id": "b64",
                    "text": "(73,"
                },
                {
                    "start": 75,
                    "end": 78,
                    "type": "bibr",
                    "ref_id": "b65",
                    "text": "74)"
                }
            ]
        },
        {
            "text": "Unfortunately, most tracking software is deeply interwoven with the experiments' reconstruction frameworks and, as a result, is poorly suited for the quick exploratory studies that will be needed to develop the next generation of algorithms.",
            "section": "Tracking",
            "paragraph_rank": 49,
            "section_rank": 11
        },
        {
            "text": "In anticipation of the coming HL-LHC data onslaught, however, improved tracking is essential. ",
            "section": "Tracking",
            "paragraph_rank": 50,
            "section_rank": 11
        },
        {
            "text": "Several projects aim to accomplish this goal indirectly by increasing the visibility and accessibility of tracking software. ",
            "section": "Tracking",
            "paragraph_rank": 50,
            "section_rank": 11
        },
        {
            "text": "Most ambitiously, the ACTS project (see http://acts.web.cern.ch/ACTS/index.php, https://github.com/trackml) seeks to implement a modular and experiment-independent software stack for tracking-related studies, including detector geometry, an event data model, and seed-finding and trackfitting tools. ",
            "section": "Tracking",
            "paragraph_rank": 50,
            "section_rank": 11,
            "entity_spans": [
                {
                    "start": 40,
                    "end": 78,
                    "type": "url",
                    "rawForm": "http://acts.web.cern.ch/ACTS/index.php",
                    "resp": "service",
                    "id": "#software-simple-s15"
                }
            ]
        },
        {
            "text": "Simplified tracking models (75) have been used as a basis for studies showing the viability of LSTMs for track building (76) or for tracking data challenges (77).",
            "section": "Tracking",
            "paragraph_rank": 50,
            "section_rank": 11,
            "ref_spans": [
                {
                    "start": 27,
                    "end": 31,
                    "type": "bibr",
                    "ref_id": "b66",
                    "text": "(75)"
                },
                {
                    "start": 120,
                    "end": 124,
                    "type": "bibr",
                    "ref_id": "b67",
                    "text": "(76)"
                }
            ]
        },
        {
            "text": "Fast Simulation",
            "section_rank": 12
        },
        {
            "text": "The ability to model high-dimensional distributions not only enables improved statistical analysis, but also provides a new path towards fast simulation. ",
            "section": "Fast Simulation",
            "paragraph_rank": 51,
            "section_rank": 12
        },
        {
            "text": "Fast simulation is valuable because the full simulators, which faithfully describe the low-level interactions of particles with matter, are very computationally intensive and consume a significant fraction of the computing budgets of current experimental collaborations.",
            "section": "Fast Simulation",
            "paragraph_rank": 51,
            "section_rank": 12
        },
        {
            "text": "Until now the dominant approach to fast simulation has been to develop fast parametric distributions largely by hand (78). ",
            "section": "Fast Simulation",
            "paragraph_rank": 52,
            "section_rank": 12
        },
        {
            "text": "A more recent deep-learning approach is to train a network to learn the simulation from an initial pool of traditionally simulated events. ",
            "section": "Fast Simulation",
            "paragraph_rank": 52,
            "section_rank": 12
        },
        {
            "text": "This approach creates a generative model G which which approximates the distribution of samples produced by the simulator by mapping an input space of random numbers to the space of the data.",
            "section": "Fast Simulation",
            "paragraph_rank": 52,
            "section_rank": 12
        },
        {
            "text": "One promising approach is based on Generative Adversarial Networks (GANs). ",
            "section": "Fast Simulation",
            "paragraph_rank": 53,
            "section_rank": 12
        },
        {
            "text": "The training of the generative model G is accomplished through competition with an adversary network A. While G generates simulated samples, the adversarial network A tries to determine whether a given sample is from the generative model or from the full simulator. ",
            "section": "Fast Simulation",
            "paragraph_rank": 53,
            "section_rank": 12
        },
        {
            "text": "The two networks are pitted against each other: A attempts to identify differences between the traditional samples and those generated by G, while G attempts to fool A into accepting its events, and in doing so learns to mimic the original sample generation. ",
            "section": "Fast Simulation",
            "paragraph_rank": 53,
            "section_rank": 12
        },
        {
            "text": "The stability of such a training arrangement, however, can be difficult to achieve, and expert knowledge is required to construct an effective network.",
            "section": "Fast Simulation",
            "paragraph_rank": 53,
            "section_rank": 12
        },
        {
            "text": "Paganini et al. (85) applied the GAN approach to simulation of the electromagnetic showers in a multi-layer calorimeter, one of the most computationally expensive steps for the low-level simulator. ",
            "section": "Fast Simulation",
            "paragraph_rank": 54,
            "section_rank": 12,
            "ref_spans": [
                {
                    "start": 16,
                    "end": 20,
                    "type": "bibr",
                    "ref_id": "b75",
                    "text": "(85)"
                }
            ]
        },
        {
            "text": "They report large computational speedups while achieving reasonable modeling of the energy deposition, though not yet matching the accuracy of the full simulation. ",
            "section": "Fast Simulation",
            "paragraph_rank": 54,
            "section_rank": 12
        },
        {
            "text": "In a related approach, de Olivereira et al. find similar success simulating jet images (86). ",
            "section": "Fast Simulation",
            "paragraph_rank": 54,
            "section_rank": 12,
            "ref_spans": [
                {
                    "start": 87,
                    "end": 91,
                    "type": "bibr",
                    "ref_id": "b76",
                    "text": "(86)"
                }
            ]
        },
        {
            "text": "Future simulation tools built on GANs may provide important speed boosts for the slower elements of the simulation chain, or they may be sophisticated enough to provide end-to-end simulations.",
            "section": "Fast Simulation",
            "paragraph_rank": 54,
            "section_rank": 12
        },
        {
            "text": "The resulting network evaluation is much less computationally demanding than the lowlevel simulation, and can be viewed as a non-parametric fast simulation. ",
            "section": "Fast Simulation",
            "paragraph_rank": 55,
            "section_rank": 12
        },
        {
            "text": "The promise of this approach to mitigate the computational burden for simulation has been called out in the strategic planning for HL-LHC software efforts (87,88). ",
            "section": "Fast Simulation",
            "paragraph_rank": 55,
            "section_rank": 12,
            "ref_spans": [
                {
                    "start": 155,
                    "end": 159,
                    "type": "bibr",
                    "ref_id": "b77",
                    "text": "(87,"
                },
                {
                    "start": 159,
                    "end": 162,
                    "type": "bibr",
                    "ref_id": "b78",
                    "text": "88)"
                }
            ]
        },
        {
            "text": "See Refs. ",
            "section": "Fast Simulation",
            "paragraph_rank": 55,
            "section_rank": 12
        },
        {
            "text": "(79)(80)(81)(82)(83)(84) for alternative approaches to fast simulation.",
            "section": "Fast Simulation",
            "paragraph_rank": 55,
            "section_rank": 12,
            "ref_spans": [
                {
                    "start": 0,
                    "end": 4,
                    "type": "bibr",
                    "ref_id": "b69",
                    "text": "(79)"
                },
                {
                    "start": 4,
                    "end": 8,
                    "type": "bibr",
                    "ref_id": "b70",
                    "text": "(80)"
                },
                {
                    "start": 8,
                    "end": 12,
                    "type": "bibr",
                    "ref_id": "b71",
                    "text": "(81)"
                },
                {
                    "start": 12,
                    "end": 16,
                    "type": "bibr",
                    "ref_id": "b72",
                    "text": "(82)"
                },
                {
                    "start": 16,
                    "end": 20,
                    "type": "bibr",
                    "ref_id": "b73",
                    "text": "(83)"
                },
                {
                    "start": 20,
                    "end": 24,
                    "type": "bibr",
                    "ref_id": "b74",
                    "text": "(84)"
                }
            ]
        },
        {
            "text": "Impact",
            "section_rank": 13
        },
        {
            "text": "Taken together, the new tools made possible by deep learning promise to make a significant impact on high-energy physics. ",
            "section": "Impact",
            "paragraph_rank": 56,
            "section_rank": 13
        },
        {
            "text": "The specific examples above -mass resconstruction, jet substructure and jet-flavor classification -are important benchmarks and long-standing challenges. ",
            "section": "Impact",
            "paragraph_rank": 56,
            "section_rank": 13
        },
        {
            "text": "The significant improvements offered by deep learning in these areas support the claim that many areas of LHC data analysis suffer from long-standing sub-optimal feature engineering, and deserve re-examination.",
            "section": "Impact",
            "paragraph_rank": 56,
            "section_rank": 13
        },
        {
            "text": "CONCERNS",
            "section_rank": 14
        },
        {
            "text": "What Is the Optimization Objective?",
            "section_rank": 15
        },
        {
            "text": "A challenge of incorporating machine learning techniques into HEP data analysis is that tools are often optimized for performance on a particular task that is several steps removed from the ultimate physical goal of searching for a new particle or testing a new physical theory. ",
            "section": "What Is the Optimization Objective?",
            "paragraph_rank": 57,
            "section_rank": 15
        },
        {
            "text": "Moreover, some tools are used in multiple applications, which may have conflicting demands. ",
            "section": "What Is the Optimization Objective?",
            "paragraph_rank": 57,
            "section_rank": 15
        },
        {
            "text": "For instance, a deep learning jet flavor-tagging algorithm might be used for searches for supersymmetry as well as precision measurements of the Higgs sector, which may have different needs with respect to balancing signal efficiency and background rejection.",
            "section": "What Is the Optimization Objective?",
            "paragraph_rank": 57,
            "section_rank": 15
        },
        {
            "text": "These considerations are further complicated by the fact that the sensitivity to highlevel physics questions must account for systematic uncertainties, which involve a nonlinear trade-off between the typical machine learning performance metrics and the systematic uncertainty estimates. ",
            "section": "What Is the Optimization Objective?",
            "paragraph_rank": 58,
            "section_rank": 15
        },
        {
            "text": "For example, a new classifier may have a better false-positive rate than a baseline algorithm, yet simultaneously be more susceptible to systematic mismodeling between the simulation and the real data. ",
            "section": "What Is the Optimization Objective?",
            "paragraph_rank": 58,
            "section_rank": 15
        },
        {
            "text": "Whether or not this new classifier will improve the sensitivity for the ultimate high-level physics goal depends on details such as the signalto-background ratio, the total number of data, and the size of the systematic uncertainty, which are not typically included in the classifier training.",
            "section": "What Is the Optimization Objective?",
            "paragraph_rank": 58,
            "section_rank": 15
        },
        {
            "text": "Traditionally, HEP physicists have taken these considerations into account through heuristics and intuition. ",
            "section": "What Is the Optimization Objective?",
            "paragraph_rank": 59,
            "section_rank": 15
        },
        {
            "text": "But as deep learning penetrates into the analysis pipeline, it is important to revisit these trade-offs and attempt to make them explicit to design new loss functions and learning algorithms that directly optimize for our ultimate physics goals.",
            "section": "What Is the Optimization Objective?",
            "paragraph_rank": 59,
            "section_rank": 15
        },
        {
            "text": "For example, in order to to be robust to systematic uncertainties, one can use a classifier parametrized in terms of the nuisance parameters (38,82), allowing for major speedups compared to earlier strategies (89). ",
            "section": "What Is the Optimization Objective?",
            "paragraph_rank": 60,
            "section_rank": 15,
            "ref_spans": [
                {
                    "start": 141,
                    "end": 145,
                    "type": "bibr",
                    "ref_id": "b36",
                    "text": "(38,"
                },
                {
                    "start": 145,
                    "end": 148,
                    "type": "bibr",
                    "ref_id": "b72",
                    "text": "82)"
                },
                {
                    "start": 209,
                    "end": 213,
                    "type": "bibr",
                    "ref_id": "b79",
                    "text": "(89)"
                }
            ]
        },
        {
            "text": "An alternative approach is to train a network to be insensitive to the systematic uncertainty, which is achieved either by boosting (90) or by using an adversarial training procedure that encourages the output of the network to be independent of the nuisance parameters (91,92). ",
            "section": "What Is the Optimization Objective?",
            "paragraph_rank": 60,
            "section_rank": 15,
            "ref_spans": [
                {
                    "start": 132,
                    "end": 136,
                    "type": "bibr",
                    "ref_id": "b80",
                    "text": "(90)"
                },
                {
                    "start": 270,
                    "end": 274,
                    "type": "bibr",
                    "ref_id": "b81",
                    "text": "(91,"
                },
                {
                    "start": 274,
                    "end": 277,
                    "type": "bibr",
                    "ref_id": "b82",
                    "text": "92)"
                }
            ]
        },
        {
            "text": "The technique can also be used to enforce independence from another variable, such as the jet mass (93). ",
            "section": "What Is the Optimization Objective?",
            "paragraph_rank": 60,
            "section_rank": 15,
            "ref_spans": [
                {
                    "start": 99,
                    "end": 103,
                    "type": "bibr",
                    "ref_id": "b83",
                    "text": "(93)"
                }
            ]
        },
        {
            "text": "In general, there is a trade-off between performance in the classification (or regression) task and robustness to systematics, which can be adjusted via a hyperparameter \u03bb in the objective function. ",
            "section": "What Is the Optimization Objective?",
            "paragraph_rank": 60,
            "section_rank": 15
        },
        {
            "text": "Unfortunately, optimization of \u03bb requires retraining the network, and the objective function may not be differentiable with respect to \u03bb.",
            "section": "What Is the Optimization Objective?",
            "paragraph_rank": 60,
            "section_rank": 15
        },
        {
            "text": "Optimization of differentiable components is efficiently handled with various forms of stochastic gradient descent, although these algorithms often come with their own hyperparameters. ",
            "section": "What Is the Optimization Objective?",
            "paragraph_rank": 61,
            "section_rank": 15
        },
        {
            "text": "The optimization with respect to hyperparameters that arise in the network architecture, loss function, and learning algorithms are often performed through a black-box optimization algorithm that does not require gradients. ",
            "section": "What Is the Optimization Objective?",
            "paragraph_rank": 61,
            "section_rank": 15
        },
        {
            "text": "This includes Bayesian optimization (94,95) and genetic algorithms (89), as well as variational optimization (96,97).",
            "section": "What Is the Optimization Objective?",
            "paragraph_rank": 61,
            "section_rank": 15,
            "ref_spans": [
                {
                    "start": 36,
                    "end": 40,
                    "type": "bibr",
                    "ref_id": "b84",
                    "text": "(94,"
                },
                {
                    "start": 40,
                    "end": 43,
                    "type": "bibr",
                    "ref_id": "b85",
                    "text": "95)"
                },
                {
                    "start": 67,
                    "end": 71,
                    "type": "bibr",
                    "ref_id": "b79",
                    "text": "(89)"
                },
                {
                    "start": 109,
                    "end": 113,
                    "type": "bibr",
                    "ref_id": "b86",
                    "text": "(96,"
                },
                {
                    "start": 113,
                    "end": 116,
                    "type": "bibr",
                    "ref_id": "b87",
                    "text": "97)"
                }
            ]
        },
        {
            "text": "Interpretability and Reliance on Simulation",
            "section_rank": 16
        },
        {
            "text": "Machine learning provides an effective and powerful solution to many data analysis challenges in HEP, in some cases lessening the need for engineered features driven by physical insight. ",
            "section": "Interpretability and Reliance on Simulation",
            "paragraph_rank": 62,
            "section_rank": 16
        },
        {
            "text": "However, in some sense this approach is not satisfactory, as progress on the computational side is not always matched by gains in physical understanding and is heavily reliant on simulation programs.",
            "section": "Interpretability and Reliance on Simulation",
            "paragraph_rank": 62,
            "section_rank": 16
        },
        {
            "text": "The nonparametric nature of the neural network approach makes it very difficult to interpret the solution. ",
            "section": "Interpretability and Reliance on Simulation",
            "paragraph_rank": 63,
            "section_rank": 16
        },
        {
            "text": "Unlike a simple analytic function written in mathematical language familiar to physicists, a neural network cannot be easily inspected to discover the structure of its learned solution. ",
            "section": "Interpretability and Reliance on Simulation",
            "paragraph_rank": 63,
            "section_rank": 16
        },
        {
            "text": "Due to the high-dimensional nature of the input data x, reverseengineering the classification strategy to identify signal-like or background-like regions of the original space is also very challenging (98). ",
            "section": "Interpretability and Reliance on Simulation",
            "paragraph_rank": 63,
            "section_rank": 16
        },
        {
            "text": "This is not surprising, and generically we should anticipate a trade-off between performance and interpretability.",
            "section": "Interpretability and Reliance on Simulation",
            "paragraph_rank": 63,
            "section_rank": 16
        },
        {
            "text": "Aside from understanding the nature of the decision, the use of machine learning also complicates the scientific communication and theoretical interpretation of LHC results. ",
            "section": "Interpretability and Reliance on Simulation",
            "paragraph_rank": 64,
            "section_rank": 16
        },
        {
            "text": "While traditional cut-based analyses can be conveyed in tables and prose, a learned neural network cannot. ",
            "section": "Interpretability and Reliance on Simulation",
            "paragraph_rank": 64,
            "section_rank": 16
        },
        {
            "text": "Not only does this make it difficult for a reader to glean the essential physics, it is also an impediment to reinterpreting the result in the context of a different theoretical model. ",
            "section": "Interpretability and Reliance on Simulation",
            "paragraph_rank": 64,
            "section_rank": 16
        },
        {
            "text": "This issue further motivates analysis preservation and reinterpretation systems, such as RECAST (99,100), that can re-execute the original data analysis pipeline.",
            "section": "Interpretability and Reliance on Simulation",
            "paragraph_rank": 64,
            "section_rank": 16,
            "ref_spans": [
                {
                    "start": 96,
                    "end": 100,
                    "type": "bibr",
                    "ref_id": "b89",
                    "text": "(99,"
                },
                {
                    "start": 100,
                    "end": 104,
                    "type": "bibr",
                    "ref_id": "b90",
                    "text": "100)"
                }
            ],
            "entity_spans": [
                {
                    "start": 89,
                    "end": 96,
                    "type": "software",
                    "rawForm": "RECAST",
                    "resp": "service",
                    "id": "software-simple-s16"
                }
            ]
        },
        {
            "text": "Systematic uncertainties due to mismodeling in the simulation are also a major concern. ",
            "section": "Interpretability and Reliance on Simulation",
            "paragraph_rank": 65,
            "section_rank": 16
        },
        {
            "text": "The networks are routinely trained using large samples of simulated collisions, and the nature of their solution is relevant to experimental data only if those simulated samples are faithful descriptions of the collected data. ",
            "section": "Interpretability and Reliance on Simulation",
            "paragraph_rank": 65,
            "section_rank": 16
        },
        {
            "text": "Although the simulation programs have been extensively tuned and validated over years of use, skepticism remains about their ability to accurately describe the correlations in a high-dimensional space, leading to reasonable concerns about whether a network's learned solution relies on a well-modeled physical effect or an overlooked weak point. ",
            "section": "Interpretability and Reliance on Simulation",
            "paragraph_rank": 65,
            "section_rank": 16
        },
        {
            "text": "This concern applies to shallow networks as well, but is even more severe when working with higher-dimensional lower-level data.",
            "section": "Interpretability and Reliance on Simulation",
            "paragraph_rank": 65,
            "section_rank": 16
        },
        {
            "text": "One means to assuage these concerns is a classic piece of experimental scientific strategy: validation using adjacent control regions. ",
            "section": "Interpretability and Reliance on Simulation",
            "paragraph_rank": 66,
            "section_rank": 16
        },
        {
            "text": "The primary concern is whether the correlations among the input features are well modeled, which can be verified through a comparison of the network function evaluation in real and simulated data. ",
            "section": "Interpretability and Reliance on Simulation",
            "paragraph_rank": 66,
            "section_rank": 16
        },
        {
            "text": "Adjacent control regions can be employed to keep the data that are most sensitive to the hypothetical signal blind. ",
            "section": "Interpretability and Reliance on Simulation",
            "paragraph_rank": 66,
            "section_rank": 16
        },
        {
            "text": "This technique is often applied in the case of single-dimensional data analysis, and is essentially a generalization of the sideband approach. ",
            "section": "Interpretability and Reliance on Simulation",
            "paragraph_rank": 66,
            "section_rank": 16
        },
        {
            "text": "If the input feature correlations that the network function relies on are poorly modeled, the distributions of the real and simulated data will disagree. ",
            "section": "Interpretability and Reliance on Simulation",
            "paragraph_rank": 66,
            "section_rank": 16
        },
        {
            "text": "This provides some validation of the network function, but no insight into its structure.",
            "section": "Interpretability and Reliance on Simulation",
            "paragraph_rank": 66,
            "section_rank": 16
        },
        {
            "text": "An alternative approach relaxes the reliance on simulated samples by using real data in the training step and avoids the need for training labels by using weakly supervised learning (101,102). ",
            "section": "Interpretability and Reliance on Simulation",
            "paragraph_rank": 67,
            "section_rank": 16,
            "ref_spans": [
                {
                    "start": 182,
                    "end": 187,
                    "type": "bibr",
                    "ref_id": "b91",
                    "text": "(101,"
                },
                {
                    "start": 187,
                    "end": 191,
                    "type": "bibr",
                    "text": "102)"
                }
            ]
        },
        {
            "text": "In one approach, one needs to know only the proportion of labels in different subsets of data (101), for instance, samples of events with known proportions of quark and gluon jets. ",
            "section": "Interpretability and Reliance on Simulation",
            "paragraph_rank": 67,
            "section_rank": 16,
            "ref_spans": [
                {
                    "start": 94,
                    "end": 99,
                    "type": "bibr",
                    "ref_id": "b91",
                    "text": "(101)"
                }
            ]
        },
        {
            "text": "One approach, known as classification without labels (CWoLa), requires only that the different samples of events have different label proportions even if they are unknown (102).",
            "section": "Interpretability and Reliance on Simulation",
            "paragraph_rank": 67,
            "section_rank": 16,
            "entity_spans": [
                {
                    "start": 54,
                    "end": 59,
                    "type": "software",
                    "rawForm": "CWoLa",
                    "resp": "service",
                    "id": "software-simple-s17"
                }
            ]
        },
        {
            "text": "In other cases, the weak points of the modeling are well known to physicists, who would prefer a learned solution that avoids detailed reliance on these features. ",
            "section": "Interpretability and Reliance on Simulation",
            "paragraph_rank": 68,
            "section_rank": 16
        },
        {
            "text": "For example, most simulated samples of collisions that result in jets use either Pythia (6) or Herwig (7) to model the parton shower, but these two heuristic approaches can make significantly different predictions about the jet images (50). ",
            "section": "Interpretability and Reliance on Simulation",
            "paragraph_rank": 68,
            "section_rank": 16,
            "ref_spans": [
                {
                    "start": 235,
                    "end": 239,
                    "type": "bibr",
                    "ref_id": "b46",
                    "text": "(50)"
                }
            ],
            "entity_spans": [
                {
                    "start": 81,
                    "end": 88,
                    "type": "software",
                    "rawForm": "Pythia",
                    "resp": "service",
                    "id": "software-simple-s18"
                },
                {
                    "start": 95,
                    "end": 102,
                    "type": "software",
                    "rawForm": "Herwig",
                    "resp": "service",
                    "id": "software-simple-s19"
                }
            ]
        },
        {
            "text": "One way to address this issue is to explicitly parameterize the network in the space of the unknown nuisance parameter (38,82) so that the dependence can be studied or constrained in data. ",
            "section": "Interpretability and Reliance on Simulation",
            "paragraph_rank": 68,
            "section_rank": 16,
            "ref_spans": [
                {
                    "start": 119,
                    "end": 123,
                    "type": "bibr",
                    "ref_id": "b36",
                    "text": "(38,"
                },
                {
                    "start": 123,
                    "end": 126,
                    "type": "bibr",
                    "ref_id": "b72",
                    "text": "82)"
                }
            ]
        },
        {
            "text": "Another is to attempt to explicitly reduce the dependence of the network on aspects that are sensitive to the underlying uncertainties (92), making the resulting network less sensitive to these uncertainties. ",
            "section": "Interpretability and Reliance on Simulation",
            "paragraph_rank": 68,
            "section_rank": 16
        },
        {
            "text": "This can be regarded as a constrained optimization problem; for example, one might seek an optimal combination of jet substructure tagging variables that does not distort the smooth background (93).",
            "section": "Interpretability and Reliance on Simulation",
            "paragraph_rank": 68,
            "section_rank": 16
        },
        {
            "text": "Software",
            "section_rank": 17
        },
        {
            "text": "The growing complexity of neural networks, in terms of both the architecture and the raw computational power required for training, might seem overwhelming to the pragmatic high-energy physicist. ",
            "section": "Software",
            "paragraph_rank": 69,
            "section_rank": 17
        },
        {
            "text": "Worse, the summary above is merely a brief review of state-of-the-art deep learning, and given the rapid pace at which the field evolves, techniques may change in the near future. ",
            "section": "Software",
            "paragraph_rank": 69,
            "section_rank": 17
        },
        {
            "text": "Fortunately, a number of software packages (11-13) already provide efficient automatic gradient computation and interfaces to hardware such as GPUs. ",
            "section": "Software",
            "paragraph_rank": 69,
            "section_rank": 17
        },
        {
            "text": "These are well supported outside HEP. ",
            "section": "Software",
            "paragraph_rank": 69,
            "section_rank": 17,
            "entity_spans": [
                {
                    "start": 33,
                    "end": 36,
                    "type": "software",
                    "rawForm": "HEP",
                    "resp": "service",
                    "id": "software-simple-s20"
                }
            ]
        },
        {
            "text": "Thanks to these tools, the role of the physicist is reduced to choosing an appropriate problem, data representation, architecture, and training strategy.",
            "section": "Software",
            "paragraph_rank": 69,
            "section_rank": 17
        },
        {
            "text": "The software landscape continues to evolve rapidly compared with typical timescales for software in collider physics. ",
            "section": "Software",
            "paragraph_rank": 70,
            "section_rank": 17
        },
        {
            "text": "This contrast presents a challenge for any experiment using deep learning: By choosing one of the dozen currently available frameworks, the experiment risks being marooned with an unsupported and bloated dependency when the deep learning industry moves on. ",
            "section": "Software",
            "paragraph_rank": 70,
            "section_rank": 17
        },
        {
            "text": "Fortunately, while projections into the future of a specific deep learning package, or even a particular architecture, would be premature, the underlying representation of a network as a stack of differentiable tensor operations has proven quite robust.",
            "section": "Software",
            "paragraph_rank": 70,
            "section_rank": 17
        },
        {
            "text": "As deep learning matures, the language and specifications become more precise. ",
            "section": "Software",
            "paragraph_rank": 71,
            "section_rank": 17
        },
        {
            "text": "Several ongoing projects with significant commercial backing (103; for a list of useful tools and references bridging the gap between collider physics and machine learning, see the following repository: https://github.com/iml-wg/HEP-ML-Resources) seek to formalize these specifications further. ",
            "section": "Software",
            "paragraph_rank": 71,
            "section_rank": 17,
            "entity_spans": [
                {
                    "start": 203,
                    "end": 233,
                    "type": "url",
                    "rawForm": "https://github.com/iml-wg/HEP-",
                    "resp": "service",
                    "id": "#software-simple-s20"
                }
            ]
        },
        {
            "text": "Such formal specifications allow a factorization between the training phase-which can depend on specific hardware and a myriad of software packages-and the application or inference phase. ",
            "section": "Software",
            "paragraph_rank": 71,
            "section_rank": 17
        },
        {
            "text": "Training a neural network requires millions of iterations, whereas inference requires only a single pass per classified pattern. ",
            "section": "Software",
            "paragraph_rank": 71,
            "section_rank": 17
        },
        {
            "text": "As a result, computational demands at the inference stage are mild. ",
            "section": "Software",
            "paragraph_rank": 71,
            "section_rank": 17
        },
        {
            "text": "The factorization enables inference-only implementations (104) or autogenerated inference functions to be the primary vehicle for incorporation into the trigger and reconstruction software of the LHC experiments.",
            "section": "Software",
            "paragraph_rank": 71,
            "section_rank": 17,
            "ref_spans": [
                {
                    "start": 57,
                    "end": 62,
                    "type": "bibr",
                    "ref_id": "b93",
                    "text": "(104)"
                }
            ]
        },
        {
            "text": "In contrast, as machine learning is incorporated into high-level data analysis, there is a benefit to having closer integration of modern machine learning frameworks and statistical analysis software. ",
            "section": "Software",
            "paragraph_rank": 72,
            "section_rank": 17
        },
        {
            "text": "Various tools are beginning to blend deep learning, probabilistic modeling, and statistical inference (37,(105)(106)(107).",
            "section": "Software",
            "paragraph_rank": 72,
            "section_rank": 17,
            "ref_spans": [
                {
                    "start": 102,
                    "end": 106,
                    "type": "bibr",
                    "ref_id": "b35",
                    "text": "(37,"
                },
                {
                    "start": 106,
                    "end": 111,
                    "type": "bibr",
                    "ref_id": "b94",
                    "text": "(105)"
                },
                {
                    "start": 111,
                    "end": 116,
                    "type": "bibr",
                    "ref_id": "b95",
                    "text": "(106)"
                },
                {
                    "start": 116,
                    "end": 121,
                    "type": "bibr",
                    "ref_id": "b96",
                    "text": "(107)"
                }
            ]
        },
        {
            "text": "PROSPECTS",
            "section_rank": 18
        },
        {
            "text": "In the past few years, advances in machine learning have enabled the development of tools that have the power to transform the nature of data analysis in HEP.",
            "section": "PROSPECTS",
            "paragraph_rank": 73,
            "section_rank": 18,
            "entity_spans": [
                {
                    "start": 154,
                    "end": 157,
                    "type": "software",
                    "rawForm": "HEP",
                    "resp": "service",
                    "id": "software-simple-s21"
                }
            ]
        },
        {
            "text": "Expected increases in computational power and further advances in training strategies can be reasonably expected to extend the power of these tools. ",
            "section": "PROSPECTS",
            "paragraph_rank": 74,
            "section_rank": 18
        },
        {
            "text": "But important questions remain regarding how to best apply this power. ",
            "section": "PROSPECTS",
            "paragraph_rank": 74,
            "section_rank": 18
        },
        {
            "text": "Should physicists aim for end-to-end learning, giving the network the data at the lowest level and highest dimensionality that it can effectively process and ask it to solve the entire problem at once? ",
            "section": "PROSPECTS",
            "paragraph_rank": 74,
            "section_rank": 18
        },
        {
            "text": "Or is it more sensible to maintain outlines of the existing structures, but replace engineered solutions based on domain knowledge with learned solutions? ",
            "section": "PROSPECTS",
            "paragraph_rank": 74,
            "section_rank": 18
        },
        {
            "text": "A middle road, inspired by the hierarchical nature of the LHC data and the success of highly structured networks, suggests building end-to-end tools whose internal structure reflects the outlines of existing analysis pipelines ( Figure 5). ",
            "section": "PROSPECTS",
            "paragraph_rank": 74,
            "section_rank": 18,
            "ref_spans": [
                {
                    "start": 229,
                    "end": 237,
                    "type": "figure",
                    "text": "Figure 5"
                }
            ]
        },
        {
            "text": "Planning for such future efforts is already under way (87,88).",
            "section": "PROSPECTS",
            "paragraph_rank": 74,
            "section_rank": 18,
            "ref_spans": [
                {
                    "start": 54,
                    "end": 58,
                    "type": "bibr",
                    "ref_id": "b77",
                    "text": "(87,"
                },
                {
                    "start": 58,
                    "end": 61,
                    "type": "bibr",
                    "ref_id": "b78",
                    "text": "88)"
                }
            ]
        },
        {
            "text": "Beyond the application of deep learning to the problems described in this review, related research in the fields of statistical and machine learning offer promising solutions to the challenges of data analysis in particle physics. ",
            "section": "PROSPECTS",
            "paragraph_rank": 75,
            "section_rank": 18
        },
        {
            "text": "For example, the problem of modeling smooth background distributions from observed data has long been treated using ad hoc parametric functions; techniques from the study of Gaussian processes have recently been shown to provide a powerful and promising alternative (108). ",
            "section": "PROSPECTS",
            "paragraph_rank": 75,
            "section_rank": 18,
            "ref_spans": [
                {
                    "start": 266,
                    "end": 271,
                    "type": "bibr",
                    "ref_id": "b97",
                    "text": "(108)"
                }
            ]
        },
        {
            "text": "Another area with significant untapped potential with relevance to collider physics is that of anomaly detection; recent techniques (109) have improved the data compression and anomaly detecting speed such that applications to use these techniques to search for anomalous signatures in the LHC data set may be practical. ",
            "section": "PROSPECTS",
            "paragraph_rank": 75,
            "section_rank": 18,
            "ref_spans": [
                {
                    "start": 132,
                    "end": 137,
                    "type": "bibr",
                    "ref_id": "b98",
                    "text": "(109)"
                }
            ]
        },
        {
            "text": "Recently, Bayesian optimization has been used for more efficient tuning of the simulation programs (110), and adversarial training of GANs has been extended to the tuning of nondifferentiable simulators (97). ",
            "section": "PROSPECTS",
            "paragraph_rank": 75,
            "section_rank": 18,
            "ref_spans": [
                {
                    "start": 99,
                    "end": 104,
                    "type": "bibr",
                    "ref_id": "b99",
                    "text": "(110)"
                },
                {
                    "start": 203,
                    "end": 207,
                    "type": "bibr",
                    "ref_id": "b87",
                    "text": "(97)"
                }
            ]
        },
        {
            "text": "Finally, several groups have used machine learning to grapple with the interpretation of results in high-dimensional parameter spaces for theories such as supersymmetry (111,112; also see https://indico.cern.ch/event/632141/).",
            "section": "PROSPECTS",
            "paragraph_rank": 75,
            "section_rank": 18,
            "ref_spans": [
                {
                    "start": 169,
                    "end": 174,
                    "type": "bibr",
                    "ref_id": "b100",
                    "text": "(111,"
                },
                {
                    "start": 174,
                    "end": 177,
                    "type": "bibr",
                    "ref_id": "b101",
                    "text": "112"
                }
            ]
        },
        {
            "text": "One of the most profound developments in machine learning is the ability to model high-dimensional distributions from large samples of data. ",
            "section": "PROSPECTS",
            "paragraph_rank": 76,
            "section_rank": 18
        },
        {
            "text": "The ability to estimate high-dimensional probability densities or density ratios enables probabilistic inference in situations that were previously intractable. ",
            "section": "PROSPECTS",
            "paragraph_rank": 76,
            "section_rank": 18
        },
        {
            "text": "The key machine learning developments here (79-84, 113) allow the tuning of the classification tool for the particular problem at hand, opening the door to deeper levels of optimization and potentially more powerful analyses.",
            "section": "PROSPECTS",
            "paragraph_rank": 76,
            "section_rank": 18
        },
        {
            "text": "Further work in this area includes efforts to modify the simulation tools for improved sampling of the high dimensional space (114,115). ",
            "section": "PROSPECTS",
            "paragraph_rank": 77,
            "section_rank": 18,
            "ref_spans": [
                {
                    "start": 126,
                    "end": 131,
                    "type": "bibr",
                    "ref_id": "b103",
                    "text": "(114,"
                },
                {
                    "start": 131,
                    "end": 135,
                    "type": "bibr",
                    "ref_id": "b104",
                    "text": "115)"
                }
            ]
        },
        {
            "text": "An exciting direction of this research is to automatically discover what sequence of events in the simulation of a background process leads to rare events being misclassified as signal.",
            "section": "PROSPECTS",
            "paragraph_rank": 77,
            "section_rank": 18
        },
        {
            "text": "Deep learning has already influenced data analysis at the LHC and sparked a new wave of collaboration between the machine learning and particle physics communities, which is progressing at a rapid pace. ",
            "section": "PROSPECTS",
            "paragraph_rank": 78,
            "section_rank": 18
        },
        {
            "text": "While it is difficult to predict the ultimate impact these developments will have, we anticipate that new applications will be found, motivating new strategies for analysis of the LHC data and yielding deeper insights into fundamental questions in particle physics.",
            "section": "PROSPECTS",
            "paragraph_rank": 78,
            "section_rank": 18
        },
        {
            "text": "DISCLOSURE STATEMENT",
            "section_rank": 19
        },
        {
            "text": "The authors are not aware of any affiliations, memberships, funding, or financial holdings that might be perceived as affecting the objectivity of this review.",
            "section": "DISCLOSURE STATEMENT",
            "paragraph_rank": 79,
            "section_rank": 19
        },
        {
            "text": "Figure 1",
            "section_rank": 20
        },
        {
            "text": "Figure 1",
            "section": "Figure 1",
            "paragraph_rank": 80,
            "section_rank": 20
        },
        {
            "text": "Figure 2",
            "paragraph_rank": 81,
            "section_rank": 21
        },
        {
            "text": "Figure 3",
            "section_rank": 22
        },
        {
            "text": "Figure 3",
            "section": "Figure 3",
            "paragraph_rank": 82,
            "section_rank": 22
        },
        {
            "text": "Figure 4",
            "paragraph_rank": 83,
            "section_rank": 23
        },
        {
            "text": "(",
            "section_rank": 24
        },
        {
            "text": "Figure 5",
            "section": "(",
            "paragraph_rank": 84,
            "section_rank": 24
        },
        {
            "text": "www.annualreviews.org \u2022 Deep Learning and Its Application to LHC Physics",
            "section": "(",
            "paragraph_rank": 85,
            "section_rank": 24,
            "entity_spans": [
                {
                    "start": 24,
                    "end": 38,
                    "type": "software",
                    "rawForm": "Deep Learning",
                    "resp": "service",
                    "id": "software-simple-s22"
                }
            ]
        },
        {
            "text": "In the language of statistics and machine learning, the full simulation chain would be considered a generative model for the data as they can be used to generate synthetic data of the same complexity and format as the actual collision data.",
            "section": "(",
            "paragraph_rank": 86,
            "section_rank": 24
        },
        {
            "text": "Other machine learning paradigms like unsupervised, semi-supervised, and weakly supervised learning relax or remove the need for labels in the training data.www.annualreviews.org \u2022 Deep Learning and Its Application to LHC Physics",
            "section": "(",
            "paragraph_rank": 87,
            "section_rank": 24,
            "entity_spans": [
                {
                    "start": 181,
                    "end": 195,
                    "type": "software",
                    "rawForm": "Deep Learning",
                    "resp": "service",
                    "id": "software-simple-s23"
                }
            ]
        },
        {
            "text": "ACKNOWLEDGMENTS",
            "section_rank": 26
        },
        {
            "text": "The authors thank Ben Nachman for helpful comments. ",
            "section": "ACKNOWLEDGMENTS",
            "paragraph_rank": 88,
            "section_rank": 26
        },
        {
            "text": "DW and DG are supported by the Office of Science at the Department of Energy. ",
            "section": "ACKNOWLEDGMENTS",
            "paragraph_rank": 88,
            "section_rank": 26
        },
        {
            "text": "KC is supported by the National Science Foundation (ACI-1450310 and PHY-1505463) and by the Moore-Sloan Data Science Environ-ment at at NYU.",
            "section": "ACKNOWLEDGMENTS",
            "paragraph_rank": 88,
            "section_rank": 26
        },
        {
            "text": "LITERATURE CITED",
            "section_rank": 27
        }
    ]
}