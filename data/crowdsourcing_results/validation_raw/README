We launched 200 article chunks sampled from all the PMC and econ articles our team annotated before (when creating the gold standard dataset) on MTurk for validating crowd annotation.

Crowd annotation identified 39 negative article chunks (two crowd workers agreed on) from the 200 article chunks, see Pipeline-1E-SoftciteI_fromJSON-MTurk.IAA-2020-10-30T1450-NegativeTasks.csv.

Crowd workers also highlighted 423 software mentions (without de-duplication) in 161 article chunks, see Pipeline-1E-SoftciteI_fromJSON-MTurk.IAA-2020-10-30T1450-Tags.csv. 

200 software mentions from the initial 423 highlights have been double-checked and further annotated with findings of software attributes mentioned in text. See Pipeline-2E-Details-Validation-MTurk-2020-12-17T1543-DataHunt.csv.

