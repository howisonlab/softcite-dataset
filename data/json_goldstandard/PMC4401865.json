{
    "id": "a489a5c0cc",
    "level": "sentence",
    "abstract": [
        {
            "text": "Objective: Microarray-related studies often involve a very large number of genes and small sample size. ",
            "paragraph_rank": 2,
            "section_rank": 0
        },
        {
            "text": "Cross-validating or bootstrapping is therefore imperative to obtain a fair assessment of the prediction/classification performance of a gene signature. ",
            "paragraph_rank": 2,
            "section_rank": 0
        },
        {
            "text": "A deficiency of these methods is the reduced training sample size because of the partition process in cross-validation and sampling with replacement in bootstrapping. ",
            "paragraph_rank": 2,
            "section_rank": 0
        },
        {
            "text": "To address this problem, we aim to obtain a prediction performance estimate that strikes a good balance between bias and variance and has a small root mean squared error. ",
            "paragraph_rank": 2,
            "section_rank": 0
        },
        {
            "text": "Methods: We propose to make a one-step extrapolation from the fitted learning curve to estimate the prediction/classification performance of the model trained by all the samples. ",
            "paragraph_rank": 2,
            "section_rank": 0
        },
        {
            "text": "Results: Simulation studies show that the method strikes a good balance between bias and variance and has a small root mean squared error. ",
            "paragraph_rank": 2,
            "section_rank": 0
        },
        {
            "text": "Three microarray data sets are used for demonstration. ",
            "paragraph_rank": 2,
            "section_rank": 0
        },
        {
            "text": "Conclusions: Our method is advocated to estimate the prediction performance of a gene signature derived from a small study. ",
            "paragraph_rank": 2,
            "section_rank": 0
        },
        {
            "text": "Open Access Research Contributors L-YW designed the simulation study and drafted the manuscript. ",
            "paragraph_rank": 2,
            "section_rank": 0
        },
        {
            "text": "W-CL conceived the study and participated in its design and coordination.",
            "paragraph_rank": 2,
            "section_rank": 0
        }
    ],
    "body_text": [
        {
            "text": "INTRODUCTION",
            "section_rank": 1
        },
        {
            "text": "With the advances in microarray technology, hundreds of thousands of genes with expression information on an individual can be obtained in a single experiment. ",
            "section": "INTRODUCTION",
            "paragraph_rank": 3,
            "section_rank": 1
        },
        {
            "text": "This high throughput technology enables us to make diagnostic and prognostic predictions based on a participant's gene signature. ",
            "section": "INTRODUCTION",
            "paragraph_rank": 3,
            "section_rank": 1
        },
        {
            "text": "[1][2][3][4][5][6][7][8][9][10][11][12][13][14] There are four key steps in microarray-based studies: (1) data processing (eg, data normalisation), (2) gene selection, (3) prediction model construction and (4) prediction performance evaluation. ",
            "section": "INTRODUCTION",
            "paragraph_rank": 3,
            "section_rank": 1,
            "ref_spans": [
                {
                    "start": 0,
                    "end": 3,
                    "type": "bibr",
                    "ref_id": "b0",
                    "text": "[1]"
                },
                {
                    "start": 3,
                    "end": 6,
                    "type": "bibr",
                    "ref_id": "b1",
                    "text": "[2]"
                },
                {
                    "start": 6,
                    "end": 9,
                    "type": "bibr",
                    "ref_id": "b2",
                    "text": "[3]"
                },
                {
                    "start": 9,
                    "end": 12,
                    "type": "bibr",
                    "ref_id": "b3",
                    "text": "[4]"
                },
                {
                    "start": 12,
                    "end": 15,
                    "type": "bibr",
                    "ref_id": "b4",
                    "text": "[5]"
                },
                {
                    "start": 15,
                    "end": 18,
                    "type": "bibr",
                    "ref_id": "b5",
                    "text": "[6]"
                },
                {
                    "start": 18,
                    "end": 21,
                    "type": "bibr",
                    "ref_id": "b6",
                    "text": "[7]"
                },
                {
                    "start": 21,
                    "end": 24,
                    "type": "bibr",
                    "ref_id": "b7",
                    "text": "[8]"
                },
                {
                    "start": 24,
                    "end": 27,
                    "type": "bibr",
                    "ref_id": "b8",
                    "text": "[9]"
                },
                {
                    "start": 27,
                    "end": 31,
                    "type": "bibr",
                    "ref_id": "b9",
                    "text": "[10]"
                },
                {
                    "start": 31,
                    "end": 35,
                    "type": "bibr",
                    "ref_id": "b10",
                    "text": "[11]"
                },
                {
                    "start": 35,
                    "end": 39,
                    "type": "bibr",
                    "ref_id": "b11",
                    "text": "[12]"
                },
                {
                    "start": 39,
                    "end": 43,
                    "type": "bibr",
                    "ref_id": "b12",
                    "text": "[13]"
                },
                {
                    "start": 43,
                    "end": 47,
                    "type": "bibr",
                    "ref_id": "b13",
                    "text": "[14]"
                }
            ]
        },
        {
            "text": "15 This paper focuses on the last step, the evaluation of prediction performances.",
            "section": "INTRODUCTION",
            "paragraph_rank": 3,
            "section_rank": 1,
            "ref_spans": [
                {
                    "start": 0,
                    "end": 2,
                    "type": "bibr",
                    "ref_id": "b14",
                    "text": "15"
                }
            ]
        },
        {
            "text": "Microarray-based studies often involve a very large number of genes and a relatively small sample size. ",
            "section": "INTRODUCTION",
            "paragraph_rank": 4,
            "section_rank": 1
        },
        {
            "text": "The same small data set being used for constructing the prediction model and subsequently evaluating the prediction performance tends to give over-optimistic estimates. ",
            "section": "INTRODUCTION",
            "paragraph_rank": 4,
            "section_rank": 1
        },
        {
            "text": "This is why cross-validating or bootstrapping is imperative if a fair assessment of the prediction/classification performance of a gene signature is to be made. ",
            "section": "INTRODUCTION",
            "paragraph_rank": 4,
            "section_rank": 1
        },
        {
            "text": "Popular crossvalidation (CV) methods are k-fold CV, Monte Carlo CV and leave-one-out CV (LOOCV, also known as jackknifing). ",
            "section": "INTRODUCTION",
            "paragraph_rank": 4,
            "section_rank": 1
        },
        {
            "text": "16 These methods partition the original data into a training set and a testing set. ",
            "section": "INTRODUCTION",
            "paragraph_rank": 4,
            "section_rank": 1,
            "ref_spans": [
                {
                    "start": 0,
                    "end": 2,
                    "type": "bibr",
                    "ref_id": "b15",
                    "text": "16"
                }
            ]
        },
        {
            "text": "The training sample size, therefore, is reduced. ",
            "section": "INTRODUCTION",
            "paragraph_rank": 4,
            "section_rank": 1
        },
        {
            "text": "The bootstrap method is an alternative to CV that operates by sampling with replacement of the original data. ",
            "section": "INTRODUCTION",
            "paragraph_rank": 4,
            "section_rank": 1
        },
        {
            "text": "17 18 Even though the bootstrap sample has the same sample size as the original one, the overlapping of subjects between the bootstrap sample and the original data still reduces the effective (non-overlapping) training sample size. ",
            "section": "INTRODUCTION",
            "paragraph_rank": 4,
            "section_rank": 1,
            "ref_spans": [
                {
                    "start": 0,
                    "end": 5,
                    "type": "bibr",
                    "text": "17 18"
                }
            ]
        },
        {
            "text": "The reduced training sample size will curtail the prediction/classification performance of a gene signature, especially when the sample size of a study is already small. ",
            "section": "INTRODUCTION",
            "paragraph_rank": 4,
            "section_rank": 1
        },
        {
            "text": "19 Estimating the performance of a prediction model built from a small study is a vexing task. ",
            "section": "INTRODUCTION",
            "paragraph_rank": 4,
            "section_rank": 1,
            "ref_spans": [
                {
                    "start": 0,
                    "end": 2,
                    "type": "bibr",
                    "ref_id": "b18",
                    "text": "19"
                }
            ]
        },
        {
            "text": "For CV purposes, the already small sample size still needs to be partitioned further into a training set and a testing one. ",
            "section": "INTRODUCTION",
            "paragraph_rank": 4,
            "section_rank": 1
        },
        {
            "text": "If we make the training size as large as possible (such as LOOCV), it would be nearly unbiased, but the effective sample size left for validation is one subject and we would get a variance that is unduly large, that is, the notorious bias-variance dilemma. ",
            "section": "INTRODUCTION",
            "paragraph_rank": 4,
            "section_rank": 1
        },
        {
            "text": "20 21 Strengths and limitations of this study \u25aa The proposed method estimates the prediction performance of a gene signature derived from a small study. ",
            "section": "INTRODUCTION",
            "paragraph_rank": 4,
            "section_rank": 1,
            "ref_spans": [
                {
                    "start": 0,
                    "end": 5,
                    "type": "bibr",
                    "text": "20 21"
                }
            ]
        },
        {
            "text": "\u25aa The proposed method strikes a good balance between bias and variance and has a small root mean squared error. ",
            "section": "INTRODUCTION",
            "paragraph_rank": 4,
            "section_rank": 1
        },
        {
            "text": "\u25aa The proposed method can be applied to linear and non-linear prediction models. ",
            "section": "INTRODUCTION",
            "paragraph_rank": 4,
            "section_rank": 1
        },
        {
            "text": "\u25aa The proposed method may not work well for studies with an extremely small sample size (eg, n<10).",
            "section": "INTRODUCTION",
            "paragraph_rank": 4,
            "section_rank": 1
        },
        {
            "text": "The accuracy rate, the error rate and the area under the receiver operating characteristic curve (AUC) are commonly used performance indicators of a prediction model for a binary outcome. ",
            "section": "INTRODUCTION",
            "paragraph_rank": 5,
            "section_rank": 1
        },
        {
            "text": "15 22 In this paper, we focus on the AUC index because it evaluates the global performance of a prediction model, not just at a particular cut-off point, but for each and every possible cut-off point. ",
            "section": "INTRODUCTION",
            "paragraph_rank": 5,
            "section_rank": 1,
            "ref_spans": [
                {
                    "start": 0,
                    "end": 5,
                    "type": "bibr",
                    "text": "15 22"
                }
            ]
        },
        {
            "text": "In a small data set, each and every subject is indispensable. ",
            "section": "INTRODUCTION",
            "paragraph_rank": 5,
            "section_rank": 1
        },
        {
            "text": "We propose to make a one-step extrapolation from the fitted learning curve for AUC to estimate the prediction/classification performance of the model trained by all the samples.",
            "section": "INTRODUCTION",
            "paragraph_rank": 5,
            "section_rank": 1
        },
        {
            "text": "METHODS",
            "section_rank": 2
        },
        {
            "text": "Monte Carlo CVs",
            "section_rank": 3
        },
        {
            "text": "Suppose that a given prediction model is to be evaluated based on a data set with a total of N 1 cases (or individuals with adverse events) and N 0 controls (or individuals without adverse events). ",
            "section": "Monte Carlo CVs",
            "paragraph_rank": 6,
            "section_rank": 3
        },
        {
            "text": "First, we use CVs with five different folds (LOOCV, 10-fold, 5-fold, 3-fold and 2-fold CV, respectively) to evaluate the performances of the prediction model. ",
            "section": "Monte Carlo CVs",
            "paragraph_rank": 6,
            "section_rank": 3
        },
        {
            "text": "(LOOCV is the largest training size possible and the 2-fold CV is the smallest. Between these two extremes, we add three additional fold numbers. More folds can also be tried, but the results are similar.) ",
            "section": "Monte Carlo CVs",
            "paragraph_rank": 6,
            "section_rank": 3
        },
        {
            "text": "To be more precise, we use the Monte Carlo random partition to partition the data set into two parts: the training set and the testing set. ",
            "section": "Monte Carlo CVs",
            "paragraph_rank": 6,
            "section_rank": 3
        },
        {
            "text": "The random partitions are operated separately for the case group and the control group to ensure that the number of cases and controls is as balanced as possible; 23 24 the training set has a total of n 1 \u00bc N 1 \u00c0 \u00f0N 1 =k\u00de cases and n 0 \u00bc N 0 \u00c0 \u00f0N 0 =k\u00de controls (both to the nearest integers), where k \u00bc N 1 \u00f0N 0 \u00de; 10; 5; 3; 2 for LOOCV, 10-fold, 5-fold, 3-fold and 2-fold CVs, respectively.",
            "section": "Monte Carlo CVs",
            "paragraph_rank": 6,
            "section_rank": 3,
            "ref_spans": [
                {
                    "start": 163,
                    "end": 168,
                    "type": "bibr",
                    "text": "23 24"
                }
            ]
        },
        {
            "text": "To be representative of all possible partitions, we suggest running at least 100 Monte Carlo random partitions for all folds, although this may be superfluous for some situations. ",
            "section": "Monte Carlo CVs",
            "paragraph_rank": 7,
            "section_rank": 3
        },
        {
            "text": "(For example, there are only 8 \u00c2 8 \u00bc 64 distinctive partitions for LOOCV with N 1 \u00bc N 0 \u00bc 8. Note that in a random partition, we leave out 'one pair' of a case and a control, instead of 'one subject'. 24 ) ",
            "section": "Monte Carlo CVs",
            "paragraph_rank": 7,
            "section_rank": 3,
            "ref_spans": [
                {
                    "start": 201,
                    "end": 203,
                    "type": "bibr",
                    "ref_id": "b23",
                    "text": "24"
                }
            ]
        },
        {
            "text": "For each Monte Carlo partition, a prediction model will be built from the training set, and the testing set used to evaluate the performance of this model. ",
            "section": "Monte Carlo CVs",
            "paragraph_rank": 7,
            "section_rank": 3
        },
        {
            "text": "The AUCs under the same fold are to be averaged (denoted as AUC), which leaves us with a total of five AUCs.",
            "section": "Monte Carlo CVs",
            "paragraph_rank": 7,
            "section_rank": 3
        },
        {
            "text": "Learning curve for AUC A learning curve is an assumed functional relation between prediction performance and training sample size. ",
            "section": "Monte Carlo CVs",
            "paragraph_rank": 8,
            "section_rank": 3
        },
        {
            "text": "25 In this study, we take y \u00bc a \u00fe bx as our learning curve, where y \u00bc Z \u00c02",
            "section": "Monte Carlo CVs",
            "paragraph_rank": 8,
            "section_rank": 3,
            "ref_spans": [
                {
                    "start": 0,
                    "end": 2,
                    "type": "bibr",
                    "ref_id": "b24",
                    "text": "25"
                }
            ]
        },
        {
            "text": "AUC and x \u00bc n \u00c01 1 \u00fe n \u00c01 0 . ",
            "section": "Monte Carlo CVs",
            "paragraph_rank": 9,
            "section_rank": 3
        },
        {
            "text": "Essentially, this learning curve is a straight line in a double-inverse coordinate. ",
            "section": "Monte Carlo CVs",
            "paragraph_rank": 9,
            "section_rank": 3
        },
        {
            "text": "For the ordinate, the AUC values are first transformed to quantiles of standard normal distribution, then squared and finally inversed. ",
            "section": "Monte Carlo CVs",
            "paragraph_rank": 9,
            "section_rank": 3
        },
        {
            "text": "This expands the range of 0.5-1.0 in an AUC to a range of 0 \u00c0 1 in the ordinate. ",
            "section": "Monte Carlo CVs",
            "paragraph_rank": 9,
            "section_rank": 3
        },
        {
            "text": "For the abscissa, the range of the sample size is also between 0 and \u221e. ",
            "section": "Monte Carlo CVs",
            "paragraph_rank": 9,
            "section_rank": 3
        },
        {
            "text": "Such a sample size in inverse should be more sensitive to changes when the original sample size is small. ",
            "section": "Monte Carlo CVs",
            "paragraph_rank": 9,
            "section_rank": 3
        },
        {
            "text": "The online supplementary appendix 1 shows that this learning curve is the exact functional relation between prediction performance and training sample size when normality and independence of the data are assumed.",
            "section": "Monte Carlo CVs",
            "paragraph_rank": 9,
            "section_rank": 3
        },
        {
            "text": "One-step extrapolation from the learning curve We calculate y \u00bc Z \u00c02",
            "section": "Monte Carlo CVs",
            "paragraph_rank": 10,
            "section_rank": 3
        },
        {
            "text": "AUC and x \u00bc n \u00c01 1 \u00fe n \u00c01 0 for each fold. ",
            "section": "Monte Carlo CVs",
            "paragraph_rank": 11,
            "section_rank": 3
        },
        {
            "text": "On the basis of the five coordinate points, (x, y)s, we draw a linear regression line, that is, y \u00bc a \u00fe bx: To extrapolate the performance (denoted as AUC T ) of the prediction model when all the subjects",
            "section": "Monte Carlo CVs",
            "paragraph_rank": 11,
            "section_rank": 3
        },
        {
            "text": "where F\u00f0\u00c1\u00de is the cumulative distribution for the standard normal.",
            "section": "Monte Carlo CVs",
            "paragraph_rank": 12,
            "section_rank": 3
        },
        {
            "text": "SIMULATION STUDIES Data and prediction models",
            "section_rank": 4
        },
        {
            "text": "We consider four different sample sizes: N T \u00bc 10 (cases) +10 (controls), 15+15, 20+20 and 25+25, respectively. ",
            "section": "SIMULATION STUDIES Data and prediction models",
            "paragraph_rank": 13,
            "section_rank": 4
        },
        {
            "text": "A total of 10 genes are considered. ",
            "section": "SIMULATION STUDIES Data and prediction models",
            "paragraph_rank": 13,
            "section_rank": 4
        },
        {
            "text": "The gene expression levels are generated from a normal distribution with a variance of 1. ",
            "section": "SIMULATION STUDIES Data and prediction models",
            "paragraph_rank": 13,
            "section_rank": 4
        },
        {
            "text": "For the cases, the means of the gene expressions are distributed as uniform (\u22120.8, 0.8); for the controls, the means are set to 0.",
            "section": "SIMULATION STUDIES Data and prediction models",
            "paragraph_rank": 13,
            "section_rank": 4
        },
        {
            "text": "Four different data structures are considered. ",
            "section": "SIMULATION STUDIES Data and prediction models",
            "paragraph_rank": 14,
            "section_rank": 4
        },
        {
            "text": "The first three are normally distributed with a correlation coefficient of 0 (independence), 0.2 and 0.5 (dependency). ",
            "section": "SIMULATION STUDIES Data and prediction models",
            "paragraph_rank": 14,
            "section_rank": 4
        },
        {
            "text": "The last data structure is more complicated. ",
            "section": "SIMULATION STUDIES Data and prediction models",
            "paragraph_rank": 14,
            "section_rank": 4
        },
        {
            "text": "For the cases, the expression level of each gene is distributed as a mixture of three normal distributions with variances of 1. ",
            "section": "SIMULATION STUDIES Data and prediction models",
            "paragraph_rank": 14,
            "section_rank": 4
        },
        {
            "text": "The three means are generated from a uniform (\u22121.5, 1.5) distribution with a probability of 0.6, a uniform (\u22121.2, 1.2) distribution with a probability of 0.3 and a uniform (\u22121, 1) distribution with a probability of 0.1, respectively (see online supplementary appendix 2 for this non-normal distribution). ",
            "section": "SIMULATION STUDIES Data and prediction models",
            "paragraph_rank": 14,
            "section_rank": 4
        },
        {
            "text": "The gene expression level for the controls follows the standard normal distribution. ",
            "section": "SIMULATION STUDIES Data and prediction models",
            "paragraph_rank": 14,
            "section_rank": 4
        },
        {
            "text": "The correlation coefficient between any two genes is set at 0.5 in these complex data.",
            "section": "SIMULATION STUDIES Data and prediction models",
            "paragraph_rank": 14,
            "section_rank": 4
        },
        {
            "text": "There are many methods to build prediction models. ",
            "section": "SIMULATION STUDIES Data and prediction models",
            "paragraph_rank": 15,
            "section_rank": 4
        },
        {
            "text": "In this paper, we use the na\u00efve multiple regression and the support vector machine (SVM), as detailed below, to build prediction models. ",
            "section": "SIMULATION STUDIES Data and prediction models",
            "paragraph_rank": 15,
            "section_rank": 4
        },
        {
            "text": "Another machine learning method, the random forest (RF), is detailed in online supplementary materials.",
            "section": "SIMULATION STUDIES Data and prediction models",
            "paragraph_rank": 15,
            "section_rank": 4
        },
        {
            "text": "The na\u00efve multiple regression is a simple prediction method. ",
            "section": "SIMULATION STUDIES Data and prediction models",
            "paragraph_rank": 16,
            "section_rank": 4
        },
        {
            "text": "First, the \u03b2-coefficients ( b b i ; i \u00bc 1; :::; p where p is the number of genes in the gene signature) are calculated as the mean expression difference for each gene between the case and the control groups in the training set. ",
            "section": "SIMULATION STUDIES Data and prediction models",
            "paragraph_rank": 16,
            "section_rank": 4
        },
        {
            "text": "The prediction score of the na\u00efve multiple regression for the jth subject in the testing set is then S p i\u00bc1 b b i x ij , where \u03c7 ij is the observed gene expression level of the ith gene for this jth subject. ",
            "section": "SIMULATION STUDIES Data and prediction models",
            "paragraph_rank": 16,
            "section_rank": 4
        },
        {
            "text": "(The na\u00efve multiple regression used in this study is similar to a previously proposed compound covariate method 26 where the prediction score for the jth subject in the testing set is S p i\u00bc1 t i x ij with the two-sample t-statistic of each gene serving its own weight in the prediction model).",
            "section": "SIMULATION STUDIES Data and prediction models",
            "paragraph_rank": 16,
            "section_rank": 4,
            "ref_spans": [
                {
                    "start": 112,
                    "end": 114,
                    "type": "bibr",
                    "ref_id": "b25",
                    "text": "26"
                }
            ]
        },
        {
            "text": "SVM is a more sophisticated method; it is a very efficient learning algorithm for high-dimensional data in classification, regression and pattern recognition. ",
            "section": "SIMULATION STUDIES Data and prediction models",
            "paragraph_rank": 17,
            "section_rank": 4
        },
        {
            "text": "The basis of SVM is to implicitly map data to a higher dimensional space via a kernel function in order to identify an optimal hyperplane that maximises the margin between the two groups. ",
            "section": "SIMULATION STUDIES Data and prediction models",
            "paragraph_rank": 17,
            "section_rank": 4
        },
        {
            "text": "27 There are many software packages available to implement SVM. ",
            "section": "SIMULATION STUDIES Data and prediction models",
            "paragraph_rank": 17,
            "section_rank": 4,
            "ref_spans": [
                {
                    "start": 0,
                    "end": 2,
                    "type": "bibr",
                    "text": "27"
                }
            ]
        },
        {
            "text": "In this study, we use thee1071-package of R with a default radial basis function kernel to obtain the prediction scores. ",
            "section": "SIMULATION STUDIES Data and prediction models",
            "paragraph_rank": 17,
            "section_rank": 4,
            "entity_spans": [
                {
                    "start": 25,
                    "end": 43,
                    "type": "software",
                    "rawForm": "e1071-package of R",
                    "resp": "#annotator18",
                    "used": true,
                    "id": "a489a5c0cc-software-simple-0",
                    "cert": "0.9"
                }
            ]
        },
        {
            "text": "28 CVs of the prediction models In our simulation study, we perform a total of 5000 simulations. ",
            "section": "SIMULATION STUDIES Data and prediction models",
            "paragraph_rank": 17,
            "section_rank": 4,
            "ref_spans": [
                {
                    "start": 0,
                    "end": 2,
                    "type": "bibr",
                    "text": "28"
                }
            ]
        },
        {
            "text": "In each simulation, a total of 100 random partitions are performed for each fold CV (LOOCV, 10-fold, 5-fold, 3-fold and 2-fold CVs, respectively). ",
            "section": "SIMULATION STUDIES Data and prediction models",
            "paragraph_rank": 17,
            "section_rank": 4
        },
        {
            "text": "From these, we use the previously described learning curve to make a one-step extrapolation to the cross-validated AUC when all the samples are utilised to train the model. ",
            "section": "SIMULATION STUDIES Data and prediction models",
            "paragraph_rank": 17,
            "section_rank": 4
        },
        {
            "text": "For a comparison, we also calculate the internally validated AUCs of the LOO bootstrap in each simulation. ",
            "section": "SIMULATION STUDIES Data and prediction models",
            "paragraph_rank": 17,
            "section_rank": 4
        },
        {
            "text": "This is a modified bootstrap procedure of the ordinary bootstrap. ",
            "section": "SIMULATION STUDIES Data and prediction models",
            "paragraph_rank": 17,
            "section_rank": 4
        },
        {
            "text": "We draw a total of 100 resamplings. ",
            "section": "SIMULATION STUDIES Data and prediction models",
            "paragraph_rank": 17,
            "section_rank": 4
        },
        {
            "text": "At each draw, the observations left out serve as the testing set. ",
            "section": "SIMULATION STUDIES Data and prediction models",
            "paragraph_rank": 17,
            "section_rank": 4
        },
        {
            "text": "The effective (non-overlapping) training sample size of the LOO bootstrap is around 63.2% of the total sample size. ",
            "section": "SIMULATION STUDIES Data and prediction models",
            "paragraph_rank": 17,
            "section_rank": 4
        },
        {
            "text": "17 18 (Out-of-bag (OOB) 29 estimation employs a majority vote on the multiple prediction made for observation i based on the bootstrap samples at each draw, while the LOO bootstrap takes an average on error of these predictions. ",
            "section": "SIMULATION STUDIES Data and prediction models",
            "paragraph_rank": 17,
            "section_rank": 4,
            "ref_spans": [
                {
                    "start": 0,
                    "end": 5,
                    "type": "bibr",
                    "text": "17 18"
                },
                {
                    "start": 24,
                    "end": 26,
                    "type": "bibr",
                    "text": "29"
                }
            ]
        },
        {
            "text": "Therefore, OOB estimation may have larger variability than the LOO bootstrap when the sample size is small. ",
            "section": "SIMULATION STUDIES Data and prediction models",
            "paragraph_rank": 17,
            "section_rank": 4
        },
        {
            "text": "30) In the simulation, we additionally create a large data set of 1000 cases and 1000 controls for external validation. ",
            "section": "SIMULATION STUDIES Data and prediction models",
            "paragraph_rank": 17,
            "section_rank": 4,
            "ref_spans": [
                {
                    "start": 0,
                    "end": 2,
                    "type": "bibr",
                    "text": "30"
                }
            ]
        },
        {
            "text": "For a prediction model, an externally validated AUC against this data set is considered as its true AUC value (one true AUC for each round of simulation). ",
            "section": "SIMULATION STUDIES Data and prediction models",
            "paragraph_rank": 17,
            "section_rank": 4
        },
        {
            "text": "It should be pointed out that in real practice, one rarely has the luxury to conduct such a large-scale external validation, but will often have to settle for a satisfactory internal validation method which is precisely the focal point of this paper.",
            "section": "SIMULATION STUDIES Data and prediction models",
            "paragraph_rank": 17,
            "section_rank": 4
        },
        {
            "text": "CVs of the prediction models",
            "section_rank": 5
        },
        {
            "text": "In our simulation study, we perform a total of 5000 simulations. ",
            "section": "CVs of the prediction models",
            "paragraph_rank": 18,
            "section_rank": 5
        },
        {
            "text": "In each simulation, a total of 100 random partitions are performed for each fold CV (LOOCV, 10-fold, 5-fold, 3-fold and 2-fold CVs, respectively). ",
            "section": "CVs of the prediction models",
            "paragraph_rank": 18,
            "section_rank": 5
        },
        {
            "text": "From these, we use the previously described learning curve to make a one-step extrapolation to the cross-validated AUC when all the samples are utilised to train the model. ",
            "section": "CVs of the prediction models",
            "paragraph_rank": 18,
            "section_rank": 5
        },
        {
            "text": "For a comparison, we also calculate the internally validated AUCs of the LOO bootstrap in each simulation. ",
            "section": "CVs of the prediction models",
            "paragraph_rank": 18,
            "section_rank": 5
        },
        {
            "text": "This is a modified bootstrap procedure of the ordinary bootstrap. ",
            "section": "CVs of the prediction models",
            "paragraph_rank": 18,
            "section_rank": 5
        },
        {
            "text": "We draw a total of 100 resamplings. ",
            "section": "CVs of the prediction models",
            "paragraph_rank": 18,
            "section_rank": 5
        },
        {
            "text": "At each draw, the observations left out serve as the testing set. ",
            "section": "CVs of the prediction models",
            "paragraph_rank": 18,
            "section_rank": 5
        },
        {
            "text": "The effective (non-overlapping) training sample size of the LOO bootstrap is around 63.2% of the total sample size. ",
            "section": "CVs of the prediction models",
            "paragraph_rank": 18,
            "section_rank": 5
        },
        {
            "text": "17 18 (Out-of-bag (OOB) 29 estimation employs a majority vote on the multiple prediction made for observation i based on the bootstrap samples at each draw, while the LOO bootstrap takes an average on error of these predictions. ",
            "section": "CVs of the prediction models",
            "paragraph_rank": 18,
            "section_rank": 5,
            "ref_spans": [
                {
                    "start": 0,
                    "end": 5,
                    "type": "bibr",
                    "text": "17 18"
                },
                {
                    "start": 24,
                    "end": 26,
                    "type": "bibr",
                    "ref_id": "b28",
                    "text": "29"
                }
            ]
        },
        {
            "text": "Therefore, OOB estimation may have larger variability than the LOO bootstrap when the sample size is small. ",
            "section": "CVs of the prediction models",
            "paragraph_rank": 18,
            "section_rank": 5
        },
        {
            "text": "30 ) In the simulation, we additionally create a large data set of 1000 cases and 1000 controls for external validation. ",
            "section": "CVs of the prediction models",
            "paragraph_rank": 18,
            "section_rank": 5,
            "ref_spans": [
                {
                    "start": 0,
                    "end": 2,
                    "type": "bibr",
                    "ref_id": "b29",
                    "text": "30"
                }
            ]
        },
        {
            "text": "For a prediction model, an externally validated AUC against this data set is considered as its true AUC value (one true AUC for each round of simulation). ",
            "section": "CVs of the prediction models",
            "paragraph_rank": 18,
            "section_rank": 5
        },
        {
            "text": "It should be pointed out that in real practice, one rarely has the luxury to conduct such a large-scale external validation, but will often have to settle for a satisfactory internal validation method which is precisely the focal point of this paper.",
            "section": "CVs of the prediction models",
            "paragraph_rank": 18,
            "section_rank": 5
        },
        {
            "text": "Bias, variance and root mean squared error",
            "section_rank": 6
        },
        {
            "text": "In each round of the simulation, we calculate an estimated AUC, an error (the difference between the estimated AUC and the true AUC) and an error square for each performance evaluation method. ",
            "section": "Bias, variance and root mean squared error",
            "paragraph_rank": 19,
            "section_rank": 6
        },
        {
            "text": "On the basis of the 5000 simulations, the bias is calculated as the sample mean of the errors; the variance is the sample variance of the estimated AUCs; and the mean squared error (MSE) is the sample mean of the error squares. ",
            "section": "Bias, variance and root mean squared error",
            "paragraph_rank": 19,
            "section_rank": 6
        },
        {
            "text": "Finally, the root mean squared error (RMSE) is calculated from the square root of MSE. ",
            "section": "Bias, variance and root mean squared error",
            "paragraph_rank": 19,
            "section_rank": 6
        },
        {
            "text": "(RMSE simultaneously considers bias and variance. ",
            "section": "Bias, variance and root mean squared error",
            "paragraph_rank": 19,
            "section_rank": 6
        },
        {
            "text": "This value represents the 'average' (root-meansquare average, to be precise) difference between the estimated AUC and the true AUC).",
            "section": "Bias, variance and root mean squared error",
            "paragraph_rank": 19,
            "section_rank": 6
        },
        {
            "text": "Simulation results",
            "section_rank": 7
        },
        {
            "text": "In figure 1, we present the bias ( panels A-D), the variance ( panels E-H) and the RMSE ( panels I-L), respectively, using na\u00efve multiple regression under different sample sizes. ",
            "section": "Simulation results",
            "paragraph_rank": 20,
            "section_rank": 7,
            "ref_spans": [
                {
                    "start": 3,
                    "end": 46,
                    "type": "figure",
                    "ref_id": "fig_0",
                    "text": "figure 1, we present the bias ( panels A-D)"
                }
            ]
        },
        {
            "text": "When the variables are independently distributed ( panels A, E and I), the bias becomes smaller as the sample size becomes larger (closer to zero; panel A). ",
            "section": "Simulation results",
            "paragraph_rank": 20,
            "section_rank": 7
        },
        {
            "text": "All the fold-based CV methods underestimate the true AUC value because the training sample sizes they use are smaller than the total sample size given. ",
            "section": "Simulation results",
            "paragraph_rank": 20,
            "section_rank": 7
        },
        {
            "text": "The training sample size of LOOCV is closest to the total sample size (total sample size minus one pair); hence, it is the least biased (blue line) among all the fold-based CV methods. ",
            "section": "Simulation results",
            "paragraph_rank": 20,
            "section_rank": 7
        },
        {
            "text": "The training sample size of the LOO bootstrap is about 63% of the total sample size, 17 18 which makes its bias (black dashed line) comparable to that of the twofold CV (with 50% of the total sample size; green line). ",
            "section": "Simulation results",
            "paragraph_rank": 20,
            "section_rank": 7,
            "ref_spans": [
                {
                    "start": 85,
                    "end": 90,
                    "type": "bibr",
                    "text": "17 18"
                }
            ]
        },
        {
            "text": "As for the bias of our extrapolation method (red line), it is comparable to that of LOOCV.",
            "section": "Simulation results",
            "paragraph_rank": 20,
            "section_rank": 7
        },
        {
            "text": "In figure 1E, we see that the variance reveals a different story; the LOOCV now has the largest variance, and the twofold CV has the smallest variance among the fold-based CV methods. ",
            "section": "Simulation results",
            "paragraph_rank": 21,
            "section_rank": 7,
            "ref_spans": [
                {
                    "start": 3,
                    "end": 12,
                    "type": "figure",
                    "ref_id": "fig_0",
                    "text": "figure 1E"
                }
            ]
        },
        {
            "text": "In terms of variance, the extrapolation method is now comparable to the twofold CV. ",
            "section": "Simulation results",
            "paragraph_rank": 21,
            "section_rank": 7
        },
        {
            "text": "From the RMSE index ( figure 1I), we see that the proposed extrapolation method strikes a good balance between the bias and variance.",
            "section": "Simulation results",
            "paragraph_rank": 21,
            "section_rank": 7,
            "ref_spans": [
                {
                    "start": 22,
                    "end": 32,
                    "type": "figure",
                    "ref_id": "fig_0",
                    "text": "figure 1I)"
                }
            ]
        },
        {
            "text": "Similar results can be found when the variables are correlated ( panels B, F and J, with a correlation coefficient of 0.2; panels C, G and K, with a correlation coefficient of 0.5) and when they are not normally distributed ( panels D, H and L), or when SVM (figure 2) is used for constructing prediction models.",
            "section": "Simulation results",
            "paragraph_rank": 22,
            "section_rank": 7
        },
        {
            "text": "We simulated a more substantially non-normal data set (see online supplementary appendix 3). ",
            "section": "Simulation results",
            "paragraph_rank": 23,
            "section_rank": 7
        },
        {
            "text": "We found that the proposed extrapolation method can still strike a good balance between bias and variance (see online supplementary appendix 4). ",
            "section": "Simulation results",
            "paragraph_rank": 23,
            "section_rank": 7
        },
        {
            "text": "In addition, we examined the performances of the 0.632 bootstrap 17 and the 0.632+ bootstrap, 31 both of which are weighted averages between the LOO bootstrap estimate and the resubstitution estimate. ",
            "section": "Simulation results",
            "paragraph_rank": 23,
            "section_rank": 7,
            "ref_spans": [
                {
                    "start": 94,
                    "end": 96,
                    "type": "bibr",
                    "ref_id": "b30",
                    "text": "31"
                }
            ]
        },
        {
            "text": "(The 0.632+ bootstrap is an improved version of the 0.632 bootstrap.) ",
            "section": "Simulation results",
            "paragraph_rank": 23,
            "section_rank": 7
        },
        {
            "text": "We found that the 0.632 bootstrap produces very large upward biases while the 0.632+ bootstrap is quite comparable to our method (see online supplementary appendix 5). ",
            "section": "Simulation results",
            "paragraph_rank": 23,
            "section_rank": 7
        },
        {
            "text": "We also see that the proposed extrapolation method can outperform the 0.632+ bootstrap in terms of RMSE when sample size , \u00f010 cases \u00fe 10 controls\u00de (online supplementary appendix 6).",
            "section": "Simulation results",
            "paragraph_rank": 23,
            "section_rank": 7
        },
        {
            "text": "We also tried extrapolation based on different learning curves (a linear equation y \u00bc a \u00fe bx with y \u00bc AUC and",
            "section": "Simulation results",
            "paragraph_rank": 24,
            "section_rank": 7
        },
        {
            "text": "x \u00bc n 1 \u00fe n 0 , and a quadratic equation y \u00bc a \u00fe bx \u00fe cx 2 with y \u00bc Z \u00c02",
            "section": "Simulation results",
            "paragraph_rank": 25,
            "section_rank": 7
        },
        {
            "text": "AUC and x \u00bc n \u00c01 1 \u00fe n \u00c01 0 ), but we found the results to be no better than using the learning curve in this paper (see online supplementary appendices 7 and 8). ",
            "section": "Simulation results",
            "paragraph_rank": 26,
            "section_rank": 7
        },
        {
            "text": "REAL DATA DEMONSTRATION",
            "section_rank": 8
        },
        {
            "text": "We take three microarray data sets to demonstrate how the extrapolation method can be applied step by step. ",
            "section": "REAL DATA DEMONSTRATION",
            "paragraph_rank": 27,
            "section_rank": 8
        },
        {
            "text": "[32][33][34] As this paper focuses on prediction model evaluation, and not on gene selection, we conveniently construct a 10-gene signature based on the top 10 genes with the smallest Mann-Whitney U test p values for the first two data sets, respectively. ",
            "section": "REAL DATA DEMONSTRATION",
            "paragraph_rank": 27,
            "section_rank": 8,
            "ref_spans": [
                {
                    "start": 0,
                    "end": 4,
                    "type": "bibr",
                    "ref_id": "b31",
                    "text": "[32]"
                },
                {
                    "start": 4,
                    "end": 8,
                    "type": "bibr",
                    "ref_id": "b32",
                    "text": "[33]"
                },
                {
                    "start": 8,
                    "end": 12,
                    "type": "bibr",
                    "ref_id": "b33",
                    "text": "[34]"
                }
            ]
        },
        {
            "text": "In the last example, we directly take the 76-gene signature identified by the original study as the prediction model. ",
            "section": "REAL DATA DEMONSTRATION",
            "paragraph_rank": 27,
            "section_rank": 8
        },
        {
            "text": "Both na\u00efve multiple regression and SVM are used to build the prediction model for each data set. ",
            "section": "REAL DATA DEMONSTRATION",
            "paragraph_rank": 27,
            "section_rank": 8
        },
        {
            "text": "Monte Carlo random partition (a total of 1000 partitions for each fold) is performed to obtain the cross-validated AUCs.",
            "section": "REAL DATA DEMONSTRATION",
            "paragraph_rank": 27,
            "section_rank": 8
        },
        {
            "text": "Example 1",
            "section_rank": 9
        },
        {
            "text": "The first data set is colon cancer data. ",
            "section": "Example 1",
            "paragraph_rank": 28,
            "section_rank": 9
        },
        {
            "text": "32 The data consist of 2000 gene expressions in 62 tissue samples (40 tumour and 22 normal colon tissue samples). ",
            "section": "Example 1",
            "paragraph_rank": 28,
            "section_rank": 9,
            "ref_spans": [
                {
                    "start": 0,
                    "end": 2,
                    "type": "bibr",
                    "ref_id": "b31",
                    "text": "32"
                }
            ]
        },
        {
            "text": "The data are available at http://genomics-pubs.princeton.edu/oncology/. ",
            "section": "Example 1",
            "paragraph_rank": 28,
            "section_rank": 9
        },
        {
            "text": "The gene expression level is presented in intensity value, and is otherwise unprocessed. ",
            "section": "Example 1",
            "paragraph_rank": 28,
            "section_rank": 9
        },
        {
            "text": "Hence, we first normalise the data by the mean and SD of each gene. ",
            "section": "Example 1",
            "paragraph_rank": 28,
            "section_rank": 9
        },
        {
            "text": "The data are then randomly divided into two parts, one for gene selection (28 tumour tissue samples and 10 normal colon tissue samples) and the other for model building and CV (12 tumour tissue samples and 12 normal colon tissue samples). ",
            "section": "Example 1",
            "paragraph_rank": 28,
            "section_rank": 9
        },
        {
            "text": "In the gene selection data set, we use the Mann-Whitney U test to identify the top 10 genes with the smallest p value from among the 2000 genes. ",
            "section": "Example 1",
            "paragraph_rank": 28,
            "section_rank": 9
        },
        {
            "text": "These are Hsa.627_M26383, Hsa.6814_H08393, Hsa.37937_R87126, Hsa.692_M76378-3, Hsa.3016_T47377, Hsa.31630_R64115, Hsa.831_M22382, Hsa.36689_Z50753, Hsa.3331_T86473 and Hsa.43279_H64489. ",
            "section": "Example 1",
            "paragraph_rank": 28,
            "section_rank": 9
        },
        {
            "text": "In the remaining data set, we build and cross-validate a prediction model for this 10-gene signature.",
            "section": "Example 1",
            "paragraph_rank": 28,
            "section_rank": 9
        },
        {
            "text": "For na\u00efve multiple regression, the AUCs (averaged from 1000 Monte Carlo partitions) are 0.936 (LOOCV), 0.929 (10-fold CV), 0.928 (5-fold CV), 0.925 (3-fold CV) and 0.921 (2-fold CV), respectively. ",
            "section": "Example 1",
            "paragraph_rank": 29,
            "section_rank": 9
        },
        {
            "text": "The (x, y)s are then calculated as: (11 \u00c01 \u00fe 11 \u00c01 ; Z \u00c02 0:936 )=(0.182, 0.432) for LOOCV, (0.200, 0.465) for 10-fold CV, (0.220, 0.466) for 5-fold CV, (0.250, 0.483) for 3-fold CV and (0.330, 0.503) for 2-fold CV, respectively. ",
            "section": "Example 1",
            "paragraph_rank": 29,
            "section_rank": 9
        },
        {
            "text": "These results are plotted in figure 3A. ",
            "section": "Example 1",
            "paragraph_rank": 29,
            "section_rank": 9,
            "ref_spans": [
                {
                    "start": 29,
                    "end": 38,
                    "type": "figure",
                    "text": "figure 3A"
                }
            ]
        },
        {
            "text": "We then draw a linear regression based on the five (x, y) points: y \u00bc 0:373 \u00fe 0:409x (the red line in figure 3A). ",
            "section": "Example 1",
            "paragraph_rank": 29,
            "section_rank": 9,
            "ref_spans": [
                {
                    "start": 102,
                    "end": 112,
                    "type": "figure",
                    "text": "figure 3A)"
                }
            ]
        },
        {
            "text": "To predict the performance with a sample size of 24 (all samples in the model building and CV data set are used as the training set, ie, 12 tumour and 12 normal tissue samples), we enter x \u00bc 12 \u00c01 \u00fe 12 \u00c01 into the regression equation to get\u0177 \u00bc 0:441 (* in figure 3A). ",
            "section": "Example 1",
            "paragraph_rank": 29,
            "section_rank": 9,
            "ref_spans": [
                {
                    "start": 256,
                    "end": 266,
                    "type": "figure",
                    "text": "figure 3A)"
                }
            ]
        },
        {
            "text": "The extrapolated performance is therefore d AUC T \u00bc F ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi 0:441 \u00c01 p \u00bc 0:930. ",
            "section": "Example 1",
            "paragraph_rank": 29,
            "section_rank": 9
        },
        {
            "text": "We next perform a total of 100 bootstrapping for this example and the bootstrapped SE for d AUC T is calculated as 0.080. ",
            "section": "Example 1",
            "paragraph_rank": 29,
            "section_rank": 9
        },
        {
            "text": "The results for this example when SVM is used for constructing the prediction model are shown in figure 3B. ",
            "section": "Example 1",
            "paragraph_rank": 29,
            "section_rank": 9,
            "ref_spans": [
                {
                    "start": 97,
                    "end": 106,
                    "type": "figure",
                    "text": "figure 3B"
                }
            ]
        },
        {
            "text": "The d AUC T (\u00b1 bootstrapped SE) is calculated as 0.940 (\u00b10.079).",
            "section": "Example 1",
            "paragraph_rank": 29,
            "section_rank": 9
        },
        {
            "text": "Example 2",
            "section_rank": 10
        },
        {
            "text": "The second example is a breast cancer data set, 33 which is available at the Gene Expression Omnibus database (http://www.ncbi.nlm.nih.gov/geo), with accession code GSE2990. ",
            "section": "Example 2",
            "paragraph_rank": 30,
            "section_rank": 10,
            "ref_spans": [
                {
                    "start": 48,
                    "end": 50,
                    "type": "bibr",
                    "ref_id": "b32",
                    "text": "33"
                }
            ]
        },
        {
            "text": "These data consist of 22 215 genes for 189 patients with breast cancer (120 patients without relapse and 67 with relapse; 2 patients with unknown relapse status are omitted from our demonstration). ",
            "section": "Example 2",
            "paragraph_rank": 30,
            "section_rank": 10
        },
        {
            "text": "The provided data set has already been processed (with background correction, quantile normalisation and log transformation). ",
            "section": "Example 2",
            "paragraph_rank": 30,
            "section_rank": 10
        },
        {
            "text": "The data set details and patient profile can be found in the corresponding reference and aforementioned GEO website. ",
            "section": "Example 2",
            "paragraph_rank": 30,
            "section_rank": 10
        },
        {
            "text": "We choose those 'extreme' patients to be in the gene selection data set, that is, those 43 patients who developed relapse within 5 years and those 91 patients who were free of relapse for at least 5 years. ",
            "section": "Example 2",
            "paragraph_rank": 30,
            "section_rank": 10
        },
        {
            "text": "The remaining data set (for model building and CV) now consists of 67\u221243=24 patients who developed relapses after 5 years and 120\u221291=29 patients free of relapses during their less than 5-year follow-up periods.",
            "section": "Example 2",
            "paragraph_rank": 30,
            "section_rank": 10
        },
        {
            "text": "In the gene selection data set, we again pick the top 10 genes from among the 22 215 genes with the smallest p value after Mann-Whitney U test. ",
            "section": "Example 2",
            "paragraph_rank": 31,
            "section_rank": 10
        },
        {
            "text": "These 10 genes are 203213_at, 210222_s_at, 205898_at, 218883_s_at, 203485_at, 201890_at, 214710_s_at, 202779_s_at, 202503_s_at and 201291_s_at. ",
            "section": "Example 2",
            "paragraph_rank": 31,
            "section_rank": 10
        },
        {
            "text": "The remaining data set is used to build and cross-validate the prediction model for this 10-gene signature.",
            "section": "Example 2",
            "paragraph_rank": 31,
            "section_rank": 10
        },
        {
            "text": "The results for na\u00efve multiple regression are presented in figure 3A (blue line). ",
            "section": "Example 2",
            "paragraph_rank": 32,
            "section_rank": 10,
            "ref_spans": [
                {
                    "start": 59,
                    "end": 80,
                    "type": "figure",
                    "text": "figure 3A (blue line)"
                }
            ]
        },
        {
            "text": "The one-step extrapolated AUC (\u00b1 bootstrappedSE) is calculated as 0.803 (\u00b10.082). ",
            "section": "Example 2",
            "paragraph_rank": 32,
            "section_rank": 10
        },
        {
            "text": "The results for SVM are presented in figure 3B (blue line). ",
            "section": "Example 2",
            "paragraph_rank": 32,
            "section_rank": 10,
            "ref_spans": [
                {
                    "start": 37,
                    "end": 58,
                    "type": "figure",
                    "text": "figure 3B (blue line)"
                }
            ]
        },
        {
            "text": "The one-step extrapolated AUC (\u00b1 bootstrappedSE) is calculated as 0.781 (\u00b10.063).",
            "section": "Example 2",
            "paragraph_rank": 32,
            "section_rank": 10
        },
        {
            "text": "Example 3",
            "section_rank": 11
        },
        {
            "text": "The third example is a different breast cancer data set. ",
            "section": "Example 3",
            "paragraph_rank": 33,
            "section_rank": 11
        },
        {
            "text": "The data set 34 and its patient profile are available at the GEO database (http://www.ncbi.nlm.nih.gov/geo) with accession code GSE2034. ",
            "section": "Example 3",
            "paragraph_rank": 33,
            "section_rank": 11,
            "ref_spans": [
                {
                    "start": 13,
                    "end": 15,
                    "type": "bibr",
                    "ref_id": "b33",
                    "text": "34"
                }
            ]
        },
        {
            "text": "The data consisting of 107 patients with breast cancer with distant relapse and 197 without distant relapse) was divided into training (115 patients) and testing (171 patients) by concentration of the oestrogen receptor and a 76-gene signature was identified by previous researchers. ",
            "section": "Example 3",
            "paragraph_rank": 33,
            "section_rank": 11
        },
        {
            "text": "34 We use our method to estimate the prediction performance of this 76-gene signature.",
            "section": "Example 3",
            "paragraph_rank": 33,
            "section_rank": 11,
            "ref_spans": [
                {
                    "start": 0,
                    "end": 2,
                    "type": "bibr",
                    "ref_id": "b33",
                    "text": "34"
                }
            ]
        },
        {
            "text": "The results for na\u00efve multiple regression are presented in figure 3A (green line). ",
            "section": "Example 3",
            "paragraph_rank": 34,
            "section_rank": 11,
            "ref_spans": [
                {
                    "start": 59,
                    "end": 81,
                    "type": "figure",
                    "text": "figure 3A (green line)"
                }
            ]
        },
        {
            "text": "The one-step extrapolated AUC (\u00b1 bootstrappedSE) is calculated as 0.726 (\u00b10.005). ",
            "section": "Example 3",
            "paragraph_rank": 34,
            "section_rank": 11
        },
        {
            "text": "The results for SVM are presented in figure 3B (green  line). ",
            "section": "Example 3",
            "paragraph_rank": 34,
            "section_rank": 11,
            "ref_spans": [
                {
                    "start": 37,
                    "end": 60,
                    "type": "figure",
                    "text": "figure 3B (green  line)"
                }
            ]
        },
        {
            "text": "The one-step extrapolated AUC (\u00b1 bootstrapped SE is calculated as 0.716 (\u00b10.010).",
            "section": "Example 3",
            "paragraph_rank": 34,
            "section_rank": 11
        },
        {
            "text": "DISCUSSION",
            "section_rank": 12
        },
        {
            "text": "When estimating the performance of a model derived from a small study, there seems to be no reason to settle for a sample size of N T \u00c0 1 (or N T \u00c0 2, in a leaveone-pair-out CV), since what we are looking for is the performance at sample size N T . ",
            "section": "DISCUSSION",
            "paragraph_rank": 35,
            "section_rank": 12
        },
        {
            "text": "In this study, we extrapolate the performance to N T by exploiting the linear relation between Z \u00c02 AUC and n \u00c01 1 \u00fe n \u00c01 0 . ",
            "section": "DISCUSSION",
            "paragraph_rank": 35,
            "section_rank": 12
        },
        {
            "text": "The extrapolation is based on five CV methods (LOOCV, 10-fold, 5-fold, 3-fold and 2-fold CV) and is carried out only one-step ahead. ",
            "section": "DISCUSSION",
            "paragraph_rank": 35,
            "section_rank": 12
        },
        {
            "text": "The resulting estimate thus inherits the lack of bias in LOOCV and strikes a satisfying variance among the five CV methods. ",
            "section": "DISCUSSION",
            "paragraph_rank": 35,
            "section_rank": 12
        },
        {
            "text": "A computer simulation shows that our method performs the best in terms of RMSE when sample sizes are small. ",
            "section": "DISCUSSION",
            "paragraph_rank": 35,
            "section_rank": 12
        },
        {
            "text": "The learning curve for AUC used in this study is based on a linear prediction model (online supplementary appendix 1). ",
            "section": "DISCUSSION",
            "paragraph_rank": 35,
            "section_rank": 12
        },
        {
            "text": "However, our simulation study shows that the learning curve is equally suited for non-linear prediction models, such as SVM (figure 2) and RF (online supplementary appendix 9). ",
            "section": "DISCUSSION",
            "paragraph_rank": 35,
            "section_rank": 12
        },
        {
            "text": "When making the extrapolation, we may sometimes encounter a slope (b in y \u00bc a \u00fe bx) that is near zero (eg, the colon cancer example demonstrated in figure 3). ",
            "section": "DISCUSSION",
            "paragraph_rank": 35,
            "section_rank": 12,
            "ref_spans": [
                {
                    "start": 148,
                    "end": 157,
                    "type": "figure",
                    "text": "figure 3)"
                }
            ]
        },
        {
            "text": "This may occur when the model performance has reached its plateau; thus, varying the training size (as in different CV methods) has little effect on AUC estimates. ",
            "section": "DISCUSSION",
            "paragraph_rank": 35,
            "section_rank": 12
        },
        {
            "text": "This can also occur at the other extreme when the prediction/ classification problem at hand is more complex and requires a much larger training size than is currently available, to significantly enhance the model performance. ",
            "section": "DISCUSSION",
            "paragraph_rank": 35,
            "section_rank": 12
        },
        {
            "text": "In either case, our method amounts to taking the average of the five CV estimates, thereby stabilising the variances.",
            "section": "DISCUSSION",
            "paragraph_rank": 35,
            "section_rank": 12
        },
        {
            "text": "Two previous studies 30 35 also exploited the extrapolation concept. ",
            "section": "DISCUSSION",
            "paragraph_rank": 36,
            "section_rank": 12,
            "ref_spans": [
                {
                    "start": 21,
                    "end": 26,
                    "type": "bibr",
                    "text": "30 35"
                }
            ]
        },
        {
            "text": "Both used an inverse power-law model as the empirical learning curve. ",
            "section": "DISCUSSION",
            "paragraph_rank": 36,
            "section_rank": 12
        },
        {
            "text": "In this paper, we are only interested in how the performance will result if all the samples we have are utilised to train the model. ",
            "section": "DISCUSSION",
            "paragraph_rank": 36,
            "section_rank": 12
        },
        {
            "text": "We do need an equation (a learning curve) for extrapolation, but this requires extrapolating a mere one step ahead. ",
            "section": "DISCUSSION",
            "paragraph_rank": 36,
            "section_rank": 12
        },
        {
            "text": "Our results should therefore be less dependent on what learning curves are being used. ",
            "section": "DISCUSSION",
            "paragraph_rank": 36,
            "section_rank": 12
        },
        {
            "text": "Figure 3 Demonstration of the three microarray data using the proposed extrapolation method. ",
            "section": "DISCUSSION",
            "paragraph_rank": 36,
            "section_rank": 12,
            "ref_spans": [
                {
                    "start": 0,
                    "end": 8,
                    "type": "figure",
                    "text": "Figure 3"
                }
            ]
        },
        {
            "text": "In this double-inverse coordinate system, the ordinate is the inverse of the (transformed) AUC value, and the abscissa is the inverse of the training sample size. ",
            "section": "DISCUSSION",
            "paragraph_rank": 36,
            "section_rank": 12
        },
        {
            "text": "The red, blue and green lines represent the colon cancer data (example 1), the GSE2990 breast cancer data (example 2) and the GSE2034 breast cancer data (example 3), respectively. ",
            "section": "DISCUSSION",
            "paragraph_rank": 36,
            "section_rank": 12
        },
        {
            "text": "The five dots along each line are the estimates of the five different fold CV and the * is the extrapolated AUC (AUC, area under the receiver operating characteristic curve; CV, cross-validation).",
            "section": "DISCUSSION",
            "paragraph_rank": 36,
            "section_rank": 12
        },
        {
            "text": "Figure 1",
            "section_rank": 13
        },
        {
            "text": "Bias, variance and root mean squared error (RMSE) of the various methods under different sample sizes when the na\u00efve multiple regression is used to build the gene signature leave-one-out cross-validation (blue line), fivefold cross-validation (yellow line), twofold cross-validation (green line), leave-one-out bootstrap (black dashed line) and the proposed method (red line). ",
            "section": "Figure 1",
            "paragraph_rank": 37,
            "section_rank": 13
        },
        {
            "text": "The leftmost column of panels is for normally distributed data with a correlation coefficient of 0, the second column from left with a correlation coefficient of 0.2, and the third column from left with a correlation coefficient of 0.5. ",
            "section": "Figure 1",
            "paragraph_rank": 37,
            "section_rank": 13
        },
        {
            "text": "The rightmost column of panels is for complex data (mixture of normal distributions). ",
            "section": "Figure 1",
            "paragraph_rank": 37,
            "section_rank": 13
        },
        {
            "text": "The horizontal thin lines indicate a position of no bias.",
            "section": "Figure 1",
            "paragraph_rank": 37,
            "section_rank": 13
        },
        {
            "text": "Figure 2",
            "section_rank": 14
        },
        {
            "text": "Bias, variance and root mean squared error (RMSE) of the various methods under different sample sizes when the support vector machine is used to build the gene signature leave-one-out cross-validation (blue line), fivefold cross-validation (yellow line), twofold cross-validation (green line), leave-one-out bootstrap (black dashed line) and the proposed method (red line). ",
            "section": "Figure 2",
            "paragraph_rank": 38,
            "section_rank": 14
        },
        {
            "text": "The leftmost column of panels is for normally distributed data with a correlation coefficient of 0, the second column from left with a correlation coefficient of 0.2, and the third column from left with a correlation coefficient of 0.5. ",
            "section": "Figure 2",
            "paragraph_rank": 38,
            "section_rank": 14
        },
        {
            "text": "The rightmost column of panels is for complex data (mixture of normal distributions). ",
            "section": "Figure 2",
            "paragraph_rank": 38,
            "section_rank": 14
        },
        {
            "text": "The horizontal thin lines indicate a position of no bias.",
            "section": "Figure 2",
            "paragraph_rank": 38,
            "section_rank": 14
        },
        {
            "text": "Open Access",
            "section": "Figure 2",
            "paragraph_rank": 39,
            "section_rank": 14
        },
        {
            "text": "Acknowledgements The authors would like to thank Dr Yung-Hsiang Huang and Mr Po-Chang Hsiao for technical supports.",
            "section": "Figure 2",
            "paragraph_rank": 40,
            "section_rank": 16
        }
    ]
}