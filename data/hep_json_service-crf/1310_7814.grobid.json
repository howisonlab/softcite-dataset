{
    "level": "sentence",
    "abstract": [
        {
            "text": "The study group on data preservation in high energy physics, DPHEP, is moving to a new collaboration structure, which will focus on the implementation of preservation projects, such as those described in the group's large scale report published in 2012. ",
            "paragraph_rank": 1,
            "section_rank": 1
        },
        {
            "text": "One such project is the development of a validation framework, which checks the compatibility of evolving computing environments and technologies with the experiments software for as long as possible, with the aim of substantially extending the lifetime of the analysis software, and hence of the usability of the data. ",
            "paragraph_rank": 1,
            "section_rank": 1
        },
        {
            "text": "The framework is designed to automatically test and validate the software and data of an experiment against changes and upgrades to the computing environment, as well as changes to the experiment software itself. ",
            "paragraph_rank": 1,
            "section_rank": 1
        },
        {
            "text": "Technically, this is realised using a framework capable of hosting a number of virtual machine images, built with different configurations of operating systems and the relevant software, including any necessary external dependencies.",
            "paragraph_rank": 1,
            "section_rank": 1
        }
    ],
    "body_text": [
        {
            "text": "Introduction: Data preservation in high energy physics",
            "section_rank": 2
        },
        {
            "text": "The problem of data persistence and preservation is not new, but is becoming more prominent with the advent of so called big data, in particular within the applied sciences. ",
            "section": "Introduction: Data preservation in high energy physics",
            "paragraph_rank": 2,
            "section_rank": 2
        },
        {
            "text": "However, until recently high energy physics (HEP) had little or no tradition or clear model of long-term preservation of data in a meaningful and useful way, and the data from the majority of older experiments have simply been lost. ",
            "section": "Introduction: Data preservation in high energy physics",
            "paragraph_rank": 2,
            "section_rank": 2
        },
        {
            "text": "Attempts to preserve previous data sets have in general not been a planned initiative by the original collaboration but a push by knowledgeable people, usually at a later date. ",
            "section": "Introduction: Data preservation in high energy physics",
            "paragraph_rank": 2,
            "section_rank": 2
        },
        {
            "text": "This is despite several clear scenarios where preservation of HEP data is beneficial for a number of reasons including: to allow the re-analysis of data taken at a unique centre of mass energy and/or with unique initial state particles, especially if new theoretical predictions or analysis techniques become available; to aid the combination of data sets between similar experiments; for verification of new phenomena found by another HEP experiment; and to allow the use of real HEP data in scientific training, education and outreach.",
            "section": "Introduction: Data preservation in high energy physics",
            "paragraph_rank": 2,
            "section_rank": 2
        },
        {
            "text": "The start of the 21st century saw the end of operation of several particle colliders including LEP (e + e \u2212 , where data taking ended in 2000), HERA (e \u00b1 p, 2007), PEP-II (e + e \u2212 , 2008), KEKB (e + e \u2212 , 2010) and the Tevatron (pp, 2011), providing unique data sets in terms of initial state particles or centre of mass energy or both. ",
            "section": "Introduction: Data preservation in high energy physics",
            "paragraph_rank": 3,
            "section_rank": 2
        },
        {
            "text": "As the experiments at each of these colliders continued to publish their final results and conclude their core physics programmes, the question of what should be done with the data naturally presented itself. ",
            "section": "Introduction: Data preservation in high energy physics",
            "paragraph_rank": 3,
            "section_rank": 2
        },
        {
            "text": "Inspired by a lack of concrete solutions or guidelines to the problem of data preservation in HEP, an international study group on data preservation and long-term analysis in high-energy physics, DPHEP, was formed at the end of 2008 to address the issue in a systematic way. ",
            "section": "Introduction: Data preservation in high energy physics",
            "paragraph_rank": 3,
            "section_rank": 2
        },
        {
            "text": "The composition of the group was initially driven by BaBar and the HERA experiments H1, ZEUS and HERMES, who were soon joined by Belle, BES-III and the Tevatron experiments CDF and D\u00d8. ",
            "section": "Introduction: Data preservation in high energy physics",
            "paragraph_rank": 3,
            "section_rank": 2
        },
        {
            "text": "The LEP experiments are also represented in DPHEP and the LHC experiments ALICE, ATLAS, CMS and LHCb joined the study group in 2011. ",
            "section": "Introduction: Data preservation in high energy physics",
            "paragraph_rank": 3,
            "section_rank": 2
        },
        {
            "text": "The laboratories and associated computing centres at BNL, CERN, DESY, Fermilab, JLAB, KEK and SLAC are also all members of DPHEP, in addition to several funding agencies. ",
            "section": "Introduction: Data preservation in high energy physics",
            "paragraph_rank": 3,
            "section_rank": 2
        },
        {
            "text": "A series of seven workshops have taken place since 2009 and DPHEP is officially endorsed with a mandate by the International Committee for Future Accelerators, ICFA.",
            "section": "Introduction: Data preservation in high energy physics",
            "paragraph_rank": 3,
            "section_rank": 2
        },
        {
            "text": "The initial findings of the study group are summarised in a short interim report released in 2009 and a full status report was published in 2012 [1]. ",
            "section": "Introduction: Data preservation in high energy physics",
            "paragraph_rank": 4,
            "section_rank": 2,
            "ref_spans": [
                {
                    "start": 145,
                    "end": 148,
                    "type": "bibr",
                    "ref_id": "b0",
                    "text": "[1]"
                }
            ]
        },
        {
            "text": "The report contains: a tour of data preservation activities in other fields; an expanded description of the physics case; a guide to defining and establishing data preservation principles; and updates from experiments and joint projects, as well as person-power estimates for these and future projects. ",
            "section": "Introduction: Data preservation in high energy physics",
            "paragraph_rank": 4,
            "section_rank": 2
        },
        {
            "text": "After many decades of neglect with respect to other scientific disciplines, data preservation is now a rapidly emerging field in HEP, where DPHEP is established as the coherent multi-laboratory, multi-experiment body to examine this issue.",
            "section": "Introduction: Data preservation in high energy physics",
            "paragraph_rank": 4,
            "section_rank": 2
        },
        {
            "text": "The DPHEP Study Group is now moving to a new organisational model, the DPHEP Collaboration and the formal signing procedure of the Collaboration Agreement has now commenced. ",
            "section": "Introduction: Data preservation in high energy physics",
            "paragraph_rank": 5,
            "section_rank": 2
        },
        {
            "text": "In addition to the DPHEP Chair, a Project Manager position has now been established, which is initially based at CERN.",
            "section": "Introduction: Data preservation in high energy physics",
            "paragraph_rank": 5,
            "section_rank": 2,
            "entity_spans": [
                {
                    "start": 34,
                    "end": 50,
                    "type": "software",
                    "rawForm": "Project Manager",
                    "resp": "service",
                    "id": "software-simple-s1"
                }
            ]
        },
        {
            "text": "Preservation models",
            "section_rank": 3
        },
        {
            "text": "If one thing may be learned from previous enterprises, it is that the conservation of tapes is not equivalent to data preservation, and that not only the hardware to access the data but also the software and environment to understand the data are the necessary and more challenging aspects. ",
            "section": "Preservation models",
            "paragraph_rank": 6,
            "section_rank": 3
        },
        {
            "text": "In addition to the data, the various software, such as simulation, reconstruction and analysis software need to be considered. ",
            "section": "Preservation models",
            "paragraph_rank": 6,
            "section_rank": 3
        },
        {
            "text": "If the experimental software is not available the possibility to study new observables or to incorporate new reconstruction algorithms, detector simulations or event generators is lost. ",
            "section": "Preservation models",
            "paragraph_rank": 6,
            "section_rank": 3
        },
        {
            "text": "Without a well defined and understood software environment the scientific potential of the data may be limited. ",
            "section": "Preservation models",
            "paragraph_rank": 6,
            "section_rank": 3
        },
        {
            "text": "Just as important are the various types of documentation, covering all facets of an experiment. ",
            "section": "Preservation models",
            "paragraph_rank": 6,
            "section_rank": 3
        },
        {
            "text": "Considering this inclusive definition of HEP data, the DPHEP Collaboration has established a series of data preservation levels, as summarised in table 1. ",
            "section": "Preservation models",
            "paragraph_rank": 6,
            "section_rank": 3
        },
        {
            "text": "The levels are organised in order of increasing benefit, which comes with increasing complexity and cost. ",
            "section": "Preservation models",
            "paragraph_rank": 6,
            "section_rank": 3
        },
        {
            "text": "Each level is associated with use cases, and the preservation model adopted by an experiment should reflect the level of analysis expected to be available in the future.",
            "section": "Preservation models",
            "paragraph_rank": 6,
            "section_rank": 3
        },
        {
            "text": "The four levels represent three different areas, representing complementary initiatives:",
            "section": "Preservation models",
            "paragraph_rank": 7,
            "section_rank": 3
        },
        {
            "text": "documentation (level 1), outreach and simplified formats for data exchange (level 2) and technical preservation projects (levels 3 and 4). ",
            "section": "Preservation models",
            "paragraph_rank": 8,
            "section_rank": 3
        },
        {
            "text": "Whereas most collaborations involved in DPHEP pursue some form of level 1 and 2 strategies, levels 3 and 4 are really the main focus of the data preservation effort: to maintain usable access to analysis level data, Monte Carlo and the analysis level software, in addition (in the case of level 4) to the reconstruction and simulation software. ",
            "section": "Preservation models",
            "paragraph_rank": 8,
            "section_rank": 3
        },
        {
            "text": "Previous data preservation experiences in HEP indicate that new analyses and complete reanalyses are only possible if all the necessary ingredients to retrieve, reconstruct and understand the data are accounted for. ",
            "section": "Preservation models",
            "paragraph_rank": 8,
            "section_rank": 3
        },
        {
            "text": "Only with the full flexibility provided by a level 4 preservation model does the full potential of the data remain. ",
            "section": "Preservation models",
            "paragraph_rank": 8,
            "section_rank": 3
        },
        {
            "text": "Most experiments in DPHEP plan for a level 4 preservation programme, although different approaches are employed concerning how this goal should be achieved. ",
            "section": "Preservation models",
            "paragraph_rank": 8,
            "section_rank": 3
        },
        {
            "text": "One option, typically realised using virtualisation, is to freeze the current system to keep the software and environment alive as long as possible. ",
            "section": "Preservation models",
            "paragraph_rank": 8,
            "section_rank": 3
        },
        {
            "text": "Although this will provide a workable solution for the mediumterm future, the operability of the software and correctness of the results are not guaranteed. ",
            "section": "Preservation models",
            "paragraph_rank": 8,
            "section_rank": 3
        },
        {
            "text": "If changes are needed it will become more difficult the longer software is frozen and future compatibility issues may arise, in addition to the inevitable security concerns.",
            "section": "Preservation models",
            "paragraph_rank": 8,
            "section_rank": 3
        },
        {
            "text": "The alternative approach employed at DESY is to adapt and validate the software and environment to future changes as and when they happen. ",
            "section": "Preservation models",
            "paragraph_rank": 9,
            "section_rank": 3
        },
        {
            "text": "In this way, the working version of the experimental software is actively migrated to more modern platforms and future-proof resources, substantially extending the lifetime of the software, and hence the data, whilst retaining the flexibility to apply any necessary changes. ",
            "section": "Preservation models",
            "paragraph_rank": 9,
            "section_rank": 3
        },
        {
            "text": "The success of such migrations depends on having a robust and complete set of validation tests. ",
            "section": "Preservation models",
            "paragraph_rank": 9,
            "section_rank": 3
        },
        {
            "text": "Virtualisation techniques are again used, in this case to provide a coherent platform to build and validate software and environments created according to the recipes provided by the experiments.",
            "section": "Preservation models",
            "paragraph_rank": 9,
            "section_rank": 3
        },
        {
            "text": "The software preservation system at DESY",
            "section_rank": 4
        },
        {
            "text": "A generic validation suite, which includes automated software build tools and data validation, has been developed at DESY to automatically test and validate the software and data of an experiment against changes and upgrades to the environment, as well as changes to the experiment software itself. ",
            "section": "The software preservation system at DESY",
            "paragraph_rank": 10,
            "section_rank": 4
        },
        {
            "text": "Technically, this is realised using a framework capable of hosting a number of virtual machine images, built with different configurations of operating systems (OS) and the relevant software, including any necessary external dependencies.",
            "section": "The software preservation system at DESY",
            "paragraph_rank": 10,
            "section_rank": 4
        },
        {
            "text": "Inputs to the preservation system and work flow",
            "section_rank": 5
        },
        {
            "text": "Three distinct categories are identified as separate inputs to the validation system, as illustrated in figure 1: the experiment specific software, any external software dependencies and finally the operating system, including the compiler. ",
            "section": "Inputs to the preservation system and work flow",
            "paragraph_rank": 11,
            "section_rank": 5
        },
        {
            "text": "The work flow of the validation framework, called the sp-system, is then as follows:",
            "section": "Inputs to the preservation system and work flow",
            "paragraph_rank": 11,
            "section_rank": 5
        },
        {
            "text": "(i) In an initial, preparatory phase, the experimental software should be consolidated, the OS migrated to the most recent release, and any unnecessary external dependencies removed. ",
            "section": "Inputs to the preservation system and work flow",
            "paragraph_rank": 12,
            "section_rank": 5
        },
        {
            "text": "Any remaining, well-defined necessary dependencies are then also incorporated. ",
            "section": "Inputs to the preservation system and work flow",
            "paragraph_rank": 12,
            "section_rank": 5
        },
        {
            "text": "Analysis and data validation tests should then be defined and prepared, examining each part of the experimental software deemed necessary in the preservation model adopted. ",
            "section": "Inputs to the preservation system and work flow",
            "paragraph_rank": 12,
            "section_rank": 5
        },
        {
            "text": "(ii) A regular build of the experimental software is done automatically according to the current prescription of the working environment, and the validation tests are performed. ",
            "section": "Inputs to the preservation system and work flow",
            "paragraph_rank": 12,
            "section_rank": 5
        },
        {
            "text": "At regular intervals, new OS and software versions will then be integrated into the system, under the supervision of experts from the host IT department and experiment. ",
            "section": "Inputs to the preservation system and work flow",
            "paragraph_rank": 12,
            "section_rank": 5
        },
        {
            "text": "(iii) If the validation is successful, no further action must be taken. ",
            "section": "Inputs to the preservation system and work flow",
            "paragraph_rank": 12,
            "section_rank": 5
        },
        {
            "text": "If a test fails, any differences compared to the last successful test are examined and problems identified. ",
            "section": "Inputs to the preservation system and work flow",
            "paragraph_rank": 12,
            "section_rank": 5
        },
        {
            "text": "Intervention Figure 1. ",
            "section": "Inputs to the preservation system and work flow",
            "paragraph_rank": 12,
            "section_rank": 5,
            "ref_spans": [
                {
                    "start": 13,
                    "end": 21,
                    "type": "figure",
                    "text": "Figure 1"
                }
            ]
        },
        {
            "text": "An illustration of the validation system developed at DESY. ",
            "section": "Inputs to the preservation system and work flow",
            "paragraph_rank": 12,
            "section_rank": 5
        },
        {
            "text": "Note the clear separation of the inputs: experiment specific software, external dependencies and operating system.",
            "section": "Inputs to the preservation system and work flow",
            "paragraph_rank": 12,
            "section_rank": 5
        },
        {
            "text": "is then required either by the host of the validation suite or the experiment themselves, depending on the nature of the reported problem. ",
            "section": "Inputs to the preservation system and work flow",
            "paragraph_rank": 13,
            "section_rank": 5
        },
        {
            "text": "(iv) The final phase occurs either when no person-power is available from the experiment or IT side or the current system is deemed satisfactory for the long-term need or stable enough. ",
            "section": "Inputs to the preservation system and work flow",
            "paragraph_rank": 13,
            "section_rank": 5
        },
        {
            "text": "At this point the last working virtual image is conserved and constitutes the last version of the experimental software and environment. ",
            "section": "Inputs to the preservation system and work flow",
            "paragraph_rank": 13,
            "section_rank": 5
        },
        {
            "text": "It should be noted however, that this now frozen system is unlikely to persist in a useful manner much beyond this point.",
            "section": "Inputs to the preservation system and work flow",
            "paragraph_rank": 13,
            "section_rank": 5
        },
        {
            "text": "The sp-system is designed for software verification, validation and migration support only. ",
            "section": "Inputs to the preservation system and work flow",
            "paragraph_rank": 14,
            "section_rank": 5
        },
        {
            "text": "Neither the hardware resources nor the interface are designed for mass production or large-scale analysis. ",
            "section": "Inputs to the preservation system and work flow",
            "paragraph_rank": 14,
            "section_rank": 5
        },
        {
            "text": "The framework is rather used to establish the latest working version of the computing and software environment and it can help to prepare a production system by supplying the successfully validated recipe of the latest configuration. ",
            "section": "Inputs to the preservation system and work flow",
            "paragraph_rank": 14,
            "section_rank": 5
        },
        {
            "text": "If a production system is required, then this recipe should be deployed on a suitable resource at the time: an institute cluster, grid, cloud, sky, quantum computer, and so on.",
            "section": "Inputs to the preservation system and work flow",
            "paragraph_rank": 14,
            "section_rank": 5
        },
        {
            "text": "Within the current sp-system there are virtual machines with five different configurations: SL5/32bit with gcc4.1 and gcc4.4, SL5/64bit with gcc4.1 and gcc4.4, SL6/64bit with gcc4.4. ",
            "section": "Inputs to the preservation system and work flow",
            "paragraph_rank": 15,
            "section_rank": 5
        },
        {
            "text": "In addition, the set of external software required by the experiments is also installed, for example the ROOT versions used by the experiments: 5.26, 5.28, 5.30, 5.32, and 5.34. ",
            "section": "Inputs to the preservation system and work flow",
            "paragraph_rank": 15,
            "section_rank": 5,
            "entity_spans": [
                {
                    "start": 105,
                    "end": 110,
                    "type": "software",
                    "rawForm": "ROOT",
                    "resp": "service",
                    "id": "software-simple-s2"
                }
            ]
        },
        {
            "text": "The sp-system is designed and constructed in a such a way that new client machines (as a virtual machine or a normal physical machine like a batch or grid worker node) can easily be added. ",
            "section": "Inputs to the preservation system and work flow",
            "paragraph_rank": 15,
            "section_rank": 5
        },
        {
            "text": "The only requirement of a new machine is to have access to the common sp-system storage where the tests from the experiments as well as the test results are stored, as well as the ability to run a cron-job on the client.",
            "section": "Inputs to the preservation system and work flow",
            "paragraph_rank": 15,
            "section_rank": 5
        },
        {
            "text": "Validation tests as defined by the experiments",
            "section_rank": 6
        },
        {
            "text": "In addition to the common infrastructure provided by the IT department, the development and implementation of the tests by the participating experiments requires significant investment, even if basic validation structures already exist. ",
            "section": "Validation tests as defined by the experiments",
            "paragraph_rank": 16,
            "section_rank": 6
        },
        {
            "text": "As a first step, the number and nature of the experimental tests is surveyed, the level of which reflects the DPHEP preservation level aimed at the participating collaboration. ",
            "section": "Validation tests as defined by the experiments",
            "paragraph_rank": 16,
            "section_rank": 6
        },
        {
            "text": "Figure 2 details the results of this evaluation by the H1 Collaboration, which is implementing a full level 4 preservation programme, expected to comprise of up to 500 tests in total. ",
            "section": "Validation tests as defined by the experiments",
            "paragraph_rank": 16,
            "section_rank": 6,
            "ref_spans": [
                {
                    "start": 0,
                    "end": 8,
                    "type": "figure",
                    "ref_id": "fig_0",
                    "text": "Figure 2"
                }
            ]
        },
        {
            "text": "The structure of the tests is divided into two parts. ",
            "section": "Validation tests as defined by the experiments",
            "paragraph_rank": 16,
            "section_rank": 6
        },
        {
            "text": "Firstly the compilation of approximately 100 individual H1 software packages and the identified external dependencies is carried out, where the resulting binaries are stored as tar-balls on the common storage within the sp-system. ",
            "section": "Validation tests as defined by the experiments",
            "paragraph_rank": 16,
            "section_rank": 6
        },
        {
            "text": "Secondly, a series of validation tests is performed on the full spectrum of the H1 software, using the compiled software. ",
            "section": "Validation tests as defined by the experiments",
            "paragraph_rank": 16,
            "section_rank": 6
        },
        {
            "text": "Whereas some of these tests examine the results of stand alone executables and are run in parallel, many are run sequentially and form discrete parts in one of several full analysis chains: from MC generation and simulation, through multi-level file production and ending with a full physics analysis and subsequent validation of the results.",
            "section": "Validation tests as defined by the experiments",
            "paragraph_rank": 16,
            "section_rank": 6
        },
        {
            "text": "Validation results",
            "section_rank": 7
        },
        {
            "text": "Each test-job started in the sp-system is typically assigned a unique ID, and all scripts and input files used in the test as well as all output files are kept. ",
            "section": "Validation results",
            "paragraph_rank": 17,
            "section_rank": 7
        },
        {
            "text": "This allows the validation of all versions against each other and ensures reproducibility of previous results. ",
            "section": "Validation results",
            "paragraph_rank": 17,
            "section_rank": 7
        },
        {
            "text": "In addition to this unique ID, validation jobs may be tagged with a description, indicating which software versions were used, and the Unix time stamp of the execution to aid the bookkeeping. ",
            "section": "Validation results",
            "paragraph_rank": 17,
            "section_rank": 7
        },
        {
            "text": "Script-based web pages are used to record and display available validation runs for a given description and indicate the status of the compilation for the individual packages or tests within table cells, which are linked to a corresponding output file. ",
            "section": "Validation results",
            "paragraph_rank": 17,
            "section_rank": 7
        },
        {
            "text": "This file may be a simple yes/no, a text file, a histogram, a root file or even a link to a further page, depending on the nature of the test.",
            "section": "Validation results",
            "paragraph_rank": 17,
            "section_rank": 7
        },
        {
            "text": "In total more than 300 runs over sets of pre-defined tests have been performed within the sp-system by the HERA experiments. ",
            "section": "Validation results",
            "paragraph_rank": 18,
            "section_rank": 7
        },
        {
            "text": "A summary of the current status of the validation tests is displayed in figure 3, showing a coarse breakdown for ZEUS (orange), H1 (blue) and HERMES (red) tests and the different dependencies. ",
            "section": "Validation results",
            "paragraph_rank": 18,
            "section_rank": 7,
            "ref_spans": [
                {
                    "start": 72,
                    "end": 80,
                    "type": "figure",
                    "ref_id": "fig_1",
                    "text": "figure 3"
                }
            ]
        },
        {
            "text": "The experiments are in the process of migrating to SL6/64bit, and the tests performed so far using the sp-system have already identified and 4. ",
            "section": "Validation results",
            "paragraph_rank": 18,
            "section_rank": 7
        },
        {
            "text": "Current situation and opportunity for expansion A software preservation system has been developed at DESY, supporting a series of validation tests defined by the HERA experiments, to ensure that these unique data are available for analysis for the next 10 years or more. ",
            "section": "Validation results",
            "paragraph_rank": 18,
            "section_rank": 7
        },
        {
            "text": "The process of defining and implementing the complete set of validation tests for the whole chain of software to be preserved is still ongoing and is expected to take another year. ",
            "section": "Validation results",
            "paragraph_rank": 18,
            "section_rank": 7
        },
        {
            "text": "From the IT side, the current version of the sp-system is very light for the user tests implemented so far and the common storage allows communication between the sp-system and the experiment tests using only a few shell variables. ",
            "section": "Validation results",
            "paragraph_rank": 18,
            "section_rank": 7
        },
        {
            "text": "These variables describe for example the location of the input file of the tests, the test outputs and the external software on the client. ",
            "section": "Validation results",
            "paragraph_rank": 18,
            "section_rank": 7
        },
        {
            "text": "Using thin layers of scripts, a separation of the user part from the details of the sp-system is possible, allowing already existing user tests to be integrated into the sp-system or tests developed within the sp-system to be ported to other test platforms. ",
            "section": "Validation results",
            "paragraph_rank": 18,
            "section_rank": 7
        },
        {
            "text": "By design the sp-system is expandable and able to host and validate the requirements of multiple experiments, and can be thought of as a tool to aid migration in order to detect problems and incoherence, as well as identifying and reporting the reasons behind them.",
            "section": "Validation results",
            "paragraph_rank": 18,
            "section_rank": 7
        },
        {
            "text": "Figure 2 .",
            "section_rank": 8
        },
        {
            "text": "Figure 2. ",
            "section": "Figure 2 .",
            "paragraph_rank": 19,
            "section_rank": 8
        },
        {
            "text": "An outline of the validation tests to be prepared by the H1 experiment.",
            "section": "Figure 2 .",
            "paragraph_rank": 19,
            "section_rank": 8
        },
        {
            "text": "Figure 3 .",
            "section_rank": 9
        },
        {
            "text": "Figure 3. ",
            "section": "Figure 3 .",
            "paragraph_rank": 20,
            "section_rank": 9
        },
        {
            "text": "A summary of the validation tests carried out by the HERA experiments within the sp-system at DESY. ",
            "section": "Figure 3 .",
            "paragraph_rank": 20,
            "section_rank": 9
        },
        {
            "text": "The different tests (processes) from the ZEUS (orange, top), H1 (blue, middle) and HERMES (red, bottom) experiments are run under different configurations of operating system and external dependencies.",
            "section": "Figure 3 .",
            "paragraph_rank": 20,
            "section_rank": 9
        },
        {
            "text": "Table 1 .",
            "section_rank": 10
        },
        {
            "text": "Data preservation levels as defined by the DPHEP Collaboration.",
            "section": "Table 1 .",
            "paragraph_rank": 21,
            "section_rank": 10
        }
    ]
}